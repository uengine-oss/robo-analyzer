"""DBMS ì½”ë“œ ë¶„ì„ê¸° - PL/SQL AST â†’ Neo4j ê·¸ë˜í”„

í”„ë¡œì‹œì €/í•¨ìˆ˜ ë¶„ì„ì— í•„ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

ë¶„ì„ íŒŒì´í”„ë¼ì¸:
1. AST ìˆ˜ì§‘ (StatementCollector)
2. ì •ì  ê·¸ë˜í”„ ìƒì„± (PROCEDURE, FUNCTION ë…¸ë“œ)
3. DML ë¬¸ ë¶„ì„ (í…Œì´ë¸”/ì»¬ëŸ¼ ê´€ê³„)
4. LLM ë°°ì¹˜ ë¶„ì„ (ìš”ì•½, ë³€ìˆ˜ íƒ€ì…)
5. í”„ë¡œì‹œì € ìš”ì•½ ë° User Story ìƒì„±
"""

from __future__ import annotations

import asyncio
import json
import logging
import re
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Tuple

from config.settings import settings
from util.rule_loader import RuleLoader
from util.exception import LLMCallError, CodeProcessError, AnalysisError
from util.utility_tool import calculate_code_token, escape_for_cypher, parse_table_identifier, log_process


# ==================== ìƒìˆ˜ ì •ì˜ ====================
# ë…¸ë“œ íƒ€ì… ë¶„ë¥˜
PROCEDURE_TYPES = ("PROCEDURE", "FUNCTION", "CREATE_PROCEDURE_BODY", "TRIGGER")
NON_ANALYSIS_TYPES = frozenset(["CREATE_PROCEDURE_BODY", "FILE", "PROCEDURE", "FUNCTION", "DECLARE", "TRIGGER", "SPEC"])
NON_NEXT_RECURSIVE_TYPES = frozenset(["FUNCTION", "PROCEDURE", "PACKAGE_VARIABLE", "TRIGGER"])
DML_STATEMENT_TYPES = frozenset(["SELECT", "INSERT", "UPDATE", "DELETE", "MERGE", "EXECUTE_IMMEDIATE", "FETCH", "CREATE_TEMP_TABLE", "CTE", "OPEN_CURSOR", "CURSOR_VARIABLE"])
VARIABLE_DECLARATION_TYPES = frozenset(["PACKAGE_VARIABLE", "DECLARE", "SPEC"])

# ê´€ê³„ ë§¤í•‘
TABLE_RELATIONSHIP_MAP = {"r": "FROM", "w": "WRITES"}
VARIABLE_ROLE_MAP = {
    "PACKAGE_VARIABLE": "íŒ¨í‚¤ì§€ ì „ì—­ ë³€ìˆ˜",
    "DECLARE": "ë³€ìˆ˜ ì„ ì–¸ë° ì´ˆê¸°í™”",
    "SPEC": "í•¨ìˆ˜ ë° í”„ë¡œì‹œì € ì…ë ¥ ë§¤ê°œë³€ìˆ˜",
}

# ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¤ëŠ” ìƒìˆ˜
STATIC_QUERY_BATCH_SIZE = settings.batch.static_query_batch_size
VARIABLE_CONCURRENCY = settings.concurrency.variable_concurrency
MAX_BATCH_TOKEN = settings.batch.max_batch_token
MAX_CONCURRENCY = settings.concurrency.max_concurrency
MAX_SUMMARY_CHUNK_TOKEN = settings.batch.max_summary_chunk_token

# ì •ê·œì‹ íŒ¨í„´
LINE_NUMBER_PATTERN = re.compile(r"^\d+\s*:")


# ==================== ë°ì´í„° í´ë˜ìŠ¤ ====================
@dataclass(slots=True)
class StatementNode:
    """í‰íƒ„í™”ëœ AST ë…¸ë“œë¥¼ í‘œí˜„í•©ë‹ˆë‹¤.

    - ìˆ˜ì§‘ ë‹¨ê³„ì—ì„œ ëª¨ë“  ë…¸ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    - ì´í›„ ë°°ì¹˜ê°€ ë§Œë“¤ì–´ì§ˆ ë•Œ ì´ ê°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    - LLM ìš”ì•½ì´ ëë‚˜ë©´ `summary`ì™€ `completion_event`ê°€ ì±„ì›Œì§‘ë‹ˆë‹¤.
    - `ok` í”Œë˜ê·¸ë¡œ ì„±ê³µ ì—¬ë¶€ë¥¼ ì¶”ì í•©ë‹ˆë‹¤ (ìì‹ ì‹¤íŒ¨ ì‹œ ë¶€ëª¨ë„ False).
    """
    node_id: int
    start_line: int
    end_line: int
    node_type: str
    code: str
    token: int
    has_children: bool
    procedure_key: Optional[str]
    procedure_type: Optional[str]
    procedure_name: Optional[str]
    schema_name: Optional[str]
    analyzable: bool
    dml: bool
    lines: List[Tuple[int, str]] = field(default_factory=list)
    parent: Optional[StatementNode] = None
    children: List[StatementNode] = field(default_factory=list)
    summary: Optional[str] = None
    ok: bool = True  # LLM ë¶„ì„ ì„±ê³µ ì—¬ë¶€ (ìì‹ ì‹¤íŒ¨ ì‹œ ë¶€ëª¨ë„ False)
    completion_event: asyncio.Event = field(init=False, repr=False)

    def __post_init__(self):
        object.__setattr__(self, "completion_event", asyncio.Event())

    def get_raw_code(self) -> str:
        """ë¼ì¸ ë²ˆí˜¸ë¥¼ í¬í•¨í•˜ì—¬ ë…¸ë“œì˜ ì›ë¬¸ ì½”ë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return '\n'.join(f"{line_no}: {text}" for line_no, text in self.lines)

    def get_compact_code(self) -> str:
        """ìì‹ ìš”ì•½ì„ í¬í•¨í•œ ë¶€ëª¨ ì½”ë“œ(LLM ì…ë ¥ìš©)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.children:
            return self.code

        result_lines: List[str] = []
        line_index = 0
        total_lines = len(self.lines)
        sorted_children = sorted(self.children, key=lambda child: child.start_line)

        for child in sorted_children:
            # ìì‹ ì´ì „ì˜ ë¶€ëª¨ ê³ ìœ  ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬í•©ë‹ˆë‹¤.
            while line_index < total_lines and self.lines[line_index][0] < child.start_line:
                line_no, text = self.lines[line_index]
                result_lines.append(f"{line_no}: {text}")
                line_index += 1

            # ìì‹ êµ¬ê°„ì€ ìì‹ ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤ (ì—†ìœ¼ë©´ ê¸°ë³¸ placeholder).
            if child.summary:
                child_summary = child.summary.strip()
                summary_line = f"{child.start_line}~{child.end_line}: {child_summary}"
            else:
                log_process("ANALYZE", "COLLECT", f"âš ï¸ ë¶€ëª¨ {self.start_line}~{self.end_line}ì˜ ìì‹ {child.start_line}~{child.end_line} ìš”ì•½ ì—†ìŒ - ì›ë¬¸ ë³´ê´€")
                summary_line = '\n'.join(
                    f"{line_no}: {text}"
                    for line_no, text in child.lines
                ).strip()

            result_lines.append(summary_line)

            # ìì‹ êµ¬ê°„ ì›ë³¸ ì½”ë“œëŠ” ê±´ë„ˆëœë‹ˆë‹¤.
            while line_index < total_lines and self.lines[line_index][0] <= child.end_line:
                line_index += 1

        # ë§ˆì§€ë§‰ ìì‹ ì´í›„ ë¶€ëª¨ ì½”ë“œê°€ ë‚¨ì•„ ìˆë‹¤ë©´ ì¶”ê°€í•©ë‹ˆë‹¤.
        while line_index < total_lines:
            line_no, text = self.lines[line_index]
            result_lines.append(f"{line_no}: {text}")
            line_index += 1

        return '\n'.join(result_lines)

    def get_placeholder_code(self) -> str:
        """ìì‹ êµ¬ê°„ì„ placeholderë¡œ ìœ ì§€í•œ ì½”ë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if not self.children:
            return self.code

        result_lines: List[str] = []
        line_index = 0
        total_lines = len(self.lines)
        sorted_children = sorted(self.children, key=lambda child: child.start_line)

        for child in sorted_children:
            while line_index < total_lines and self.lines[line_index][0] < child.start_line:
                line_no, text = self.lines[line_index]
                result_lines.append(f"{line_no}: {text}")
                line_index += 1

            result_lines.append(f"{child.start_line}: ...code...")

            while line_index < total_lines and self.lines[line_index][0] <= child.end_line:
                line_index += 1

        while line_index < total_lines:
            line_no, text = self.lines[line_index]
            result_lines.append(f"{line_no}: {text}")
            line_index += 1

        return '\n'.join(result_lines)


@dataclass(slots=True)
class ProcedureInfo:
    key: str
    procedure_type: str
    procedure_name: str
    schema_name: Optional[str]
    start_line: int
    end_line: int
    pending_nodes: int = 0


@dataclass(slots=True)
class AnalysisBatch:
    batch_id: int
    nodes: List[StatementNode]
    ranges: List[Dict[str, int]]
    dml_ranges: List[Dict[str, int]]
    progress_line: int

    def build_general_payload(self) -> str:
        """ì¼ë°˜ LLM í˜¸ì¶œìš©ìœ¼ë¡œ ë…¸ë“œë“¤ì˜ compact ì½”ë“œë¥¼ ê²°í•©í•©ë‹ˆë‹¤."""
        return '\n\n'.join(node.get_compact_code() for node in self.nodes)

    def build_dml_payload(self) -> Optional[str]:
        """DML ë…¸ë“œë§Œ ì¶”ë¦° ì›ë¬¸ ì½”ë“œë¥¼ ê²°í•©í•˜ì—¬ í…Œì´ë¸” ë¶„ì„ í”„ë¡¬í”„íŠ¸ì— ì „ë‹¬í•©ë‹ˆë‹¤."""
        dml_nodes = [node for node in self.nodes if node.dml]
        if not dml_nodes:
            return None
        return '\n\n'.join(
            node.get_compact_code() if node.has_children else node.get_raw_code()
            for node in dml_nodes
        )


@dataclass(slots=True)
class BatchResult:
    batch: AnalysisBatch
    general_result: Optional[Dict[str, Any]]
    table_result: Optional[Dict[str, Any]]


# ==================== í—¬í¼ í•¨ìˆ˜ ====================
def get_procedure_name_from_code(code: str) -> Tuple[Optional[str], Optional[str]]:
    """ì½”ë“œ ë¬¸ìì—´ì—ì„œ ìŠ¤í‚¤ë§ˆ/í”„ë¡œì‹œì € ì´ë¦„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    pattern = re.compile(
        r"\b(?:CREATE\s+(?:OR\s+REPLACE\s+)?)?(?:PROCEDURE|FUNCTION|TRIGGER)\s+"
        r"((?:\"[^\"]+\"|[A-Za-z_][\w$#]*)"
        r"(?:\s*\.\s*(?:\"[^\"]+\"|[A-Za-z_][\w$#]*)){0,2})",
        re.IGNORECASE,
    )
    prefix_pattern = re.compile(r"^\d+\s*:\s*")
    normalized = prefix_pattern.sub("", code)
    match = pattern.search(normalized)
    if not match:
        return None, None
    parts = [segment.strip().strip('"') for segment in re.split(r"\s*\.\s*", match.group(1))]
    if len(parts) == 3:
        return parts[0], f"{parts[1]}.{parts[2]}"
    if len(parts) == 2:
        return parts[0], parts[1]
    if parts:
        return None, parts[0]
    return None, None


def get_original_node_code(file_content: str, start_line: int, end_line: int) -> str:
    """íŒŒì¼ ì „ì²´ ë¬¸ìì—´ì—ì„œ íŠ¹ì • êµ¬ê°„ì„ ë¼ì¸ ë²ˆí˜¸ì™€ í•¨ê»˜ ì˜ë¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    lines = file_content.split('\n')[start_line - 1:end_line]
    result: List[str] = []
    for index, line in enumerate(lines, start=start_line):
        if LINE_NUMBER_PATTERN.match(line):
            result.append(line)
        else:
            result.append(f"{index}: {line}")
    return '\n'.join(result)


def build_statement_name(node_type: str, start_line: int) -> str:
    """ë…¸ë“œ íƒ€ì…ê³¼ ì‹œì‘ ë¼ì¸ì„ ì¡°í•©í•œ ì‹ë³„ì ë¬¸ìì—´ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    return f"{node_type}[{start_line}]"


def escape_summary(summary: str) -> str:
    """LLM ìš”ì•½ ë¬¸ìì—´ì„ JSON-safe í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    return json.dumps(summary)


# ==================== RuleLoader í—¬í¼ ====================
def _rule_loader() -> RuleLoader:
    return RuleLoader(target_lang="dbms")


def analyze_code(code: str, ranges: list, count: int, api_key: str, locale: str) -> Dict[str, Any]:
    return _rule_loader().execute(
        "analysis",
        {"code": code, "ranges": ranges, "count": count, "locale": locale},
        api_key,
    )


def analyze_dml_tables(code: str, ranges: list, api_key: str, locale: str) -> Dict[str, Any]:
    return _rule_loader().execute(
        "dml",
        {"code": code, "ranges": ranges, "locale": locale},
        api_key,
    )


def analyze_summary_only(summaries: dict, api_key: str, locale: str, previous_summary: str = "") -> Dict[str, Any]:
    """í”„ë¡œì‹œì €/í•¨ìˆ˜ ì „ì²´ ìš”ì•½ ìƒì„± (Summaryë§Œ).
    
    Args:
        summaries: í•˜ìœ„ ë¸”ë¡ë“¤ì˜ ìš”ì•½ ë”•ì…”ë„ˆë¦¬
        ì˜ˆ: {"SELECT_10_12": "ì£¼ë¬¸ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤", "IF_14_18": "ì£¼ë¬¸ ìƒíƒœê°€ 'ì™„ë£Œ'ì´ë©´ í¬ì¸íŠ¸ë¥¼ ì ë¦½í•©ë‹ˆë‹¤"}
        ë˜ëŠ” ì´ì „ ì²­í¬ì˜ summary ë¬¸ìì—´
    """
    return _rule_loader().execute(
        "procedure_summary_only",
        {"summaries": summaries, "locale": locale, "previous_summary": previous_summary},
        api_key,
    )


def analyze_user_story(summary: str, api_key: str, locale: str) -> Dict[str, Any]:
    """í”„ë¡œì‹œì €/í•¨ìˆ˜ User Story + AC ìƒì„±.
    
    Args:
        summary: í”„ë¡œì‹œì €/í•¨ìˆ˜ì˜ ìƒì„¸ ìš”ì•½ (ë¬¸ìì—´)
        api_key: LLM API í‚¤
        locale: ì¶œë ¥ ì–¸ì–´
    """
    return _rule_loader().execute(
        "procedure_user_story",
        {"summary": summary, "locale": locale},
        api_key,
    )


def summarize_table_metadata(
    table_name: str,
    table_sentences: list,
    column_sentences: dict,
    column_metadata: dict,
    api_key: str,
    locale: str,
) -> Dict[str, Any]:
    return _rule_loader().execute(
        "table_summary",
        {
            "table_name": table_name,
            "table_sentences": table_sentences,
            "column_sentences": column_sentences,
            "column_metadata": column_metadata,
            "locale": locale,
        },
        api_key,
    )


def analyze_variables(declaration_code: str, api_key: str, locale: str) -> Dict[str, Any]:
    return _rule_loader().execute(
        "variables",
        {"declaration_code": declaration_code, "locale": locale},
        api_key,
    )


# ==================== ë…¸ë“œ ìˆ˜ì§‘ê¸° ====================
class StatementCollector:
    """ASTë¥¼ í›„ìœ„ìˆœíšŒí•˜ì—¬ `StatementNode`ì™€ í”„ë¡œì‹œì € ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    def __init__(self, antlr_data: Dict[str, Any], file_content: str, directory: str, file_name: str):
        """ìˆ˜ì§‘ê¸°ì— í•„ìš”í•œ AST ë°ì´í„°ì™€ íŒŒì¼ ë©”íƒ€ ì •ë³´ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.antlr_data = antlr_data
        self.file_content = file_content
        self.directory = directory
        self.file_name = file_name
        self.nodes: List[StatementNode] = []
        self.procedures: Dict[str, ProcedureInfo] = {}
        self._node_id = 0
        self._file_lines = file_content.split('\n')

    def collect(self) -> Tuple[List[StatementNode], Dict[str, ProcedureInfo]]:
        """AST ì „ì—­ì„ í›„ìœ„ ìˆœíšŒí•˜ì—¬ ë…¸ë“œ ëª©ë¡ê³¼ í”„ë¡œì‹œì € ì •ë³´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        # ë£¨íŠ¸ ë…¸ë“œë¶€í„° í›„ìœ„ìˆœíšŒí•©ë‹ˆë‹¤ (ìì‹ â†’ ë¶€ëª¨ ìˆœì„œ ë³´ì¥)
        self._visit(self.antlr_data, current_proc=None, current_type=None, current_schema=None)
        return self.nodes, self.procedures

    def _make_proc_key(self, procedure_name: Optional[str], start_line: int) -> str:
        """í”„ë¡œì‹œì € ê³ ìœ í‚¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        base = procedure_name or f"anonymous_{start_line}"
        return f"{self.directory}:{self.file_name}:{base}:{start_line}"

    def _visit(
        self,
        node: Dict[str, Any],
        current_proc: Optional[str],
        current_type: Optional[str],
        current_schema: Optional[str],
    ) -> Optional[StatementNode]:
        """ì¬ê·€ì ìœ¼ë¡œ ASTë¥¼ ë‚´ë ¤ê°€ë©° StatementNodeë¥¼ ìƒì„±í•˜ê³  ë¶€ëª¨-ìì‹ ê´€ê³„ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤."""
        # ê° ë…¸ë“œì˜ ê¸°ë³¸ ë©”íƒ€ë°ì´í„°ë¥¼ í™•ë³´í•©ë‹ˆë‹¤.
        start_line = node['startLine']
        end_line = node['endLine']
        node_type = node['type']
        children = node.get('children', []) or []

        child_nodes: List[StatementNode] = []
        procedure_key = current_proc
        procedure_type = current_type
        schema_name = current_schema

        # LLM ì…ë ¥ ë° ìš”ì•½ ìƒì„±ì— í™œìš©í•  ì›ë³¸ ì½”ë“œë¥¼ ë¼ì¸ ë‹¨ìœ„ë¡œ ì¤€ë¹„í•©ë‹ˆë‹¤.
        line_entries = [
            (line_no, self._file_lines[line_no - 1] if 0 <= line_no - 1 < len(self._file_lines) else '')
            for line_no in range(start_line, end_line + 1)
        ]
        code = '\n'.join(f"{line_no}: {text}" for line_no, text in line_entries)

        if node_type in PROCEDURE_TYPES:
            # í”„ë¡œì‹œì €/í•¨ìˆ˜ ë£¨íŠ¸ë¼ë©´ ì´ë¦„/ìŠ¤í‚¤ë§ˆë¥¼ ì¶”ì¶œí•˜ì—¬ ë³„ë„ ë²„í‚·ì„ ë§Œë“­ë‹ˆë‹¤.
            # ìƒì„±ëœ procedure_keyëŠ” í•˜ìœ„ ë…¸ë“œì™€ ìš”ì•½ ê²°ê³¼ë¥¼ ë¬¶ëŠ” ê¸°ì¤€ í‚¤ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
            schema_candidate, name_candidate = get_procedure_name_from_code(code)
            procedure_key = self._make_proc_key(name_candidate, start_line)
            procedure_type = node_type
            schema_name = schema_candidate
            if procedure_key not in self.procedures:
                self.procedures[procedure_key] = ProcedureInfo(
                    key=procedure_key,
                    procedure_type=node_type,
                    procedure_name=name_candidate or procedure_key,
                    schema_name=schema_candidate,
                    start_line=start_line,
                    end_line=end_line,
                )
                proc_name_log = name_candidate or procedure_key
                log_process("ANALYZE", "COLLECT", f"ğŸ“‹ í”„ë¡œì‹œì € ì„ ì–¸ ë°œê²¬: {proc_name_log} (ë¼ì¸ {start_line}~{end_line})")

        for child in children:
            child_node = self._visit(child, procedure_key, procedure_type, schema_name)
            if child_node is not None:
                child_nodes.append(child_node)

        # í›„ì† ë‹¨ê³„ì—ì„œ í™œìš©í•  ë¶„ì„ ê°€ëŠ¥ ì—¬ë¶€ ë° í† í° ì •ë³´ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        analyzable = node_type not in NON_ANALYSIS_TYPES
        token = calculate_code_token(code)
        dml = node_type in DML_STATEMENT_TYPES
        has_children = bool(child_nodes)

        self._node_id += 1
        statement_node = StatementNode(
            node_id=self._node_id,
            start_line=start_line,
            end_line=end_line,
            node_type=node_type,
            code=code,
            token=token,
            has_children=has_children,
            procedure_key=procedure_key,
            procedure_type=procedure_type,
            procedure_name=self.procedures.get(procedure_key).procedure_name if procedure_key in self.procedures else None,
            schema_name=schema_name,
            analyzable=analyzable,
            dml=dml,
            lines=line_entries,
        )
        for child_node in child_nodes:
            child_node.parent = statement_node
        statement_node.children.extend(child_nodes)

        # í”„ë¡œì‹œì € ìš”ì•½ ì™„ë£Œ ì‹œì ì„ íŒë³„í•˜ê¸° ìœ„í•´ pending ë…¸ë“œ ìˆ˜ë¥¼ ì¶”ì í•©ë‹ˆë‹¤.
        if analyzable and procedure_key and procedure_key in self.procedures:
            self.procedures[procedure_key].pending_nodes += 1
        else:
            statement_node.completion_event.set()

        self.nodes.append(statement_node)
        log_process("ANALYZE", "COLLECT", f"âœ… {node_type} ë…¸ë“œ ìˆ˜ì§‘ ì™„ë£Œ: ë¼ì¸ {start_line}~{end_line}, í† í° {token}, ìì‹ {len(child_nodes)}ê°œ")
        return statement_node


# ==================== ë°°ì¹˜ í”Œë˜ë„ˆ ====================
class BatchPlanner:
    """ìˆ˜ì§‘ëœ ë…¸ë“œë¥¼ í† í° í•œë„ ë‚´ì—ì„œ ë°°ì¹˜ë¡œ ë¬¶ìŠµë‹ˆë‹¤."""
    def __init__(self, token_limit: int = MAX_BATCH_TOKEN):
        """í† í° í•œë„ë¥¼ ì§€ì •í•˜ì—¬ ë°°ì¹˜ ìƒì„±ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.token_limit = token_limit

    def plan(self, nodes: List[StatementNode], system_file: str) -> List[AnalysisBatch]:
        """í† í° í•œë„ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ë…¸ë“œë¥¼ ë¶„í• í•˜ì—¬ ë¶„ì„ ë°°ì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        batches: List[AnalysisBatch] = []
        current_nodes: List[StatementNode] = []
        current_tokens = 0
        batch_id = 1

        for node in nodes:
            if not node.analyzable:
                continue

            # ë¶€ëª¨ ë…¸ë“œëŠ” ìì‹ ìš”ì•½ì´ ì¤€ë¹„ëœ í›„ ë‹¨ë…ìœ¼ë¡œ ì‹¤í–‰ë˜ë¯€ë¡œ ì¦‰ì‹œ ë°°ì¹˜ë¥¼ í™•ì •í•©ë‹ˆë‹¤.
            if node.has_children:
                # ë¶€ëª¨ ë…¸ë“œëŠ” ìì‹ ìš”ì•½ì´ ëª¨ë‘ ì¤€ë¹„ëœ ìƒíƒœì—ì„œ ë‹¨ë…ìœ¼ë¡œ LLMì— ì „ë‹¬í•©ë‹ˆë‹¤.
                if current_nodes:
                    # í˜„ì¬ê¹Œì§€ ëˆ„ì ëœ ë¦¬í”„ ë°°ì¹˜ë¥¼ ë¨¼ì € í™•ì •í•©ë‹ˆë‹¤.
                    log_process("ANALYZE", "BATCH", f"ğŸ“¦ ë°°ì¹˜ #{batch_id} í™•ì •: ë¦¬í”„ ë…¸ë“œ {len(current_nodes)}ê°œ (í† í° {current_tokens}/{self.token_limit})")
                    batches.append(self._create_batch(batch_id, current_nodes))
                    batch_id += 1
                    current_nodes = []
                    current_tokens = 0

                log_process("ANALYZE", "BATCH", f"ğŸ“¦ ë°°ì¹˜ #{batch_id} í™•ì •: ë¶€ëª¨ ë…¸ë“œ ë‹¨ë… ì‹¤í–‰ (ë¼ì¸ {node.start_line}~{node.end_line}, í† í° {node.token})")
                batches.append(self._create_batch(batch_id, [node]))
                batch_id += 1
                continue

            # í˜„ì¬ ë°°ì¹˜ê°€ í† í° í•œë„ë¥¼ ì´ˆê³¼í•œë‹¤ë©´ ìŒ“ì¸ ë¦¬í”„ ë…¸ë“œë“¤ì„ ë¨¼ì € ì‹¤í–‰í•©ë‹ˆë‹¤.
            if current_nodes and current_tokens + node.token > self.token_limit:
                # í† í° í•œë„ë¥¼ ì´ˆê³¼í•˜ê¸° ì§ì „ ë°°ì¹˜ë¥¼ í™•ì •í•©ë‹ˆë‹¤.
                log_process("ANALYZE", "BATCH", f"ğŸ“¦ ë°°ì¹˜ #{batch_id} í™•ì •: í† í° í•œë„ ë„ë‹¬ë¡œ ì„  ì‹¤í–‰ (ëˆ„ì  {current_tokens}/{self.token_limit})")
                batches.append(self._create_batch(batch_id, current_nodes))
                batch_id += 1
                current_nodes = []
                current_tokens = 0

            current_nodes.append(node)
            current_tokens += node.token

        if current_nodes:
            # ë‚¨ì•„ ìˆëŠ” ë…¸ë“œê°€ ìˆìœ¼ë©´ ë§ˆë¬´ë¦¬ ë°°ì¹˜ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.
            log_process("ANALYZE", "BATCH", f"ğŸ“¦ ë°°ì¹˜ #{batch_id} í™•ì •: ë§ˆì§€ë§‰ ë¦¬í”„ ë…¸ë“œ {len(current_nodes)}ê°œ (í† í° {current_tokens}/{self.token_limit})")
            batches.append(self._create_batch(batch_id, current_nodes))

        return batches

    def _create_batch(self, batch_id: int, nodes: List[StatementNode]) -> AnalysisBatch:
        """ë°°ì¹˜ IDì™€ ë…¸ë“œ ë¦¬ìŠ¤íŠ¸ë¡œ AnalysisBatch ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        # LLM í˜¸ì¶œê³¼ ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•´ ë²”ìœ„ ì •ë³´ë¥¼ ë¯¸ë¦¬ ê³„ì‚°í•©ë‹ˆë‹¤.
        ranges = [{"startLine": node.start_line, "endLine": node.end_line} for node in nodes]
        dml_ranges = [
            {"startLine": node.start_line, "endLine": node.end_line, "type": node.node_type}
            for node in nodes
            if node.dml
        ]
        # ì§„í–‰ë¥  í‘œì‹œëŠ” ë°°ì¹˜ ë‚´ ê°€ì¥ ë§ˆì§€ë§‰ ë¼ì¸ ê¸°ì¤€ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        progress_line = max(node.end_line for node in nodes)
        return AnalysisBatch(
            batch_id=batch_id,
            nodes=nodes,
            ranges=ranges,
            dml_ranges=dml_ranges,
            progress_line=progress_line,
        )


# ==================== LLM í˜¸ì¶œ ====================
class LLMInvoker:
    """ë°°ì¹˜ë¥¼ ì…ë ¥ ë°›ì•„ ì¼ë°˜ ìš”ì•½/DML ë©”íƒ€ ë¶„ì„ì„ ë³‘ë ¬ í˜¸ì¶œí•©ë‹ˆë‹¤."""
    def __init__(self, api_key: str, locale: str):
        """í˜¸ì¶œì— ì‚¬ìš©í•  API í‚¤ì™€ ë¡œì¼€ì¼ì„ ë³´ê´€í•©ë‹ˆë‹¤."""
        self.api_key = api_key
        self.locale = locale

    async def invoke(self, batch: AnalysisBatch) -> Tuple[Optional[Dict[str, Any]], Optional[Dict[str, Any]]]:
        """ë°°ì¹˜ì— í¬í•¨ëœ ë²”ìœ„ë¥¼ ì¼ë°˜ LLM/í…Œì´ë¸” LLMì— ê°ê° ì „ë‹¬í•©ë‹ˆë‹¤."""
        general_task = None
        if batch.ranges:
            # ì¼ë°˜ ìš”ì•½ì€ ë…¸ë“œ compact codeë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë™ê¸°ì‹ í˜¸ì¶œì„ ìŠ¤ë ˆë“œë¡œ ìœ„ì„í•©ë‹ˆë‹¤.
            general_task = asyncio.to_thread(
                analyze_code,
                batch.build_general_payload(),
                batch.ranges,
                len(batch.ranges),
                self.api_key,
                self.locale,
            )

        table_task = None
        dml_payload = batch.build_dml_payload()
        if dml_payload and batch.dml_ranges:
            # DML ë¶„ì„ì€ ë³„ë„ì˜ í”„ë¡¬í”„íŠ¸ë¡œ ë³‘ë ¬ ì‹¤í–‰í•˜ì—¬ í…Œì´ë¸” ë©”íƒ€ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
            table_task = asyncio.to_thread(
                analyze_dml_tables,
                dml_payload,
                batch.dml_ranges,
                self.api_key,
                self.locale,
            )

        if general_task and table_task:
            return await asyncio.gather(general_task, table_task)
        if general_task:
            return await general_task, None
        if table_task:
            return None, await table_task
        # ë¶„ì„í•  ëŒ€ìƒì´ ì—†ìœ¼ë©´ ì˜ˆì™¸ ë°œìƒ
        raise AnalysisError("LLM ë¶„ì„ ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤ (ì¼ë°˜ ë¶„ì„ ë° í…Œì´ë¸” ë¶„ì„ ëª¨ë‘ ì—†ìŒ)")


# ==================== AST í”„ë¡œì„¸ì„œ ë³¸ì²´ ====================
class DbmsAstProcessor:
    """DBMS AST ì²˜ë¦¬ ë° LLM ë¶„ì„ íŒŒì´í”„ë¼ì¸
    
    2ë‹¨ê³„ ë¶„ì„ ì§€ì› (Frameworkì™€ ë™ì¼):
    - Phase 1: build_static_graph_queries() - ì •ì  ê·¸ë˜í”„ ì¿¼ë¦¬ ìƒì„±
    - Phase 2: run_llm_analysis() - LLM ë¶„ì„ í›„ ì—…ë°ì´íŠ¸ ì¿¼ë¦¬ ìƒì„±
    """
    def __init__(
        self,
        antlr_data: dict,
        file_content: str,
        directory: str,
        file_name: str,
        user_id: str,
        api_key: str,
        locale: str,
        dbms: str,
        project_name: str,
        last_line: int,
        default_schema: str = "public",
        ddl_table_metadata: Optional[Dict[Tuple[str, str], Dict[str, Any]]] = None,
    ):
        """Analyzerê°€ íŒŒì¼ ë¶„ì„ì— í•„ìš”í•œ ëª¨ë“  ì»¨í…ìŠ¤íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.antlr_data = antlr_data
        self.file_content = file_content
        self.last_line = last_line
        # Windows ê²½ë¡œ êµ¬ë¶„ì(\\)ë¥¼ /ë¡œ ë³€í™˜í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€
        normalized_dir = directory.replace('\\', '/') if directory else ''
        self.directory = normalized_dir
        self.file_name = file_name
        self.user_id = user_id
        self.api_key = api_key
        self.locale = locale
        self.dbms = (dbms or 'postgres').lower()
        self.project_name = project_name or ''
        self.default_schema = default_schema  # ìŠ¤í‚¤ë§ˆ ë¯¸ì‹ë³„ ì‹œ ì‚¬ìš©í•  ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ
        self._ddl_table_metadata = ddl_table_metadata or {}  # DDL ë©”íƒ€ë°ì´í„° ìºì‹œ (ë©”ëª¨ë¦¬)
        # full_directory: ë””ë ‰í† ë¦¬ + íŒŒì¼ëª… (Neo4j directory ì†ì„±ìœ¼ë¡œ ì‚¬ìš©)
        self.full_directory = f"{normalized_dir}/{file_name}" if normalized_dir else file_name

        self.node_base_props = (
            f"directory: '{escape_for_cypher(self.full_directory)}', file_name: '{file_name}', user_id: '{user_id}', project_name: '{self.project_name}'"
        )
        self.table_base_props = f"user_id: '{user_id}'"
        self.max_workers = MAX_CONCURRENCY
        self.file_last_line = last_line
        
        # AST ìˆ˜ì§‘ ê²°ê³¼ ìºì‹œ (Phase 1ì—ì„œ ìˆ˜ì§‘, Phase 2ì—ì„œ ì‚¬ìš©)
        self._nodes: Optional[List[StatementNode]] = None
        self._procedures: Optional[Dict[str, ProcedureInfo]] = None
        
        # í…Œì´ë¸”/ì»¬ëŸ¼ ì„¤ëª… ìš”ì•½ìš© ì €ì¥ì†Œ (DML ë¶„ì„ì—ì„œ ìˆ˜ì§‘)
        self._table_summary_store: Dict[Tuple[str, str], Dict[str, Any]] = {}

    # =========================================================================
    # Phase 1: ì •ì  ê·¸ë˜í”„ ì¿¼ë¦¬ ìƒì„±
    # =========================================================================
    
    def build_static_graph_queries(self) -> List[str]:
        """[Phase 1] ASTë¥¼ ìˆ˜ì§‘í•˜ê³  ì •ì  ê·¸ë˜í”„ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Returns:
            ì •ì  ë…¸ë“œ ë° ê´€ê³„ ìƒì„± ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        log_process("ANALYZE", "PHASE1", f"ğŸ—ï¸ {self.full_directory} ì •ì  ê·¸ë˜í”„ ìƒì„±")
        
        # AST ìˆ˜ì§‘
        collector = StatementCollector(
            self.antlr_data, self.file_content, self.directory, self.file_name
        )
        self._nodes, self._procedures = collector.collect()
        
        if not self._nodes:
            log_process("ANALYZE", "PHASE1", f"âš ï¸ {self.full_directory}: ë¶„ì„ ëŒ€ìƒ ë…¸ë“œ ì—†ìŒ")
            return []
        
        # ì •ì  ë…¸ë“œ ì¿¼ë¦¬ ìƒì„±
        queries: List[str] = []
        file_node = None
        for node in self._nodes:
            queries.extend(self._build_static_node_queries(node))
            if node.node_type == "FILE":
                file_node = node
        
        # Project â†’ File (CONTAINS) ê´€ê³„ ìƒì„±
        if file_node:
            queries.extend(self._build_project_file_relationship())
        
        # ê´€ê³„ ì¿¼ë¦¬ ìƒì„±
        queries.extend(self._build_relationship_queries())
        
        log_process("ANALYZE", "PHASE1", f"âœ… {self.full_directory}: {len(queries)}ê°œ ì¿¼ë¦¬ ìƒì„±")
        return queries

    def _build_static_node_queries(self, node: StatementNode) -> List[str]:
        """ì •ì  ë…¸ë“œ ìƒì„± ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        queries: List[str] = []
        label = node.node_type
        
        # name ì†ì„± ê²°ì •: PROCEDURE/FUNCTIONëŠ” ì‹¤ì œ ì´ë¦„, ê·¸ ì™¸ëŠ” íƒ€ì…[ë¼ì¸ë²ˆí˜¸]
        if label == "FILE":
            node_name = self.file_name
        elif label in PROCEDURE_TYPES and node.procedure_name:
            node_name = node.procedure_name
        else:
            node_name = f"{label}[{node.start_line}]"
        
        escaped_name = escape_for_cypher(node_name)
        has_children = "true" if node.has_children else "false"
        escaped_code = escape_for_cypher(node.code)
        
        base_set = [
            f"n.endLine = {node.end_line}",
            f"n.name = '{escaped_name}'",
            f"n.node_code = '{escaped_code}'",
            f"n.token = {node.token}",
            f"n.has_children = {has_children}",
        ]
        
        # PROCEDURE/FUNCTION: procedure_name, schema_name, procedure_type ì†ì„± ì¶”ê°€
        if label in PROCEDURE_TYPES and node.procedure_name:
            base_set.append(f"n.procedure_name = '{escape_for_cypher(node.procedure_name)}'")
            base_set.append(f"n.procedure_type = '{label}'")
            if node.schema_name:
                base_set.append(f"n.schema_name = '{escape_for_cypher(node.schema_name)}'")
        # ê·¸ ì™¸ ë…¸ë“œ: ì†Œì† í”„ë¡œì‹œì € ì •ë³´ ì €ì¥
        elif node.procedure_name:
            base_set.append(f"n.procedure_name = '{escape_for_cypher(node.procedure_name)}'")
            if node.schema_name:
                base_set.append(f"n.schema_name = '{escape_for_cypher(node.schema_name)}'")
        
        if node.has_children:
            escaped_placeholder = escape_for_cypher(node.get_placeholder_code())
            base_set.append(f"n.summarized_code = '{escaped_placeholder}'")
        
        base_set_str = ", ".join(base_set)
        
        # PROCEDURE/FUNCTION ë…¸ë“œ: MERGEë¡œ ìƒì„± (ì¤‘ë³µ ë°©ì§€)
        if label in PROCEDURE_TYPES and node.procedure_name:
            escaped_proc_name = escape_for_cypher(node.procedure_name)
            escaped_schema = escape_for_cypher(node.schema_name or "")
            schema_match = f"schema_name: '{escaped_schema}', " if node.schema_name else ""
            queries.append(
                f"MERGE (n:{label} {{{schema_match}procedure_name: '{escaped_proc_name}', user_id: '{self.user_id}', project_name: '{self.project_name}'}})\n"
                f"SET n.startLine = {node.start_line}, n.directory = '{escape_for_cypher(self.full_directory)}', n.file_name = '{self.file_name}', {base_set_str}\n"
                f"RETURN n"
            )
        else:
            queries.append(
                f"MERGE (n:{label} {{startLine: {node.start_line}, {self.node_base_props}}})\n"
                f"SET {base_set_str}\n"
                f"RETURN n"
            )
        return queries

    def _build_project_file_relationship(self) -> List[str]:
        """Project â†’ File (CONTAINS) ê´€ê³„ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        escaped_file_name = escape_for_cypher(self.file_name)
        escaped_dir = escape_for_cypher(self.full_directory)
        return [
            f"MATCH (p:Project {{user_id: '{self.user_id}', name: '{escape_for_cypher(self.project_name)}'}})\n"
            f"MATCH (f:FILE {{startLine: 1, directory: '{escaped_dir}', file_name: '{escaped_file_name}', user_id: '{self.user_id}', project_name: '{self.project_name}'}})\n"
            f"MERGE (p)-[r:CONTAINS]->(f)\n"
            f"RETURN r"
        ]

    def _build_relationship_queries(self) -> List[str]:
        """ì •ì  ê´€ê³„ ì¿¼ë¦¬ (CONTAINS, PARENT_OF, NEXT)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        ê·œì¹™:
        - File â†’ PROCEDURE/FUNCTION/TRIGGER (ìµœìƒìœ„ íƒ€ì…ë§Œ): CONTAINS
        - ê·¸ ì™¸ ë¶€ëª¨-ìì‹: PARENT_OF
        - í˜•ì œ ê´€ê³„: NEXT
        """
        queries: List[str] = []
        
        for node in self._nodes or []:
            # File â†’ ìµœìƒìœ„ íƒ€ì…(PROCEDURE/FUNCTION/TRIGGER)ë§Œ CONTAINS, ê·¸ ì™¸: PARENT_OF
            for child in node.children:
                if node.node_type == "FILE" and child.node_type in PROCEDURE_TYPES:
                    queries.append(self._build_contains_query(node, child))
                else:
                    queries.append(self._build_parent_of_query(node, child))
            
            # NEXT ê´€ê³„
            prev = None
            for child in node.children:
                if prev:
                    queries.append(self._build_next_query(prev, child))
                prev = child
        
        return queries
    
    def _build_contains_query(self, parent: StatementNode, child: StatementNode) -> str:
        """CONTAINS ê´€ê³„ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (File â†’ ì§ì ‘ ìì‹ë§Œ)."""
        return (
            f"MATCH (parent:{parent.node_type} {{startLine: {parent.start_line}, {self.node_base_props}}})\n"
            f"MATCH (child:{child.node_type} {{startLine: {child.start_line}, {self.node_base_props}}})\n"
            f"MERGE (parent)-[r:CONTAINS]->(child)\n"
            f"RETURN r"
        )

    # =========================================================================
    # Phase 2: LLM ë¶„ì„
    # =========================================================================
    
    async def run_llm_analysis(self) -> Tuple[List[str], int, List[Dict[str, Any]]]:
        """[Phase 2] LLM ë¶„ì„ì„ ì‹¤í–‰í•˜ê³  ì—…ë°ì´íŠ¸ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        ì¤‘ìš”: ìì‹â†’ë¶€ëª¨ ìš”ì•½ ì˜ì¡´ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•´ completion_event ê¸°ë°˜ ëŒ€ê¸°
        - ë¶€ëª¨ ë…¸ë“œëŠ” ìì‹ ë…¸ë“œì˜ completion_eventë¥¼ ê¸°ë‹¤ë¦° í›„ ì‹¤í–‰
        - leaf ë…¸ë“œëŠ” ë°”ë¡œ ì‹¤í–‰, parent ë…¸ë“œëŠ” ìì‹ ì™„ë£Œ í›„ ì‹¤í–‰
        
        Returns:
            (ë¶„ì„ ê²°ê³¼ ì—…ë°ì´íŠ¸ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸, ì‹¤íŒ¨í•œ ë°°ì¹˜ ìˆ˜, ì‹¤íŒ¨ ìƒì„¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸)
        """
        if self._nodes is None:
            raise AnalysisError(f"Phase 1ì´ ë¨¼ì € ì‹¤í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤: {self.file_name}")
        
        log_process("ANALYZE", "PHASE2", f"ğŸ¤– {self.full_directory} LLM ë¶„ì„ ì‹œì‘")
        
        all_queries: List[str] = []
        failed_batch_count = 0
        all_failed_details: List[Dict[str, Any]] = []
        
        # ë³€ìˆ˜ ì„ í–‰ ì²˜ë¦¬
        variable_queries = await self._analyze_variable_nodes()
        all_queries.extend(variable_queries)
        
        # ë°°ì¹˜ ë¶„ì„
        planner = BatchPlanner()
        batches = planner.plan(self._nodes, self.full_directory)
        
        if not batches:
            log_process("ANALYZE", "PHASE2", f"âš ï¸ {self.full_directory}: ë¶„ì„ ëŒ€ìƒ ë°°ì¹˜ ì—†ìŒ")
            return all_queries, 0, []
        
        log_process("ANALYZE", "PHASE2", f"ğŸ“Š ë°°ì¹˜ {len(batches)}ê°œ (completion_event ê¸°ë°˜ ì˜ì¡´ì„± ë³´ì¥)")
        
        # í”„ë¡œì‹œì €ë³„ summary ìˆ˜ì§‘ìš© ì €ì¥ì†Œ (ë°°ì¹˜ ì²˜ë¦¬ ì „ì— ì´ˆê¸°í™”)
        procedure_summary_store: Dict[str, Dict[str, str]] = {key: {} for key in (self._procedures or {})}
        
        # LLM í˜¸ì¶œ ë° ê²°ê³¼ ì²˜ë¦¬
        invoker = LLMInvoker(self.api_key, self.locale)
        
        async def process_batch(batch: AnalysisBatch, semaphore: asyncio.Semaphore) -> Tuple[List[str], Dict[str, Any]]:
            """ë°°ì¹˜ ì²˜ë¦¬ í›„ ì¿¼ë¦¬ì™€ ë¶„ì„ ê²°ê³¼ ë°˜í™˜. ë…¸ë“œì— summaryë„ ì„¤ì •.
            
            í•µì‹¬: ë¶€ëª¨ ë…¸ë“œëŠ” ìì‹ completion_eventë¥¼ ê¸°ë‹¤ë¦° í›„ ì‹¤í–‰ë¨
            â†’ ê¹Šì´ ê³„ì‚° ì—†ì´ ìì—°ìŠ¤ëŸ½ê²Œ leaf â†’ parent ìˆœì„œ ë³´ì¥
            
            ì¤‘ìš”: 
            - try/finallyë¡œ completion_event.set()ì„ ë³´ì¥í•˜ì—¬ ë°ë“œë½ ë°©ì§€
            - ìì‹ ì¤‘ ok=Falseê°€ ìˆìœ¼ë©´ ë¶€ëª¨ë„ ok=False (ë¶ˆì™„ì „ ìš”ì•½ ì „íŒŒ)
            """
            batch_failed = False
            async with semaphore:
                try:
                    # 1. ë°°ì¹˜ ë‚´ ëª¨ë“  ë…¸ë“œì˜ ìì‹ ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¼ (ê¸°ì¡´ ë°©ì‹ ë³µì›)
                    for node in batch.nodes:
                        if node.has_children:
                            for child in node.children:
                                await child.completion_event.wait()
                                # ìì‹ ì¤‘ í•˜ë‚˜ë¼ë„ ì‹¤íŒ¨í•˜ë©´ ë¶€ëª¨ë„ ë¶ˆì™„ì „
                                if not child.ok:
                                    node.ok = False
                    
                    log_process("ANALYZE", "LLM", f"ë°°ì¹˜ #{batch.batch_id} ì²˜ë¦¬ ì¤‘ ({len(batch.nodes)}ê°œ ë…¸ë“œ)")
                    general_result, table_result = await invoker.invoke(batch)
                    
                    # 2. ë…¸ë“œì— summary ì„¤ì •
                    if general_result:
                        analysis_list = general_result.get("analysis") or []
                        for node, analysis in zip(batch.nodes, analysis_list):
                            if analysis:
                                node.summary = analysis.get("summary") or ""
                    
                    queries = self._build_analysis_queries(batch, general_result, table_result, procedure_summary_store)
                    return queries, {"batch": batch, "general_result": general_result}
                except Exception:
                    # ë°°ì¹˜ ì‹¤íŒ¨ ì‹œ ëª¨ë“  ë…¸ë“œë¥¼ ok=Falseë¡œ ë§ˆí‚¹
                    batch_failed = True
                    for node in batch.nodes:
                        node.ok = False
                    raise
                finally:
                    # 3. ë¬´ì¡°ê±´ completion_event ì„¤ì • (ì‹¤íŒ¨í•´ë„ ë¶€ëª¨ê°€ ëŒ€ê¸°í•˜ì§€ ì•Šë„ë¡)
                    for node in batch.nodes:
                        node.completion_event.set()
        
        def collect_results(batch_results: list, batches_list: List[AnalysisBatch], level_name: str) -> Tuple[int, List[Dict[str, Any]]]:
            """ë°°ì¹˜ ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•˜ê³  (ì‹¤íŒ¨ ìˆ˜, ì‹¤íŒ¨ ìƒì„¸ ì •ë³´) ë°˜í™˜."""
            nonlocal all_queries
            fail_count = 0
            failed_details: List[Dict[str, Any]] = []
            
            for i, result in enumerate(batch_results):
                if isinstance(result, Exception):
                    batch = batches_list[i] if i < len(batches_list) else None
                    batch_id = batch.batch_id if batch else i
                    node_ranges = ", ".join(f"L{n.start_line}-{n.end_line}" for n in batch.nodes) if batch else "unknown"
                    error_msg = str(result)[:100]  # ìµœëŒ€ 100ì
                    
                    log_process("ANALYZE", "ERROR", f"[{level_name}] ë°°ì¹˜ #{batch_id} ì‹¤íŒ¨ ({node_ranges}): {error_msg}", logging.ERROR)
                    fail_count += 1
                    failed_details.append({
                        "batch_id": batch_id,
                        "node_ranges": node_ranges,
                        "error": error_msg
                    })
                else:
                    queries, _ = result
                    all_queries.extend(queries)
            return fail_count, failed_details
        
        # ëª¨ë“  ë°°ì¹˜ ë³‘ë ¬ ì‹¤í–‰ (completion_eventê°€ ìˆœì„œ ë³´ì¥)
        # - leaf ë°°ì¹˜: ìì‹ì´ ì—†ìœ¼ë¯€ë¡œ ë°”ë¡œ ì‹¤í–‰
        # - parent ë°°ì¹˜: ìì‹ completion_event.wait() í›„ ì‹¤í–‰ â†’ ìì—°ìŠ¤ëŸ½ê²Œ ìˆœì„œ ë³´ì¥
        semaphore = asyncio.Semaphore(min(self.max_workers, len(batches)))
        batch_results = await asyncio.gather(
            *[process_batch(b, semaphore) for b in batches],
            return_exceptions=True
        )
        fail_count, failed_details = collect_results(batch_results, batches, "LLM")
        failed_batch_count += fail_count
        all_failed_details.extend(failed_details)
        
        # í”„ë¡œì‹œì €ë³„ summary ì²˜ë¦¬
        if self._procedures:
            proc_queries = await self._process_procedure_summaries(procedure_summary_store)
            all_queries.extend(proc_queries)
        
        # í…Œì´ë¸”/ì»¬ëŸ¼ ì„¤ëª… ìš”ì•½ ì²˜ë¦¬
        table_queries = await self._finalize_table_summaries()
        all_queries.extend(table_queries)
        
        # ì‹¤íŒ¨ í†µê³„ ë¡œê¹…
        if failed_batch_count > 0:
            log_process("ANALYZE", "PHASE2", f"âš ï¸ {self.full_directory}: {failed_batch_count}ê°œ ë°°ì¹˜ ì‹¤íŒ¨", logging.WARNING)
        
        log_process("ANALYZE", "PHASE2", f"âœ… {self.full_directory}: {len(all_queries)}ê°œ ì—…ë°ì´íŠ¸ ì¿¼ë¦¬")
        return all_queries, failed_batch_count, all_failed_details

    async def _analyze_variable_nodes(self) -> List[str]:
        """ë³€ìˆ˜ ì„ ì–¸ ë…¸ë“œë¥¼ ë¶„ì„í•˜ê³  ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        queries: List[str] = []
        variable_nodes = [n for n in (self._nodes or []) if n.node_type in VARIABLE_DECLARATION_TYPES]
        
        if not variable_nodes:
            return queries
        
        semaphore = asyncio.Semaphore(VARIABLE_CONCURRENCY)
        
        async def analyze_one(node: StatementNode) -> List[str]:
            async with semaphore:
                try:
                    result = await asyncio.to_thread(
                        analyze_variables, node.code, self.api_key, self.locale
                    )
                    return self._build_variable_queries(node, result)
                except Exception as e:
                    log_process("ANALYZE", "VARIABLE", f"âŒ ë³€ìˆ˜ ë¶„ì„ ì‹¤íŒ¨ (node={node.start_line}): {e}", logging.ERROR, e)
                    return []
        
        results = await asyncio.gather(*[analyze_one(n) for n in variable_nodes])
        for r in results:
            queries.extend(r)
        
        return queries

    def _build_variable_queries(self, node: StatementNode, result: Dict[str, Any]) -> List[str]:
        """ë³€ìˆ˜ ë¶„ì„ ê²°ê³¼ë¥¼ Neo4j ì¿¼ë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        queries: List[str] = []
        
        if not isinstance(result, dict):
            return queries
        
        variables = result.get("variables") or []
        if not variables:
            return queries
        
        node_match = (
            f"MATCH (n:{node.node_type} {{startLine: {node.start_line}, {self.node_base_props}}})"
        )
        
        for var in variables:
            var_name = var.get("name", "")
            var_type = var.get("type", "")
            var_role = var.get("role", "")
            var_desc = var.get("description", "")
            
            if not var_name:
                continue
            
            escaped_name = escape_for_cypher(var_name)
            escaped_type = escape_for_cypher(var_type)
            escaped_role = escape_for_cypher(VARIABLE_ROLE_MAP.get(var_role, var_role))
            escaped_desc = escape_for_cypher(var_desc)
            
            # ë³€ìˆ˜ ë…¸ë“œ ìƒì„± ë° ê´€ê³„ ì—°ê²°
            queries.append(
                f"{node_match}\n"
                f"MERGE (v:Variable {{name: '{escaped_name}', {self.node_base_props}}})\n"
                f"SET v.type = '{escaped_type}', v.role = '{escaped_role}', v.description = '{escaped_desc}'\n"
                f"MERGE (n)-[:DECLARES]->(v)\n"
                f"RETURN v"
            )
        
        return queries

    def _build_analysis_queries(
        self,
        batch: AnalysisBatch,
        general_result: Optional[Dict[str, Any]],
        table_result: Optional[Dict[str, Any]],
        procedure_summary_store: Optional[Dict[str, Dict[str, str]]] = None,
    ) -> List[str]:
        """LLM ë¶„ì„ ê²°ê³¼ë¥¼ MATCH ê¸°ë°˜ ì—…ë°ì´íŠ¸ ì¿¼ë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        ì²˜ë¦¬ í•­ëª©:
        - ë…¸ë“œ summary ì—…ë°ì´íŠ¸
        - CALL ê´€ê³„ ìƒì„± (internal/external scope)
        - ë³€ìˆ˜ ì‚¬ìš© ë§ˆí‚¹
        - í…Œì´ë¸”/ì»¬ëŸ¼/FK/DBLink ê´€ê³„ ìƒì„±
        """
        queries: List[str] = []
        
        # ì¼ë°˜ ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬
        if general_result:
            analysis_list = general_result.get("analysis") or []
            for node, analysis in zip(batch.nodes, analysis_list):
                if not analysis:
                    continue
                
                # 1) Summary ì—…ë°ì´íŠ¸
                summary = analysis.get("summary") or ""
                if summary:
                    escaped_summary = escape_for_cypher(str(summary))
                    escaped_code = escape_for_cypher(node.code)
                    node_name = build_statement_name(node.node_type, node.start_line)
                    escaped_node_name = escape_for_cypher(node_name)
                    
                    queries.append(
                        f"MATCH (n:{node.node_type} {{startLine: {node.start_line}, {self.node_base_props}}})\n"
                        f"SET n.endLine = {node.end_line}, n.name = '{escaped_node_name}', "
                        f"n.summary = '{escaped_summary}', n.node_code = '{escaped_code}', "
                        f"n.token = {node.token}, n.procedure_name = '{escape_for_cypher(node.procedure_name or '')}', "
                        f"n.has_children = {'true' if node.has_children else 'false'}\n"
                        f"RETURN n"
                    )
                    
                    # í”„ë¡œì‹œì €ë³„ summary ì €ì¥ì†Œ ì—…ë°ì´íŠ¸
                    if procedure_summary_store is not None and node.procedure_key:
                        if node.procedure_key in procedure_summary_store:
                            key = f"{node.node_type}_{node.start_line}_{node.end_line}"
                            procedure_summary_store[node.procedure_key][key] = summary
                
                # 2) CALL ê´€ê³„ ìƒì„±
                for call_name in analysis.get('calls', []) or []:
                    if '.' in call_name:
                        # ì™¸ë¶€ í˜¸ì¶œ: íŒ¨í‚¤ì§€.í”„ë¡œì‹œì € í˜•íƒœ
                        package_raw, proc_raw = call_name.split('.', 1)
                        package_name = escape_for_cypher(package_raw.strip())
                        proc_name = escape_for_cypher(proc_raw.strip())
                        queries.append(
                            f"MATCH (c:{node.node_type} {{startLine: {node.start_line}, {self.node_base_props}}})\n"
                            f"MERGE (target:PROCEDURE {{directory: '{package_name}', procedure_name: '{proc_name}', "
                            f"user_id: '{self.user_id}', project_name: '{self.project_name}'}})\n"
                            f"MERGE (c)-[r:CALL {{scope: 'external'}}]->(target)\n"
                            f"RETURN c, target, r"
                        )
                    else:
                        # ë‚´ë¶€ í˜¸ì¶œ: ê°™ì€ íŒŒì¼ ë‚´ í”„ë¡œì‹œì €
                        escaped_call = escape_for_cypher(call_name)
                        queries.append(
                            f"MATCH (c:{node.node_type} {{startLine: {node.start_line}, {self.node_base_props}}})\n"
                            f"MATCH (p {{procedure_name: '{escaped_call}', {self.node_base_props}}})\n"
                            f"WHERE p:PROCEDURE OR p:FUNCTION\n"
                            f"MERGE (c)-[r:CALL {{scope: 'internal'}}]->(p)\n"
                            f"RETURN c, p, r"
                        )
                
                # 3) ë³€ìˆ˜ ì‚¬ìš© ë§ˆí‚¹
                for var_name in analysis.get('variables', []) or []:
                    queries.append(
                        f"MATCH (v:Variable {{name: '{escape_for_cypher(var_name)}', {self.node_base_props}}})\n"
                        f"SET v.`{node.start_line}_{node.end_line}` = 'Used'\n"
                        f"RETURN v"
                    )
        
        # í…Œì´ë¸” ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬
        if table_result:
            table_queries = self._build_table_queries(batch, table_result)
            queries.extend(table_queries)
        
        return queries

    async def _process_procedure_summaries(
        self,
        procedure_summary_store: Dict[str, Dict[str, str]]
    ) -> List[str]:
        """í”„ë¡œì‹œì €ë³„ summaryë¥¼ ì²­í¬ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ìµœì¢… summary + User Story ìƒì„±.
        
        ì²˜ë¦¬ íë¦„:
        1. í† í° ê¸°ì¤€ìœ¼ë¡œ summariesë¥¼ ì²­í¬ë¡œ ë¶„í• 
        2. ê° ì²­í¬ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ì—¬ summary ìƒì„±
        3. ìƒì„±ëœ summaryë“¤ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
        4. ìµœì¢… summaryë¡œ User Story ìƒì„±
        """
        queries: List[str] = []
        
        if not self._procedures:
            return queries
        
        for proc_key, info in self._procedures.items():
            summaries = procedure_summary_store.get(proc_key, {})
            if not summaries:
                continue
            
            # í”„ë¡œì‹œì € ìµœìƒìœ„ ë…¸ë“œ ì°¾ê¸° (í•˜ìœ„ ë¶„ì„ ì‹¤íŒ¨ í™•ì¸ìš©)
            proc_root = next(
                (n for n in (self._nodes or []) 
                 if n.procedure_key == proc_key and n.parent is None),
                None,
            )
            # í•˜ìœ„ ë…¸ë“œ ì¤‘ ì‹¤íŒ¨ê°€ ìˆìœ¼ë©´ ìµœì¢… summary/UserStory ìŠ¤í‚µ
            if proc_root and not proc_root.ok:
                log_process("ANALYZE", "SUMMARY", f"âš ï¸ {info.procedure_name}: í•˜ìœ„ ë¶„ì„ ì‹¤íŒ¨ë¡œ ìµœì¢… summary ìƒì„± ìŠ¤í‚µ")
                continue
            
            try:
                # 1ë‹¨ê³„: í† í° ê¸°ì¤€ìœ¼ë¡œ ì²­í¬ ë¶„í• 
                chunks = self._split_summaries_by_token(summaries, MAX_SUMMARY_CHUNK_TOKEN)
                
                if not chunks:
                    continue
                
                log_process("ANALYZE", "SUMMARY", f"ğŸ“¦ {info.procedure_name}: summary ì²­í¬ ë¶„í•  ({len(chunks)}ê°œ)")
                
                # 2ë‹¨ê³„: ê° ì²­í¬ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ì—¬ summary ìƒì„±
                async def process_chunk(chunk: dict) -> str:
                    result = await asyncio.to_thread(
                        analyze_summary_only, chunk, self.api_key, self.locale, ""
                    )
                    if isinstance(result, dict):
                        return result.get('summary', '')
                    return ""
                
                chunk_results = await asyncio.gather(*[process_chunk(c) for c in chunks])
                chunk_results = [r for r in chunk_results if r]
                
                if not chunk_results:
                    continue
                
                # 3ë‹¨ê³„: ëª¨ë“  ì²­í¬ì˜ summaryë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
                if len(chunk_results) == 1:
                    final_summary = chunk_results[0]
                else:
                    combined = {f"CHUNK_{i+1}": s for i, s in enumerate(chunk_results)}
                    result = await asyncio.to_thread(
                        analyze_summary_only, combined, self.api_key, self.locale, ""
                    )
                    if isinstance(result, dict):
                        final_summary = result.get('summary', "\n\n".join(chunk_results))
                    else:
                        final_summary = "\n\n".join(chunk_results)
                
                log_process("ANALYZE", "SUMMARY", f"âœ… {info.procedure_name}: summary í†µí•© ì™„ë£Œ")
                
                # 4ë‹¨ê³„: User Story ìƒì„±
                all_user_stories = []
                if final_summary:
                    us_result = await asyncio.to_thread(
                        analyze_user_story, final_summary, self.api_key, self.locale
                    )
                    if isinstance(us_result, dict):
                        all_user_stories = us_result.get('user_stories', []) or []
                
                # 5ë‹¨ê³„: Neo4j ì¿¼ë¦¬ ìƒì„±
                summary_json = json.dumps(final_summary, ensure_ascii=False)
                queries.append(
                    f"MATCH (n:{info.procedure_type} {{procedure_name: '{escape_for_cypher(info.procedure_name)}', {self.node_base_props}}})\n"
                    f"SET n.summary = {summary_json}\n"
                    f"RETURN n"
                )
                
                # User Story ë…¸ë“œ ë° ê´€ê³„ ìƒì„±
                proc_name_escaped = escape_for_cypher(info.procedure_name)
                for us_idx, us in enumerate(all_user_stories, 1):
                    us_id = us.get('id', f"US-{us_idx}")
                    role = escape_for_cypher(us.get('role', ''))
                    goal = escape_for_cypher(us.get('goal', ''))
                    benefit = escape_for_cypher(us.get('benefit', ''))
                    
                    queries.append(
                        f"MATCH (p:{info.procedure_type} {{procedure_name: '{proc_name_escaped}', {self.node_base_props}}})\n"
                        f"MERGE (us:UserStory {{id: '{us_id}', procedure_name: '{proc_name_escaped}', {self.node_base_props}}})\n"
                        f"SET us.role = '{role}', us.goal = '{goal}', us.benefit = '{benefit}'\n"
                        f"MERGE (p)-[r:HAS_USER_STORY]->(us)\n"
                        f"RETURN p, us, r"
                    )
                    
                    # Acceptance Criteria ë…¸ë“œ
                    for ac_idx, ac in enumerate(us.get('acceptance_criteria', []), 1):
                        if not isinstance(ac, dict):
                            continue
                        ac_id = ac.get('id', f"AC-{us_idx}-{ac_idx}")
                        ac_title = escape_for_cypher(ac.get('title', ''))
                        ac_given = json.dumps(ac.get('given', []), ensure_ascii=False)
                        ac_when = json.dumps(ac.get('when', []), ensure_ascii=False)
                        ac_then = json.dumps(ac.get('then', []), ensure_ascii=False)
                        
                        queries.append(
                            f"MATCH (us:UserStory {{id: '{us_id}', {self.node_base_props}}})\n"
                            f"MERGE (ac:AcceptanceCriteria {{id: '{ac_id}', user_story_id: '{us_id}', {self.node_base_props}}})\n"
                            f"SET ac.title = '{ac_title}', ac.given = {ac_given}, ac.when = {ac_when}, ac.then = {ac_then}\n"
                            f"MERGE (us)-[r:HAS_AC]->(ac)\n"
                            f"RETURN us, ac, r"
                        )
                
                us_count = len(all_user_stories)
                log_process("ANALYZE", "SUMMARY", f"âœ… {info.procedure_name}: User Story {us_count}ê°œ ìƒì„±")
                
            except Exception as exc:
                log_process("ANALYZE", "SUMMARY", f"âŒ {info.procedure_name} í”„ë¡œì‹œì € ìš”ì•½ ì˜¤ë¥˜", logging.ERROR, exc)
        
        return queries
    
    def _split_summaries_by_token(self, summaries: dict, max_token: int) -> List[dict]:
        """í† í° ê¸°ì¤€ìœ¼ë¡œ summariesë¥¼ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
        if not summaries:
            return []
        
        chunks = []
        current_chunk = {}
        current_tokens = 0
        
        for key, value in summaries.items():
            item_text = f"{key}: {value}"
            item_tokens = calculate_code_token(item_text)
            
            if current_tokens + item_tokens > max_token and current_chunk:
                chunks.append(current_chunk)
                current_chunk = {}
                current_tokens = 0
            
            current_chunk[key] = value
            current_tokens += item_tokens
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks

    def _build_parent_of_query(self, parent: StatementNode, child: StatementNode) -> str:
        """ë¶€ëª¨-ìì‹ ê´€ê³„ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        return (
            f"MATCH (parent:{parent.node_type} {{startLine: {parent.start_line}, {self.node_base_props}}})\n"
            f"MATCH (child:{child.node_type} {{startLine: {child.start_line}, {self.node_base_props}}})\n"
            f"MERGE (parent)-[r:PARENT_OF]->(child)\n"
            f"RETURN r"
        )

    def _build_next_query(self, prev: StatementNode, current: StatementNode) -> str:
        """í˜•ì œ ê´€ê³„ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        return (
            f"MATCH (prev:{prev.node_type} {{startLine: {prev.start_line}, {self.node_base_props}}})\n"
            f"MATCH (current:{current.node_type} {{startLine: {current.start_line}, {self.node_base_props}}})\n"
            f"MERGE (prev)-[r:NEXT]->(current)\n"
            f"RETURN r"
        )

    def _build_table_queries(
        self,
        batch: AnalysisBatch,
        table_result: Dict[str, Any]
    ) -> List[str]:
        """DML í…Œì´ë¸” ë¶„ì„ ê²°ê³¼ë¥¼ Neo4j ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        ì²˜ë¦¬ í•­ëª©:
        - í…Œì´ë¸” ë…¸ë“œ ë° DML ê´€ê³„ (FROM/INTO)
        - ì»¬ëŸ¼ ë…¸ë“œ ë° HAS_COLUMN ê´€ê³„
        - DBLink ì²˜ë¦¬
        - FK ê´€ê³„ (FK_TO, FK_TO_TABLE)
        """
        queries: List[str] = []
        node_map: Dict[Tuple[int, int], StatementNode] = {
            (node.start_line, node.end_line): node for node in batch.nodes
        }
        ranges = table_result.get('ranges', []) or []
        
        for range_entry in ranges:
            start_line = range_entry.get('startLine')
            end_line = range_entry.get('endLine')
            tables = range_entry.get('tables') or []
            
            try:
                start_line = int(start_line)
                end_line = int(end_line)
            except (TypeError, ValueError):
                continue
            
            node = node_map.get((start_line, end_line))
            if not node:
                continue
            
            node_merge = f"MATCH (n:{node.node_type} {{startLine: {node.start_line}, {self.node_base_props}}})"
            
            # CREATE_TEMP_TABLE ì²˜ë¦¬
            if node.node_type == 'CREATE_TEMP_TABLE':
                for entry in tables:
                    table_name = (entry.get('table') or '').strip()
                    if not table_name:
                        continue
                    schema_part, name_part, _ = parse_table_identifier(table_name)
                    queries.append(
                        f"{node_merge}\n"
                        f"SET n:Table, n.name = '{escape_for_cypher(name_part)}', "
                        f"n.schema = '{escape_for_cypher(schema_part)}', n.db = '{self.dbms}'\n"
                        f"RETURN n"
                    )
                continue
            
            # ì¼ë°˜ DML í…Œì´ë¸” ì²˜ë¦¬
            for entry in tables:
                table_name = (entry.get('table') or '').strip()
                if not table_name:
                    continue
                
                schema_part, name_part, db_link_value = parse_table_identifier(table_name)
                
                # ì ‘ê·¼ ëª¨ë“œì— ë”°ë¥¸ ê´€ê³„ íƒ€ì… ê²°ì •
                access_mode = (entry.get('accessMode') or entry.get('mode') or 'r').lower()
                rel_types = []
                if 'r' in access_mode:
                    rel_types.append(TABLE_RELATIONSHIP_MAP.get('r', 'FROM'))
                if 'w' in access_mode:
                    rel_types.append(TABLE_RELATIONSHIP_MAP.get('w', 'WRITES'))
                
                table_merge = self._build_table_merge(name_part, schema_part)
                
                # í…Œì´ë¸” ì„¤ëª…ì„ ë²„í‚·ì— ëˆ„ì  (í›„ì† ìš”ì•½ìš©)
                table_desc_raw = entry.get('tableDescription') or entry.get('description') or ''
                bucket_key = self._record_table_summary(schema_part, name_part, table_desc_raw)
                
                # í…Œì´ë¸” ë…¸ë“œ ë° ê´€ê³„ ìƒì„±
                table_query = f"{node_merge}\nWITH n\n{table_merge}\nSET t.db = coalesce(t.db, '{self.dbms}')"
                
                if db_link_value:
                    table_query += f"\nSET t.db_link = COALESCE(t.db_link, '{db_link_value}')"
                
                for i, rel_type in enumerate(rel_types):
                    table_query += f"\nMERGE (n)-[r{i}:{rel_type}]->(t)"
                
                table_query += "\nRETURN n, t"
                queries.append(table_query)
                
                # ì»¬ëŸ¼ ì²˜ë¦¬
                for column in entry.get('columns', []) or []:
                    column_name = (column.get('name') or '').strip()
                    if not column_name:
                        continue
                    
                    raw_dtype = column.get('dtype') or ''
                    raw_column_desc = (column.get('description') or column.get('comment') or '').strip()
                    
                    # ì»¬ëŸ¼ ì„¤ëª…/ë©”íƒ€ë¥¼ ë²„í‚·ì— ëˆ„ì  (í›„ì† ìš”ì•½ìš©)
                    self._record_column_summary(
                        bucket_key,
                        column_name,
                        raw_column_desc,
                        dtype=raw_dtype,
                        nullable=column.get('nullable', True),
                        examples=column.get('examples') or [],
                    )
                    
                    col_type = escape_for_cypher(raw_dtype)
                    col_desc = escape_for_cypher(raw_column_desc)
                    nullable = 'true' if column.get('nullable', True) else 'false'
                    escaped_col_name = escape_for_cypher(column_name)
                    
                    if schema_part:
                        fqn = '.'.join(filter(None, [schema_part, name_part, column_name])).lower()
                        queries.append(
                            f"{table_merge}\nWITH t\n"
                            f"MERGE (c:Column {{user_id: '{self.user_id}', fqn: '{fqn}', project_name: '{self.project_name}'}})\n"
                            f"SET c.name = '{escaped_col_name}', c.dtype = '{col_type}', "
                            f"c.description = '{col_desc}', c.nullable = '{nullable}'\n"
                            f"MERGE (t)-[r:HAS_COLUMN]->(c)\n"
                            f"RETURN t, c, r"
                        )
                    else:
                        queries.append(
                            f"{table_merge}\n"
                            f"WITH t, lower(case when t.schema <> '' and t.schema IS NOT NULL "
                            f"then t.schema + '.' + '{name_part}' + '.' + '{column_name}' "
                            f"else '{name_part}' + '.' + '{column_name}' end) as fqn\n"
                            f"MERGE (c:Column {{user_id: '{self.user_id}', fqn: fqn, project_name: '{self.project_name}'}})\n"
                            f"ON CREATE SET c.name = '{escaped_col_name}', c.dtype = '{col_type}', "
                            f"c.description = '{col_desc}', c.nullable = '{nullable}'\n"
                            f"MERGE (t)-[r:HAS_COLUMN]->(c)\n"
                            f"RETURN t, c, r"
                        )
            
            # DBLink ì²˜ë¦¬
            for link_item in range_entry.get('dbLinks', []) or []:
                link_name_raw = (link_item.get('name') or '').strip()
                if not link_name_raw:
                    continue
                mode = (link_item.get('mode') or 'r').lower()
                schema_link, name_link, link_name = parse_table_identifier(link_name_raw)
                remote_merge = self._build_table_merge(name_link, schema_link)
                queries.append(
                    f"{remote_merge}\nSET t.db_link = '{link_name}'\n"
                    f"WITH t\n"
                    f"MERGE (l:DBLink {{user_id: '{self.user_id}', name: '{link_name}', project_name: '{self.project_name}'}})\n"
                    f"MERGE (l)-[r1:CONTAINS]->(t)\n"
                    f"WITH t, l\n{node_merge}\n"
                    f"MERGE (n)-[r2:DB_LINK {{mode: '{mode}'}}]->(t)\n"
                    f"RETURN l, t, n"
                )
            
            # FK ê´€ê³„ ì²˜ë¦¬
            for relation in range_entry.get('fkRelations', []) or []:
                src_table = (relation.get('sourceTable') or '').strip()
                tgt_table = (relation.get('targetTable') or '').strip()
                src_columns = [c.strip() for c in (relation.get('sourceColumns') or []) if c]
                tgt_columns = [c.strip() for c in (relation.get('targetColumns') or []) if c]
                
                if not (src_table and tgt_table and src_columns and tgt_columns):
                    continue
                
                src_schema, src_name, _ = parse_table_identifier(src_table)
                tgt_schema, tgt_name, _ = parse_table_identifier(tgt_table)
                
                src_props = f"user_id: '{self.user_id}', schema: '{src_schema or ''}', name: '{src_name}', db: '{self.dbms}', project_name: '{self.project_name}'"
                tgt_props = f"user_id: '{self.user_id}', schema: '{tgt_schema or ''}', name: '{tgt_name}', db: '{self.dbms}', project_name: '{self.project_name}'"
                
                # í…Œì´ë¸” ê°„ FK ê´€ê³„
                queries.append(
                    f"MATCH (st:Table {{{src_props}}})\n"
                    f"MATCH (tt:Table {{{tgt_props}}})\n"
                    f"MERGE (st)-[r:FK_TO_TABLE]->(tt)\n"
                    f"RETURN st, tt, r"
                )
                
                # ì»¬ëŸ¼ ê°„ FK ê´€ê³„
                for src_col, tgt_col in zip(src_columns, tgt_columns):
                    src_fqn = '.'.join(filter(None, [src_schema, src_name, src_col])).lower()
                    tgt_fqn = '.'.join(filter(None, [tgt_schema, tgt_name, tgt_col])).lower()
                    queries.append(
                        f"MATCH (sc:Column {{user_id: '{self.user_id}', fqn: '{src_fqn}', project_name: '{self.project_name}'}})\n"
                        f"MATCH (dc:Column {{user_id: '{self.user_id}', fqn: '{tgt_fqn}', project_name: '{self.project_name}'}})\n"
                        f"MERGE (sc)-[r:FK_TO]->(dc)\n"
                        f"RETURN sc, dc, r"
                    )
        
        return queries
    
    def _build_table_merge(self, table_name: str, schema: Optional[str]) -> str:
        """í…Œì´ë¸” MERGE ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        ìŠ¤í‚¤ë§ˆê°€ ì—†ìœ¼ë©´ default_schemaë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        schema_value = schema or self.default_schema
        return (
            f"MERGE (t:Table {{{self.table_base_props}, name: '{table_name}', schema: '{schema_value}', db: '{self.dbms}', project_name: '{self.project_name}'}})"
        )
    
    # =========================================================================
    # í…Œì´ë¸”/ì»¬ëŸ¼ ì„¤ëª… ìš”ì•½ ì²˜ë¦¬
    # =========================================================================
    
    def _record_table_summary(self, schema: Optional[str], name: str, description: Optional[str]) -> Tuple[str, str]:
        """í…Œì´ë¸” ì„¤ëª… ë¬¸ì¥ì„ ë²„í‚·ì— ëˆ„ì í•©ë‹ˆë‹¤.
        
        ìŠ¤í‚¤ë§ˆê°€ ì—†ìœ¼ë©´ default_schemaë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        schema_key = schema or self.default_schema
        name_key = name
        bucket = self._table_summary_store.get((schema_key, name_key))
        if bucket is None:
            bucket = {"summaries": set(), "columns": {}}
            self._table_summary_store[(schema_key, name_key)] = bucket
        text = (description or '').strip()
        if text:
            bucket["summaries"].add(text)
        return (schema_key, name_key)
    
    def _record_column_summary(
        self,
        table_key: Tuple[str, str],
        column_name: str,
        description: Optional[str],
        dtype: Optional[str] = None,
        nullable: Optional[bool] = None,
        examples: Optional[List[str]] = None,
    ):
        """ì»¬ëŸ¼ ì„¤ëª…ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ ë²„í‚·ì— ëˆ„ì í•©ë‹ˆë‹¤."""
        text = (description or '').strip()
        bucket = self._table_summary_store.setdefault(table_key, {"summaries": set(), "columns": {}})
        columns = bucket["columns"]
        canonical = column_name
        entry = columns.get(canonical)
        if entry is None:
            entry = {"name": column_name, "summaries": set(), "dtype": (dtype or ''), "nullable": True if nullable is None else bool(nullable), "examples": set()}
            columns[canonical] = entry
        if dtype is not None and not entry.get("dtype"):
            entry["dtype"] = dtype
        if nullable is not None:
            entry["nullable"] = bool(nullable)
        if text:
            entry["summaries"].add(text)
        if examples:
            for v in examples:
                if v is not None:
                    s = str(v).strip()
                    if s:
                        entry["examples"].add(s)
    
    async def _finalize_table_summaries(self) -> List[str]:
        """ë²„í‚·ì— ëª¨ì€ í…Œì´ë¸”/ì»¬ëŸ¼ ì„¤ëª…ì„ ë³‘ë ¬ë¡œ ìš”ì•½í•©ë‹ˆë‹¤."""
        if not self._table_summary_store:
            return []
        
        tasks = [
            self._summarize_table(table_key, data)
            for table_key, data in list(self._table_summary_store.items())
        ]
        if not tasks:
            return []
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        all_queries: List[str] = []
        for result in results:
            if isinstance(result, Exception):
                log_process("ANALYZE", "TABLE_SUMMARY", f"âŒ í…Œì´ë¸” ìš”ì•½ ì˜¤ë¥˜: {result}", logging.ERROR)
            elif result:
                all_queries.extend(result)
        
        self._table_summary_store.clear()
        return all_queries
    
    async def _summarize_table(self, table_key: Tuple[str, str], data: Dict[str, Any]) -> List[str]:
        """í…Œì´ë¸”/ì»¬ëŸ¼ ì„¤ëª… ë²„í‚·ì„ ê¸°ë°˜ìœ¼ë¡œ LLM ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        DDL ë©”íƒ€ë°ì´í„°ë¥¼ LLM ì…ë ¥ì— í¬í•¨í•˜ì—¬ í†µí•©ëœ descriptionì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        schema_key, name_key = table_key
        
        # DDL ë©”íƒ€ë°ì´í„° ì¡°íšŒ (ë©”ëª¨ë¦¬ ìºì‹œ) - ë¨¼ì € ì¡°íšŒí•˜ì—¬ ì²´í¬ì— í™œìš©
        ddl_key = (schema_key.lower(), name_key.lower())
        ddl_meta = self._ddl_table_metadata.get(ddl_key, {})
        ddl_description = (ddl_meta.get('description') or '').strip()
        ddl_columns = ddl_meta.get('columns') or {}
        
        summaries = list(data.get('summaries') or [])
        columns_map = data.get('columns') or {}
        column_sentences = {
            entry['name']: list(entry['summaries'])
            for entry in columns_map.values()
            if entry.get('summaries')
        }
        
        # DDL descriptionì´ ìˆìœ¼ë©´ summariesì— ì¶”ê°€ (LLM ì…ë ¥ì— í¬í•¨)
        if ddl_description:
            summaries.insert(0, f"[DDL ë©”íƒ€ë°ì´í„°] {ddl_description}")
        
        # DDL ì»¬ëŸ¼ descriptionë„ column_sentencesì— ì¶”ê°€
        for col_name, ddl_col in ddl_columns.items():
            ddl_col_desc = (ddl_col.get('description') or '').strip()
            if ddl_col_desc and col_name not in column_sentences:
                column_sentences[col_name] = [f"[DDL ë©”íƒ€ë°ì´í„°] {ddl_col_desc}"]
            elif ddl_col_desc and col_name in column_sentences:
                column_sentences[col_name].insert(0, f"[DDL ë©”íƒ€ë°ì´í„°] {ddl_col_desc}")
        
        # DDL ë©”íƒ€ë°ì´í„°ë‚˜ DML ë¶„ì„ ê²°ê³¼ê°€ í•˜ë‚˜ë¼ë„ ìˆì–´ì•¼ ì²˜ë¦¬
        if not summaries and not column_sentences:
            return []
        
        # DDL ì»¬ëŸ¼ ì •ë³´ë¥¼ column_metadataì— ë³‘í•©
        table_display = f"{schema_key}.{name_key}" if schema_key else name_key
        column_metadata = {}
        for entry in columns_map.values():
            col_name = entry['name']
            ddl_col = ddl_columns.get(col_name, {})
            column_metadata[col_name] = {
                "dtype": entry.get("dtype") or ddl_col.get("dtype") or "",
                "nullable": bool(entry.get("nullable", True)) if entry.get("nullable") is not None else ddl_col.get("nullable", True),
                "examples": sorted(list(entry.get("examples") or []))[:5],
            }
        
        # DDL ì»¬ëŸ¼ë„ column_metadataì— ì¶”ê°€ (DMLì—ì„œ ë°œê²¬ë˜ì§€ ì•Šì€ ì»¬ëŸ¼)
        for col_name, ddl_col in ddl_columns.items():
            if col_name not in column_metadata:
                column_metadata[col_name] = {
                    "dtype": ddl_col.get("dtype") or "",
                    "nullable": ddl_col.get("nullable", True),
                    "examples": [],
                }
        
        result = await asyncio.to_thread(
            summarize_table_metadata,
            table_display,
            summaries,
            column_sentences,
            column_metadata,
            self.api_key,
            self.locale,
        )
        
        if not isinstance(result, dict):
            return []
        
        queries: List[str] = []
        # LLMì´ ìƒì„±í•œ tableDescriptionì„ ê·¸ëŒ€ë¡œ descriptionì— í• ë‹¹
        llm_table_desc = (result.get('tableDescription') or '').strip()
        schema_prop = schema_key
        table_props = (
            f"user_id: '{self.user_id}', schema: '{schema_prop}', name: '{name_key}', db: '{self.dbms}', project_name: '{self.project_name}'"
        )
        
        if llm_table_desc:
            queries.append(
                f"MATCH (t:Table {{{table_props}}})\nSET t.description = '{escape_for_cypher(llm_table_desc)}'\nRETURN t"
            )
        
        # ì»¬ëŸ¼ description ì²˜ë¦¬
        for column_info in result.get('columns', []) or []:
            column_name = (column_info.get('name') or '').strip()
            llm_column_desc = (column_info.get('description') or '').strip()
            if not column_name or not llm_column_desc:
                continue
            
            fqn = '.'.join(filter(None, [schema_prop, name_key, column_name])).lower()
            column_props = (
                f"user_id: '{self.user_id}', name: '{column_name}', fqn: '{fqn}', project_name: '{self.project_name}'"
            )
            queries.append(
                f"MATCH (c:Column {{{column_props}}})\nSET c.description = '{escape_for_cypher(llm_column_desc)}'\nRETURN c"
            )
        
        return queries
