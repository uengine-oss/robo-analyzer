"""DDL ì²˜ë¦¬ Phase - í…Œì´ë¸”/ì»¬ëŸ¼/ìŠ¤í‚¤ë§ˆ ë…¸ë“œ ìƒì„±

dbms_analyzer.pyì—ì„œ ë¶„ë¦¬ëœ DDL ì²˜ë¦¬ ë¡œì§ì…ë‹ˆë‹¤.
ëª¨ë“  ë¡œì§ì€ 100% ë³´ì¡´ë˜ë©°, ìœ„ì¹˜ë§Œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.
"""

import asyncio
import logging
import os
from typing import Any, AsyncGenerator, Dict, List, Set, Tuple

import aiofiles

from analyzer.neo4j_client import Neo4jClient
from analyzer.strategy.base_analyzer import AnalysisStats
from analyzer.ddl_static_parser import parse_ddl as regex_parse_ddl
from util.stream_event import (
    emit_data,
    emit_message,
    emit_phase_event,
)
from util.text_utils import (
    escape_for_cypher,
    log_process,
    parse_table_identifier,
    calculate_code_token,
)


def list_ddl_files(orchestrator: Any) -> List[str]:
    """DDL íŒŒì¼ ëª©ë¡ ì¡°íšŒ
    
    DDL ë””ë ‰í† ë¦¬ê°€ ì—†ê±°ë‚˜ íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ê²½ê³  ì²˜ë¦¬, ì—ëŸ¬ ì•„ë‹˜)
    """
    ddl_dir = orchestrator.dirs.get("ddl", "")
    if not ddl_dir:
        log_process("ANALYZE", "DDL", "DDL ë””ë ‰í† ë¦¬ ì„¤ì • ì—†ìŒ - DDL ì²˜ë¦¬ ìƒëµ")
        return []
    if not os.path.isdir(ddl_dir):
        # DDL ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ê²½ê³ ë§Œ í•˜ê³  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        log_process("ANALYZE", "DDL", f"DDL ë””ë ‰í† ë¦¬ ì—†ìŒ: {ddl_dir} - DDL ì²˜ë¦¬ ìƒëµ")
        return []
    try:
        files = sorted(
            f for f in os.listdir(ddl_dir)
            if os.path.isfile(os.path.join(ddl_dir, f))
        )
        if not files:
            # DDL íŒŒì¼ì´ ì—†ìœ¼ë©´ ê²½ê³ ë§Œ í•˜ê³  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            log_process("ANALYZE", "DDL", f"DDL ë””ë ‰í† ë¦¬ì— íŒŒì¼ ì—†ìŒ: {ddl_dir} - DDL ì²˜ë¦¬ ìƒëµ")
            return []
        log_process("ANALYZE", "DDL", f"DDL íŒŒì¼ ë°œê²¬: {len(files)}ê°œ")
        return files
    except OSError as e:
        log_process("ANALYZE", "DDL", f"DDL ë””ë ‰í† ë¦¬ ì½ê¸° ì‹¤íŒ¨: {ddl_dir} - {e}")
        return []


def apply_name_case(name: str, name_case: str) -> str:
    """ë©”íƒ€ë°ì´í„° ëŒ€ì†Œë¬¸ì ë³€í™˜ ì ìš©
    
    Args:
        name: ë³€í™˜í•  ì´ë¦„ (í…Œì´ë¸”ëª…, ì»¬ëŸ¼ëª…, ìŠ¤í‚¤ë§ˆëª… ë“±)
        name_case: ë³€í™˜ ì˜µì…˜ (original, uppercase, lowercase)
    
    Returns:
        ë³€í™˜ëœ ì´ë¦„
    """
    if not name:
        return name
    if name_case == "uppercase":
        return name.upper()
    elif name_case == "lowercase":
        return name.lower()
    return name  # original: ê·¸ëŒ€ë¡œ ë°˜í™˜


def resolve_default_schema(
    directory: str,
    ddl_schemas: Set[str],
    name_case: str = 'original'
) -> str:
    """íŒŒì¼ ê²½ë¡œì—ì„œ ê¸°ë³¸ ìŠ¤í‚¤ë§ˆë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
    
    ìš°ì„ ìˆœìœ„:
    1. ê²½ë¡œì˜ í´ë”ëª… ì¤‘ DDL ìŠ¤í‚¤ë§ˆì™€ ì¼ì¹˜í•˜ëŠ” ê²ƒ (ê¹Šì€ í´ë” ìš°ì„ )
    2. ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ë””ë ‰í† ë¦¬ëª… ì‚¬ìš©
    
    Args:
        directory: íŒŒì¼ì´ ìœ„ì¹˜í•œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        ddl_schemas: DDLì—ì„œ ìˆ˜ì§‘ëœ ìŠ¤í‚¤ë§ˆ Set
        name_case: ëŒ€ì†Œë¬¸ì ë³€í™˜ ì˜µì…˜ (original, uppercase, lowercase)
    """
    if not directory:
        return apply_name_case("public", name_case)
    
    # ê²½ë¡œë¥¼ í´ë” ëª©ë¡ìœ¼ë¡œ ë¶„ë¦¬ (ê¹Šì€ ìˆœì„œëŒ€ë¡œ)
    parts = directory.replace("\\", "/").split("/")
    parts = [p for p in parts if p]  # ë¹ˆ ë¬¸ìì—´ ì œê±°
    
    if not parts:
        return apply_name_case("public", name_case)
    
    # DDL ìŠ¤í‚¤ë§ˆê°€ ìˆìœ¼ë©´ ë§¤ì¹­ ì‹œë„ (ê¹Šì€ í´ë”ë¶€í„°)
    # ëŒ€ì†Œë¬¸ì ë¬´ê´€ ë¹„êµ í›„, DDLì— ì €ì¥ëœ ì›ë³¸ ëŒ€ì†Œë¬¸ì ë°˜í™˜
    if ddl_schemas:
        ddl_schemas_lower_map = {s.lower(): s for s in ddl_schemas}
        for folder in reversed(parts):
            matched = ddl_schemas_lower_map.get(folder.lower())
            if matched:
                return matched  # DDLì—ì„œ name_case ì ìš©ëœ ê°’ ê·¸ëŒ€ë¡œ ë°˜í™˜
    
    # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ë””ë ‰í† ë¦¬ëª…(ê°€ì¥ ê¹Šì€ í´ë”)ì— name_case ì ìš©
    return apply_name_case(parts[-1], name_case)


async def process_ddl(
    ddl_path: str,
    client: Neo4jClient,
    file_name: str,
    orchestrator: Any,
    cypher_lock: asyncio.Lock,
    ddl_schemas: Set[str],
    ddl_table_metadata: Dict[Tuple[str, str], Dict[str, Any]],
    emit_progress: bool = True,
    file_base_progress: int = 0,
    file_end_progress: int = 100,
) -> AsyncGenerator[bytes | Tuple[dict, dict], None]:
    """DDL íŒŒì¼ ì²˜ë¦¬ ë° í…Œì´ë¸”/ì»¬ëŸ¼ ë…¸ë“œ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°)
    
    ì •ì  ì •ê·œì‹ íŒŒì„œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤ (LLM íŒŒì„œ ì œê±°ë¨).
    
    Args:
        ddl_path: DDL íŒŒì¼ ê²½ë¡œ
        client: Neo4j í´ë¼ì´ì–¸íŠ¸
        file_name: DDL íŒŒì¼ëª…
        orchestrator: ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° (ì„¤ì • ì •ë³´)
        cypher_lock: Cypher ì¿¼ë¦¬ ë™ì‹œì„± ë³´í˜¸ ë½
        ddl_schemas: DDLì—ì„œ ìˆ˜ì§‘ëœ ìŠ¤í‚¤ë§ˆ Set (mutable)
        ddl_table_metadata: DDL í…Œì´ë¸” ë©”íƒ€ë°ì´í„° ìºì‹œ (mutable)
        emit_progress: ì§„í–‰ ìƒí™© ë©”ì‹œì§€ emit ì—¬ë¶€
        file_base_progress: ì´ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘ ì‹œ ì „ì²´ ì§„í–‰ë¥  (0-100)
        file_end_progress: ì´ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ ì‹œ ì „ì²´ ì§„í–‰ë¥  (0-100)
    
    Yields:
        bytes: ì§„í–‰ ìƒí™© ë©”ì‹œì§€ (emit_message)
        tuple[dict, dict]: ìµœì¢… ê²°ê³¼ (ddl_graph, ddl_stats) - ë§ˆì§€ë§‰ì— í•œ ë²ˆë§Œ
    """
    ddl_stats = {"tables": 0, "columns": 0, "fks": 0}
    
    # ì§„í–‰ë¥  ë²”ìœ„ ê³„ì‚° (íŒŒì¼ ë‚´ì—ì„œ íŒŒì‹± 50%, ì €ì¥ 50% ë¹„ìœ¨)
    file_range = file_end_progress - file_base_progress
    parsing_end = file_base_progress + int(file_range * 0.5)
    saving_start = parsing_end  # ì €ì¥ ì‹œì‘ = íŒŒì‹± ì¢…ë£Œ ì‹œì 
    saving_end = file_end_progress  # ì €ì¥ ì¢…ë£Œ = íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ ì‹œì 
    
    async with aiofiles.open(ddl_path, "r", encoding="utf-8") as f:
        ddl_content = await f.read()
    
    total_tokens = calculate_code_token(ddl_content)
    
    # ========================================
    # ì •ê·œì‹ íŒŒì„œ ì‚¬ìš© (ì •ì  íŒŒì‹± - LLM ë¯¸ì‚¬ìš©)
    # ========================================
    if emit_progress:
        yield emit_message(f"   âš¡ ì •ê·œì‹ íŒŒì„œ ì‚¬ìš© (ì •ì  íŒŒì‹±)")
        yield emit_phase_event(
            phase_num=0,
            phase_name="DDL ì²˜ë¦¬",
            status="in_progress",
            progress=file_base_progress + int(file_range * 0.1),
            details={"mode": "regex", "tokens": total_tokens}
        )
    
    try:
        # ì •ê·œì‹ íŒŒì„œë¡œ í•œ ë²ˆì— íŒŒì‹± (ë§¤ìš° ë¹ ë¦„)
        parsed = await asyncio.to_thread(regex_parse_ddl, ddl_content)
        all_parsed_results = parsed.get("analysis", [])
        
        table_count = len(all_parsed_results)
        if emit_progress:
            # ì²˜ìŒ 5ê°œ í…Œì´ë¸”ëª… ë¯¸ë¦¬ë³´ê¸°
            table_names = [t.get("table", {}).get("name", "?") for t in all_parsed_results[:5]]
            preview = ", ".join(table_names)
            if table_count > 5:
                preview += f" ì™¸ {table_count - 5}ê°œ"
            
            yield emit_message(f"   âœ… íŒŒì‹± ì™„ë£Œ: {table_count}ê°œ í…Œì´ë¸” ({preview})")
            yield emit_phase_event(
                phase_num=0,
                phase_name="DDL ì²˜ë¦¬",
                status="in_progress",
                progress=parsing_end,
                details={"tables_parsed": table_count, "mode": "regex"}
            )
            
    except Exception as e:
        if emit_progress:
            yield emit_message(f"   âŒ ì •ê·œì‹ íŒŒì‹± ì‹¤íŒ¨: {str(e)[:80]}")
        raise RuntimeError(f"DDL ì •ê·œì‹ íŒŒì‹± ì‹¤íŒ¨: {e}")
    
    # ë³‘í•©ëœ ê²°ê³¼ë¥¼ parsedë¡œ ì‚¬ìš©
    parsed = {"analysis": all_parsed_results}
    
    # db ì†ì„±ì€ DML ì²˜ë¦¬(ast_processor)ì™€ ì¼ê´€ì„±ì„ ìœ„í•´ ì†Œë¬¸ìë¡œ ë³€í™˜
    db_name = (orchestrator.target or 'postgres').lower()
    
    # ëŒ€ì†Œë¬¸ì ë³€í™˜ ì˜µì…˜
    name_case = getattr(orchestrator, 'name_case', 'original')

    # ===========================================
    # UNWIND ë°°ì¹˜ìš© ë°ì´í„° ìˆ˜ì§‘ (ê°œë³„ ì¿¼ë¦¬ ëŒ€ì‹ )
    # ===========================================
    schemas_data = []  # ìŠ¤í‚¤ë§ˆ ë°ì´í„°
    tables_data = []   # í…Œì´ë¸” ë°ì´í„°
    columns_data = []  # ì»¬ëŸ¼ ë°ì´í„°
    fks_data = []      # FK ê´€ê³„ ë°ì´í„°
    
    # ì¤‘ë³µ ë°©ì§€ìš© ì„¸íŠ¸
    seen_schemas = set()
    seen_tables = set()

    for table_info in parsed.get("analysis", []):
        table = table_info.get("table", {})
        columns = table_info.get("columns", [])
        foreign_keys = table_info.get("foreignKeys", [])
        primary_keys = [
            str(pk).strip().upper()
            for pk in (table_info.get("primaryKeys") or [])
            if pk
        ]

        # ì›ë³¸ ê°’ì—ì„œ ë”°ì˜´í‘œ ì œê±° í›„ ëŒ€ì†Œë¬¸ì ë³€í™˜ ì ìš©
        schema_raw = (table.get("schema") or "").strip()
        table_name_raw = (table.get("name") or "").strip()
        comment = (table.get("comment") or "").strip()
        table_type = (table.get("table_type") or "BASE TABLE").strip().upper()
        
        # parse_table_identifierë¡œ ë”°ì˜´í‘œ ì œê±° ë° ìŠ¤í‚¤ë§ˆ/í…Œì´ë¸” ë¶„ë¦¬
        qualified = f"{schema_raw}.{table_name_raw}" if schema_raw else table_name_raw
        parsed_schema, parsed_name, _ = parse_table_identifier(qualified)
        
        # name_case ì˜µì…˜ì— ë”°ë¼ ëŒ€ì†Œë¬¸ì ë³€í™˜ ì ìš©
        schema = apply_name_case(parsed_schema if parsed_schema else "public", name_case)
        parsed_name = apply_name_case(parsed_name, name_case)
        
        # DDLì—ì„œ ë°œê²¬ëœ ìŠ¤í‚¤ë§ˆ ìˆ˜ì§‘ (name_case ì ìš©ëœ ê°’ìœ¼ë¡œ ì €ì¥)
        if schema and schema.lower() != 'public':
            ddl_schemas.add(schema)
        
        # ìŠ¤í‚¤ë§ˆ ë°ì´í„° ìˆ˜ì§‘ (ì¤‘ë³µ ë°©ì§€)
        schema_key = (db_name, schema)
        if schema_key not in seen_schemas:
            seen_schemas.add(schema_key)
            schemas_data.append({
                "db": db_name,
                "name": schema
            })
        
        # í…Œì´ë¸” ë°ì´í„° ìˆ˜ì§‘ (ì¤‘ë³µ ë°©ì§€)
        table_key = (db_name, schema, parsed_name)
        if table_key not in seen_tables:
            seen_tables.add(table_key)
            tables_data.append({
                "db": db_name,
                "schema": schema,
                "name": parsed_name,
                "description": escape_for_cypher(comment),
                "description_source": "ddl" if comment else "",
                "table_type": table_type
            })
            ddl_stats["tables"] += 1
        
        # DDL ë©”íƒ€ë°ì´í„° ìºì‹œ ì €ì¥ (ë©”ëª¨ë¦¬)
        column_metadata = {}
        for col in columns:
            col_name_raw = (col.get("name") or "").strip()
            if not col_name_raw:
                continue
            col_name = apply_name_case(col_name_raw, name_case)
            col_comment = (col.get("comment") or "").strip()
            column_metadata[col_name] = {
                "description": col_comment,
                "dtype": (col.get("dtype") or col.get("type") or "").strip(),
                "nullable": col.get("nullable", True),
            }
        
        cache_key = (schema.lower(), parsed_name.lower())
        ddl_table_metadata[cache_key] = {
            "description": comment,
            "columns": column_metadata,
            "original_schema": schema,
            "original_name": parsed_name,
        }

        # ì»¬ëŸ¼ ë°ì´í„° ìˆ˜ì§‘
        for col in columns:
            col_name_raw = (col.get("name") or "").strip()
            if not col_name_raw:
                continue
            
            col_name = apply_name_case(col_name_raw, name_case)
            col_type = (col.get("dtype") or col.get("type") or "").strip()
            col_nullable = col.get("nullable", True)
            col_comment = (col.get("comment") or "").strip()
            fqn = ".".join(filter(None, [schema, parsed_name, col_name])).lower()
            
            col_data = {
                "fqn": escape_for_cypher(fqn),
                "name": escape_for_cypher(col_name),
                "dtype": escape_for_cypher(col_type),
                "description": escape_for_cypher(col_comment),
                "description_source": "ddl" if col_comment else "",
                "nullable": col_nullable,
                "table_db": db_name,
                "table_schema": schema,
                "table_name": parsed_name
            }
            if col_name_raw.upper() in primary_keys:
                col_data["pk_constraint"] = f"{parsed_name}_pkey"
            
            columns_data.append(col_data)
            ddl_stats["columns"] += 1

        # FK ê´€ê³„ ë°ì´í„° ìˆ˜ì§‘
        for fk in foreign_keys:
            src_col_raw = (fk.get("column") or "").strip()
            ref = (fk.get("ref") or "").strip()
            if not src_col_raw or not ref or "." not in ref:
                continue

            ref_table_part, ref_col_raw = ref.rsplit(".", 1)
            ref_schema_parsed, ref_table_raw, _ = parse_table_identifier(ref_table_part)
            ref_schema_final = apply_name_case(ref_schema_parsed or schema, name_case)
            ref_table = apply_name_case(ref_table_raw, name_case)
            src_col = apply_name_case(src_col_raw, name_case)
            ref_col = apply_name_case(ref_col_raw, name_case)

            fks_data.append({
                "from_db": db_name,
                "from_schema": schema,
                "from_table": parsed_name,
                "from_column": escape_for_cypher(src_col),
                "to_db": db_name,
                "to_schema": ref_schema_final or "",
                "to_table": ref_table or "",
                "to_column": escape_for_cypher(ref_col)
            })
            ddl_stats["fks"] += 1

    # ===========================================
    # UNWIND ë°°ì¹˜ ì‹¤í–‰ (7~8ë²ˆì˜ Neo4j í˜¸ì¶œë¡œ ì™„ë£Œ!)
    # ===========================================
    if emit_progress:
        yield emit_message(f"   ğŸ’¾ UNWIND ë°°ì¹˜ ì €ì¥ ì‹œì‘: {ddl_stats['tables']}ê°œ í…Œì´ë¸”, {ddl_stats['columns']}ê°œ ì»¬ëŸ¼, {ddl_stats['fks']}ê°œ FK")
        yield emit_phase_event(
            phase_num=0,
            phase_name="DDL ì²˜ë¦¬",
            status="in_progress",
            progress=saving_start,
            details={
                "step": "unwind_batch",
                "tables": ddl_stats['tables'],
                "columns": ddl_stats['columns'],
                "fks": ddl_stats['fks']
            }
        )
    
    all_nodes: dict = {}
    all_relationships: dict = {}
    
    # 1. ìŠ¤í‚¤ë§ˆ ë…¸ë“œ ìƒì„±
    if schemas_data:
        if emit_progress:
            yield emit_message(f"      ğŸ“¦ [1/6] ìŠ¤í‚¤ë§ˆ {len(schemas_data)}ê°œ ìƒì„± ì¤‘...")
        schema_query = """
        UNWIND $items AS item
        MERGE (__cy_s__:Schema {db: item.db, name: item.name})
        RETURN __cy_s__
        """
        async with cypher_lock:
            result = await client.run_batch_unwind(schema_query, schemas_data)
        for node in result.get("Nodes", []):
            all_nodes[node.get("Node ID")] = node
    
    # 2. í…Œì´ë¸” ë…¸ë“œ ìƒì„±
    if tables_data:
        if emit_progress:
            yield emit_message(f"      ğŸ“¦ [2/6] í…Œì´ë¸” {len(tables_data)}ê°œ ìƒì„± ì¤‘...")
        table_query = """
        UNWIND $items AS item
        MERGE (__cy_t__:Table {db: item.db, schema: item.schema, name: item.name})
        SET __cy_t__.description = item.description,
            __cy_t__.description_source = item.description_source,
            __cy_t__.table_type = item.table_type
        RETURN __cy_t__
        """
        async with cypher_lock:
            result = await client.run_batch_unwind(table_query, tables_data)
        for node in result.get("Nodes", []):
            all_nodes[node.get("Node ID")] = node
    
    # 3. í…Œì´ë¸”-ìŠ¤í‚¤ë§ˆ ê´€ê³„ ìƒì„±
    if tables_data:
        if emit_progress:
            yield emit_message(f"      ğŸ“¦ [3/6] í…Œì´ë¸”-ìŠ¤í‚¤ë§ˆ ê´€ê³„ {len(tables_data)}ê°œ ìƒì„± ì¤‘...")
        belongs_query = """
        UNWIND $items AS item
        MATCH (__cy_t__:Table {db: item.db, schema: item.schema, name: item.name})
        MATCH (__cy_s__:Schema {db: item.db, name: item.schema})
        MERGE (__cy_t__)-[__cy_r__:BELONGS_TO]->(__cy_s__)
        RETURN __cy_t__, __cy_r__, __cy_s__
        """
        async with cypher_lock:
            result = await client.run_batch_unwind(belongs_query, tables_data)
        for node in result.get("Nodes", []):
            all_nodes[node.get("Node ID")] = node
        for rel in result.get("Relationships", []):
            all_relationships[rel.get("Relationship ID")] = rel
    
    # 4. ì»¬ëŸ¼ ë…¸ë“œ ìƒì„±
    if columns_data:
        if emit_progress:
            yield emit_message(f"      ğŸ“¦ [4/6] ì»¬ëŸ¼ {len(columns_data)}ê°œ ìƒì„± ì¤‘...")
        column_query = """
        UNWIND $items AS item
        MERGE (__cy_c__:Column {fqn: item.fqn})
        SET __cy_c__.name = item.name,
            __cy_c__.dtype = item.dtype,
            __cy_c__.description = item.description,
            __cy_c__.description_source = item.description_source,
            __cy_c__.nullable = item.nullable,
            __cy_c__.pk_constraint = CASE WHEN item.pk_constraint IS NOT NULL THEN item.pk_constraint ELSE __cy_c__.pk_constraint END
        RETURN __cy_c__
        """
        async with cypher_lock:
            result = await client.run_batch_unwind(column_query, columns_data)
        for node in result.get("Nodes", []):
            all_nodes[node.get("Node ID")] = node
    
    # 5. í…Œì´ë¸”-ì»¬ëŸ¼ ê´€ê³„ ìƒì„±
    if columns_data:
        if emit_progress:
            yield emit_message(f"      ğŸ“¦ [5/6] í…Œì´ë¸”-ì»¬ëŸ¼ ê´€ê³„ {len(columns_data)}ê°œ ìƒì„± ì¤‘...")
        has_column_query = """
        UNWIND $items AS item
        MATCH (__cy_t__:Table {db: item.table_db, schema: item.table_schema, name: item.table_name})
        MATCH (__cy_c__:Column {fqn: item.fqn})
        MERGE (__cy_t__)-[__cy_r__:HAS_COLUMN]->(__cy_c__)
        RETURN __cy_t__, __cy_r__, __cy_c__
        """
        async with cypher_lock:
            result = await client.run_batch_unwind(has_column_query, columns_data)
        for node in result.get("Nodes", []):
            all_nodes[node.get("Node ID")] = node
        for rel in result.get("Relationships", []):
            all_relationships[rel.get("Relationship ID")] = rel
    
    # 6. FK ê´€ê³„ ìƒì„± (ì°¸ì¡° í…Œì´ë¸” MERGE + FK ê´€ê³„)
    if fks_data:
        if emit_progress:
            yield emit_message(f"      ğŸ“¦ [6/6] FK ê´€ê³„ {len(fks_data)}ê°œ ìƒì„± ì¤‘...")
        # ë¨¼ì € ì°¸ì¡° í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±
        ref_tables_query = """
        UNWIND $items AS item
        MERGE (__cy_rt__:Table {db: item.to_db, schema: item.to_schema, name: item.to_table})
        RETURN __cy_rt__
        """
        async with cypher_lock:
            result = await client.run_batch_unwind(ref_tables_query, fks_data)
        for node in result.get("Nodes", []):
            all_nodes[node.get("Node ID")] = node
        
        # FK ê´€ê³„ ìƒì„±
        fk_query = """
        UNWIND $items AS item
        MATCH (__cy_t__:Table {db: item.from_db, schema: item.from_schema, name: item.from_table})
        MATCH (__cy_rt__:Table {db: item.to_db, schema: item.to_schema, name: item.to_table})
        MERGE (__cy_t__)-[__cy_r__:FK_TO_TABLE {sourceColumn: item.from_column, targetColumn: item.to_column}]->(__cy_rt__)
        ON CREATE SET __cy_r__.type = 'many_to_one', __cy_r__.source = 'ddl'
        RETURN __cy_t__, __cy_r__, __cy_rt__
        """
        async with cypher_lock:
            result = await client.run_batch_unwind(fk_query, fks_data)
        for node in result.get("Nodes", []):
            all_nodes[node.get("Node ID")] = node
        for rel in result.get("Relationships", []):
            all_relationships[rel.get("Relationship ID")] = rel
    
    if emit_progress:
        yield emit_message(f"   âœ… UNWIND ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: {len(all_nodes)}ê°œ ë…¸ë“œ, {len(all_relationships)}ê°œ ê´€ê³„")
        yield emit_phase_event(
            phase_num=0,
            phase_name="DDL ì²˜ë¦¬",
            status="in_progress",
            progress=saving_end,
            details={
                "step": "unwind_completed",
                "nodes_created": len(all_nodes),
                "relationships_created": len(all_relationships)
            }
        )
    
    result = {
        "Nodes": list(all_nodes.values()),
        "Relationships": list(all_relationships.values())
    }
    
    if emit_progress:
        yield emit_message(f"   âœ… Neo4j ì €ì¥ ì™„ë£Œ: {len(result['Nodes'])}ê°œ ë…¸ë“œ, {len(result['Relationships'])}ê°œ ê´€ê³„ ìƒì„±")
        yield emit_phase_event(
            phase_num=0,
            phase_name="DDL ì²˜ë¦¬",
            status="in_progress",
            progress=saving_end,
            details={
                "step": "neo4j_saved",
                "tables": ddl_stats['tables'],
                "columns": ddl_stats['columns'],
                "fks": ddl_stats['fks'],
                "nodes_created": len(result['Nodes']),
                "relationships_created": len(result['Relationships'])
            }
        )
    
    log_process("ANALYZE", "DDL", f"DDL ì²˜ë¦¬ ì™„ë£Œ: {file_name} (T:{ddl_stats['tables']}, C:{ddl_stats['columns']}, FK:{ddl_stats['fks']})")
    
    # ìµœì¢… ê²°ê³¼ë¥¼ íŠ¹ë³„í•œ í˜•íƒœë¡œ yield (tuple)
    yield (result, ddl_stats)


async def run_ddl_phase(
    analyzer: Any,
    client: Neo4jClient,
    orchestrator: Any,
    stats: AnalysisStats,
) -> AsyncGenerator[bytes, None]:
    """DDL íŒŒì¼ ì²˜ë¦¬ - í…Œì´ë¸”/ì»¬ëŸ¼ ìŠ¤í‚¤ë§ˆ ìƒì„±
    
    Args:
        analyzer: DbmsAnalyzer ì¸ìŠ¤í„´ìŠ¤ (emit_* ë©”ì„œë“œ, ê³µìœ  ìƒíƒœ ì ‘ê·¼ìš©)
        client: Neo4j í´ë¼ì´ì–¸íŠ¸
        orchestrator: ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
        stats: ë¶„ì„ í†µê³„
    """
    ddl_files = list_ddl_files(orchestrator)
    
    if not ddl_files:
        yield analyzer.emit_skip("DDL íŒŒì¼ ì—†ìŒ â†’ ìŠ¤í‚¤ë§ˆ ì²˜ë¦¬ ê±´ë„ˆëœ€")
        return
    
    ddl_count = len(ddl_files)
    yield emit_message("")
    yield analyzer.emit_separator()
    yield analyzer.emit_phase_header(0, "ğŸ“‹ DDL ìŠ¤í‚¤ë§ˆ ìˆ˜ì§‘", f"{ddl_count}ê°œ DDL")
    yield analyzer.emit_separator()
    
    ddl_dir = orchestrator.dirs["ddl"]
    
    for idx, ddl_file in enumerate(ddl_files, 1):
        yield emit_message("")
        yield analyzer.emit_file_start(idx, ddl_count, ddl_file)
        
        # íŒŒì¼ ë‹¨ìœ„ ì§„í–‰ë¥ : ê° íŒŒì¼ì´ (idx-1)/ddl_count ~ idx/ddl_count êµ¬ê°„ ì°¨ì§€
        file_base_progress = int(((idx - 1) / ddl_count) * 100)
        file_end_progress = int((idx / ddl_count) * 100)
        
        # _process_ddlì€ ì´ì œ AsyncGenerator - ë©”ì‹œì§€ì™€ ìµœì¢… ê²°ê³¼ë¥¼ yield
        ddl_graph = None
        ddl_stats_file = {"tables": 0, "columns": 0, "fks": 0}
        
        async for item in process_ddl(
            ddl_path=os.path.join(ddl_dir, ddl_file),
            client=client,
            file_name=ddl_file,
            orchestrator=orchestrator,
            cypher_lock=analyzer._cypher_lock,
            ddl_schemas=analyzer._ddl_schemas,
            ddl_table_metadata=analyzer._ddl_table_metadata,
            emit_progress=True,
            file_base_progress=file_base_progress,
            file_end_progress=file_end_progress,
        ):
            if isinstance(item, tuple):
                # ìµœì¢… ê²°ê³¼ (ddl_graph, ddl_stats)
                ddl_graph, ddl_stats_file = item
            else:
                # ì§„í–‰ ìƒí™© ë©”ì‹œì§€ (bytes)
                yield item
        
        if ddl_stats_file["tables"]:
            yield emit_message(f"   âœ“ Table ë…¸ë“œ: {ddl_stats_file['tables']}ê°œ")
        if ddl_stats_file["columns"]:
            yield emit_message(f"   âœ“ Column ë…¸ë“œ: {ddl_stats_file['columns']}ê°œ")
        if ddl_stats_file["fks"]:
            yield emit_message(f"   âœ“ FK ê´€ê³„: {ddl_stats_file['fks']}ê°œ")
        
        # íŒŒì¼ ì™„ë£Œ ì‹œ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        yield emit_phase_event(0, "DDL ì²˜ë¦¬", "running", file_end_progress)
        
        stats.add_ddl_result(ddl_stats_file["tables"], ddl_stats_file["columns"], ddl_stats_file["fks"])
        
        if ddl_graph and (ddl_graph.get("Nodes") or ddl_graph.get("Relationships")):
            yield emit_data(
                graph=ddl_graph,
                line_number=0,
                analysis_progress=0,
                current_file=f"DDL-{ddl_file}",
            )
    
    yield emit_message("")
    yield emit_message("ğŸ“Š DDL ì²˜ë¦¬ ì™„ë£Œ:")
    yield emit_message(f"   â€¢ í…Œì´ë¸”: {stats.ddl_tables}ê°œ")
    yield emit_message(f"   â€¢ ì»¬ëŸ¼: {stats.ddl_columns}ê°œ")
    yield emit_message(f"   â€¢ FK: {stats.ddl_fks}ê°œ")

