"""DBMS ì½”ë“œ ë¶„ì„ ì „ëµ - PL/SQL, í”„ë¡œì‹œì €, í•¨ìˆ˜ ë“±

AST ê¸°ë°˜ PL/SQL ì½”ë“œ ë¶„ì„ â†’ Neo4j ê·¸ë˜í”„ ìƒì„±.

ë¶„ì„ íë¦„ (Frameworkì™€ ë™ì¼í•œ 2ë‹¨ê³„ + DDL):
1. [Phase 1] DDL ì²˜ë¦¬ + ëª¨ë“  íŒŒì¼ AST ê·¸ë˜í”„ ìƒì„± (ë³‘ë ¬)
2. [Phase 2] ëª¨ë“  íŒŒì¼ LLM ë¶„ì„ (ë³‘ë ¬)
3. [Phase 3] User Story ë¬¸ì„œ ìƒì„± (BaseStreamingAnalyzer ê³µí†µ)
"""

import asyncio
import json
import logging
import os
from typing import Any, AsyncGenerator, Optional, List, Dict, Tuple

import aiofiles

from analyzer.neo4j_client import Neo4jClient
from analyzer.strategy.base_analyzer import BaseStreamingAnalyzer, AnalysisStats
from analyzer.strategy.base.file_context import FileStatus, FileAnalysisContext
from analyzer.strategy.dbms.ast_processor import DbmsAstProcessor
from analyzer.pipeline_control import pipeline_controller, PipelinePhase
from config.settings import settings
from util.exception import AnalysisError
from util.rule_loader import RuleLoader
from util.utility_tool import escape_for_cypher
from util.stream_utils import (
    emit_data,
    emit_message,
    emit_phase_event,
    format_graph_result,
)
from util.utility_tool import (
    escape_for_cypher,
    log_process,
    parse_table_identifier,
    generate_user_story_document,
    split_ddl_into_chunks,
    calculate_code_token,
)
from util.embedding_client import EmbeddingClient
from util.ddl_parser import parse_ddl as regex_parse_ddl
from analyzer.lineage_analyzer import LineageAnalyzer, LineageInfo


class DbmsAnalyzer(BaseStreamingAnalyzer):
    """DBMS ì½”ë“œ ë¶„ì„ ì „ëµ
    
    2ë‹¨ê³„ ë¶„ì„ + DDL ì²˜ë¦¬ (Frameworkì™€ ë™ì¼):
    - Phase 1: DDL ì²˜ë¦¬ + ëª¨ë“  íŒŒì¼ AST ê·¸ë˜í”„ ìƒì„± (ë³‘ë ¬)
    - Phase 2: ëª¨ë“  íŒŒì¼ LLM ë¶„ì„ (ë³‘ë ¬) - Phase1 ì‹¤íŒ¨ íŒŒì¼ ì œì™¸
    - Phase 3: User Story ë¬¸ì„œ ìƒì„± (ë¶€ëª¨ í´ë˜ìŠ¤ ê³µí†µ)
    """

    # =========================================================================
    # ì „ëµ ë©”íƒ€ë°ì´í„° (BaseStreamingAnalyzer êµ¬í˜„)
    # =========================================================================
    
    @property
    def strategy_name(self) -> str:
        return "DBMS"
    
    @property
    def strategy_emoji(self) -> str:
        return "ğŸ—„ï¸"
    
    @property
    def file_type_description(self) -> str:
        return "SQL íŒŒì¼"

    def __init__(self):
        self._cypher_lock = asyncio.Lock()
        self._file_semaphore: Optional[asyncio.Semaphore] = None
        self._ddl_schemas: set[str] = set()  # DDLì—ì„œ ìˆ˜ì§‘ëœ ìŠ¤í‚¤ë§ˆ Set
        # DDL ë©”íƒ€ë°ì´í„° ìºì‹œ: {(schema, table_name): {description, columns}}
        self._ddl_table_metadata: Dict[Tuple[str, str], Dict[str, Any]] = {}

    # =========================================================================
    # ë©”ì¸ íŒŒì´í”„ë¼ì¸ (BaseStreamingAnalyzer êµ¬í˜„)
    # =========================================================================

    async def run_pipeline(
        self,
        file_names: list[tuple[str, str]],
        client: Neo4jClient,
        orchestrator: Any,
        stats: AnalysisStats,
    ) -> AsyncGenerator[bytes, None]:
        """DBMS ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        íë¦„ (Frameworkì™€ ë™ì¼):
        1. DDL ì²˜ë¦¬ + íŒŒì¼ ë¡œë“œ (ë³‘ë ¬)
        2. Phase 1: ëª¨ë“  íŒŒì¼ AST ê·¸ë˜í”„ ìƒì„± (ë³‘ë ¬)
        3. Phase 2: ëª¨ë“  íŒŒì¼ LLM ë¶„ì„ (ë³‘ë ¬) - Phase1 ì‹¤íŒ¨ íŒŒì¼ ì œì™¸
        
        Note: User Story PhaseëŠ” ë¶€ëª¨ í´ë˜ìŠ¤ì—ì„œ ì²˜ë¦¬
        """
        total_files = len(file_names)
        self._file_semaphore = asyncio.Semaphore(settings.concurrency.file_concurrency)
        
        # íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì´ˆê¸°í™”
        pipeline_controller.reset()
        pipeline_state = pipeline_controller.get_state()

        # LLM ìºì‹œ ìƒíƒœ í‘œì‹œ
        if settings.llm.cache_enabled:
            cache_path = settings.llm.cache_db_path
            if not os.path.isabs(cache_path):
                cache_path = os.path.join(settings.path.base_dir, cache_path)
            cache_exists = os.path.exists(cache_path)
            cache_size = os.path.getsize(cache_path) if cache_exists else 0
            cache_size_str = f"{cache_size / 1024:.1f}KB" if cache_size < 1024*1024 else f"{cache_size / (1024*1024):.1f}MB"
            yield emit_message(f"ğŸ—„ï¸ LLM ìºì‹œ: í™œì„±í™” ({cache_size_str if cache_exists else 'ì‹ ê·œ'})")
        else:
            yield emit_message("ğŸ”„ LLM ìºì‹œ: ë¹„í™œì„±í™” (ë§¤ë²ˆ ìƒˆë¡œìš´ LLM í˜¸ì¶œ)")

        if total_files > 0:
            yield emit_message(f"âš¡ ë³‘ë ¬ ì²˜ë¦¬: íŒŒì¼ {settings.concurrency.file_concurrency}ê°œ ë™ì‹œ")

        # ========== Phase 0: DDL ì²˜ë¦¬ ==========
        pipeline_state.set_phase(PipelinePhase.DDL_PROCESSING, "DDL íŒŒì¼ ì²˜ë¦¬ ì¤‘", 0)
        yield emit_phase_event(0, "DDL ì²˜ë¦¬", "started", 0, {"canPause": True})
        
        async for chunk in self._run_ddl_phase(client, orchestrator, stats):
            yield chunk
        
        yield emit_phase_event(0, "DDL ì²˜ë¦¬", "completed", 100)
        
        # DDL í›„ ì¼ì‹œì •ì§€ ì²´í¬
        if not await pipeline_state.wait_if_paused():
            yield emit_message("â¹ï¸ íŒŒì´í”„ë¼ì¸ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
            pipeline_state.set_phase(PipelinePhase.CANCELLED)
            return

        # DDLë§Œ ìˆëŠ” ê²½ìš° (ì†ŒìŠ¤ íŒŒì¼ ì—†ìŒ) - Phase 1,2 ìŠ¤í‚µ
        if total_files == 0:
            yield emit_message("")
            yield emit_message("ğŸ“‹ DDL íŒŒì¼ë§Œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤ (ì†ŒìŠ¤ íŒŒì¼ ì—†ìŒ)")
            pipeline_state.set_phase(PipelinePhase.COMPLETED)
            return

        # ========== Phase 1: AST ê·¸ë˜í”„ ìƒì„± ==========
        pipeline_state.set_phase(PipelinePhase.AST_GENERATION, "AST êµ¬ì¡° ê·¸ë˜í”„ ìƒì„± ì¤‘", 0)
        yield emit_phase_event(1, "AST êµ¬ì¡° ìƒì„±", "started", 0, {"canPause": True})
        
        yield emit_message("")
        yield self.emit_separator()
        yield self.emit_phase_header(1, "ğŸ—ï¸ AST êµ¬ì¡° ê·¸ë˜í”„ ìƒì„±", f"{total_files}ê°œ íŒŒì¼ ë³‘ë ¬")
        yield self.emit_separator()

        contexts = await self._load_all_files(file_names, orchestrator)
        yield emit_message(f"   âœ“ {len(contexts)}ê°œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ")

        async for chunk in self._run_phase1(contexts, client, orchestrator, stats):
            yield chunk

        # Phase 1 ê²°ê³¼ ìš”ì•½
        ph1_ok_count = sum(1 for c in contexts if c.status == FileStatus.PH1_OK)
        ph1_fail_count = sum(1 for c in contexts if c.status == FileStatus.PH1_FAIL)
        
        yield emit_message("")
        yield self.emit_phase_complete(1, f"{stats.static_nodes_created}ê°œ ë…¸ë“œ ìƒì„±")
        yield emit_phase_event(1, "AST êµ¬ì¡° ìƒì„±", "completed", 100, {"nodes": stats.static_nodes_created})
        
        if ph1_fail_count > 0:
            yield self.emit_warning(f"Phase 1 ì‹¤íŒ¨: {ph1_fail_count}ê°œ íŒŒì¼ â†’ Phase 2 ìŠ¤í‚µ (í† í° ì ˆê°)")

        # Phase 1 í›„ ì¼ì‹œì •ì§€ ì²´í¬
        if not await pipeline_state.wait_if_paused():
            yield emit_message("â¹ï¸ íŒŒì´í”„ë¼ì¸ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
            pipeline_state.set_phase(PipelinePhase.CANCELLED)
            return

        # ========== Phase 2: LLM ë¶„ì„ ==========
        ph2_targets = [c for c in contexts if c.status == FileStatus.PH1_OK]
        
        pipeline_state.set_phase(PipelinePhase.LLM_ANALYSIS, "AI ë¶„ì„ ì¤‘", 0)
        yield emit_phase_event(2, "AI ë¶„ì„", "started", 0, {"canPause": True, "files": len(ph2_targets)})
        
        yield emit_message("")
        yield self.emit_separator()
        yield self.emit_phase_header(2, "ğŸ¤– AI ë¶„ì„", f"{len(ph2_targets)}ê°œ íŒŒì¼ ë³‘ë ¬")
        yield self.emit_separator()
        
        if ph1_fail_count > 0:
            yield emit_message(f"   â„¹ï¸ {ph1_fail_count}ê°œ íŒŒì¼ì€ Phase 1 ì‹¤íŒ¨ë¡œ ìŠ¤í‚µë¨ (í† í° ì ˆê°)")

        async for chunk in self._run_phase2(ph2_targets, client, orchestrator, stats):
            yield chunk

        yield emit_message("")
        yield self.emit_phase_complete(2, f"{stats.llm_batches_executed}ê°œ ë¶„ì„ ì™„ë£Œ")
        yield emit_phase_event(2, "AI ë¶„ì„", "completed", 100, {"batches": stats.llm_batches_executed})
        
        # Phase 2 í›„ ì¼ì‹œì •ì§€ ì²´í¬
        if not await pipeline_state.wait_if_paused():
            yield emit_message("â¹ï¸ íŒŒì´í”„ë¼ì¸ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
            pipeline_state.set_phase(PipelinePhase.CANCELLED)
            return
        
        # ========== Phase 3: í…Œì´ë¸”/ì»¬ëŸ¼ ì„¤ëª… ë³´ê°• ==========
        # Note: ì‹¤ì œ í…Œì´ë¸” ìš”ì•½ì€ Phase 2ì˜ run_llm_analysis ë‚´ì—ì„œ ì´ë¯¸ ìˆ˜í–‰ë¨
        # ì—¬ê¸°ì„œëŠ” ì§„í–‰ ìƒíƒœë§Œ í‘œì‹œ
        pipeline_state.set_phase(PipelinePhase.TABLE_ENRICHMENT, "í…Œì´ë¸”/ì»¬ëŸ¼ ì„¤ëª… ë³´ê°• ì¤‘", 0)
        yield emit_phase_event(3, "í…Œì´ë¸” ì„¤ëª… ë³´ê°•", "started", 0, {"canPause": True})
        yield self.emit_phase_header(3, "ğŸ“Š í…Œì´ë¸”/ì»¬ëŸ¼ ì„¤ëª… ë³´ê°•", "LLM ë¶„ì„ ê²°ê³¼ ì ìš©")
        
        # í…Œì´ë¸” ìš”ì•½ ê²°ê³¼ ì¹´ìš´íŠ¸ (ì´ë¯¸ Phase 2ì—ì„œ ìˆ˜í–‰ë¨)
        table_count = sum(
            1 for ctx in ph2_targets 
            if ctx.processor and hasattr(ctx.processor, '_table_summary_store') 
            and ctx.processor._table_summary_store
        )
        
        yield emit_message(f"   âœ… í…Œì´ë¸”/ì»¬ëŸ¼ ì„¤ëª… ë³´ê°• ì™„ë£Œ")
        yield self.emit_phase_complete(3, "ì„¤ëª… ë³´ê°• ì™„ë£Œ")
        yield emit_phase_event(3, "í…Œì´ë¸” ì„¤ëª… ë³´ê°•", "completed", 100)
        
        # Phase 3 í›„ ì¼ì‹œì •ì§€ ì²´í¬
        if not await pipeline_state.wait_if_paused():
            yield emit_message("â¹ï¸ íŒŒì´í”„ë¼ì¸ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
            pipeline_state.set_phase(PipelinePhase.CANCELLED)
            return

        # ========== Phase 4: ë²¡í„°ë¼ì´ì§• (ì„ë² ë”© ìƒì„±) ==========
        pipeline_state.set_phase(PipelinePhase.VECTORIZING, "í…Œì´ë¸”/ì»¬ëŸ¼ ë²¡í„°ë¼ì´ì§• ì¤‘", 0)
        yield emit_phase_event(4, "ë²¡í„°ë¼ì´ì§•", "started", 0, {"canPause": True})
        yield emit_message("")
        yield self.emit_separator()
        yield self.emit_phase_header(4, "ğŸ”¢ ë²¡í„°ë¼ì´ì§•", "ì„ë² ë”© ìƒì„±")
        yield self.emit_separator()
        
        async for chunk in self._run_vectorize_phase(client, orchestrator, stats):
            yield chunk
        
        yield emit_message("")
        yield self.emit_phase_complete(4, "ë²¡í„°ë¼ì´ì§• ì™„ë£Œ")
        yield emit_phase_event(4, "ë²¡í„°ë¼ì´ì§•", "completed", 100, {
            "tables_vectorized": stats.tables_vectorized,
            "columns_vectorized": stats.columns_vectorized
        })
        
        # Phase 4 í›„ ì¼ì‹œì •ì§€ ì²´í¬
        if not await pipeline_state.wait_if_paused():
            yield emit_message("â¹ï¸ íŒŒì´í”„ë¼ì¸ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
            pipeline_state.set_phase(PipelinePhase.CANCELLED)
            return
        
        # ========== Phase 5: ë¦¬ë‹ˆì§€ ë¶„ì„ (ETL íŒ¨í„´ ê°ì§€) ==========
        pipeline_state.set_phase(PipelinePhase.LINEAGE_ANALYSIS, "ë°ì´í„° ë¦¬ë‹ˆì§€ ë¶„ì„ ì¤‘", 0)
        yield emit_phase_event(5, "ë¦¬ë‹ˆì§€ ë¶„ì„", "started", 0, {"canPause": True})
        yield emit_message("")
        yield self.emit_separator()
        yield self.emit_phase_header(5, "ğŸ”— ë°ì´í„° ë¦¬ë‹ˆì§€ ë¶„ì„", "ETL íŒ¨í„´ ê°ì§€")
        yield self.emit_separator()
        
        async for chunk in self._run_lineage_phase(client, orchestrator, stats):
            yield chunk
        
        yield emit_message("")
        yield self.emit_phase_complete(5, "ë¦¬ë‹ˆì§€ ë¶„ì„ ì™„ë£Œ")
        yield emit_phase_event(5, "ë¦¬ë‹ˆì§€ ë¶„ì„", "completed", 100, {
            "etl_count": getattr(stats, 'etl_count', 0),
            "data_flows": getattr(stats, 'data_flows', 0)
        })
        
        # Phase 5 í›„ ì¼ì‹œì •ì§€ ì²´í¬
        if not await pipeline_state.wait_if_paused():
            yield emit_message("â¹ï¸ íŒŒì´í”„ë¼ì¸ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
            pipeline_state.set_phase(PipelinePhase.CANCELLED)
            return

    # =========================================================================
    # User Story ë¬¸ì„œ ìƒì„± (BaseStreamingAnalyzer êµ¬í˜„)
    # =========================================================================

    async def build_user_story_doc(
        self,
        client: Neo4jClient,
        orchestrator: Any,
    ) -> Optional[str]:
        """ë¶„ì„ëœ í”„ë¡œì‹œì €ì—ì„œ User Story ë¬¸ì„œ ìƒì„±"""
        query = """
            MATCH (__cy_n__)
            WHERE (__cy_n__:PROCEDURE OR __cy_n__:FUNCTION OR __cy_n__:TRIGGER)
              AND __cy_n__.summary IS NOT NULL
            OPTIONAL MATCH (__cy_n__)-[:HAS_USER_STORY]->(__cy_us__:UserStory)
            OPTIONAL MATCH (__cy_us__)-[:HAS_AC]->(__cy_ac__:AcceptanceCriteria)
            WITH __cy_n__, __cy_us__, collect(DISTINCT {
                id: __cy_ac__.id,
                title: __cy_ac__.title,
                given: __cy_ac__.given,
                when: __cy_ac__.when,
                then: __cy_ac__.then
            }) AS acceptance_criteria
            WITH __cy_n__, collect(DISTINCT {
                id: __cy_us__.id,
                role: __cy_us__.role,
                goal: __cy_us__.goal,
                benefit: __cy_us__.benefit,
                acceptance_criteria: acceptance_criteria
            }) AS user_stories
            RETURN __cy_n__.procedure_name AS name, 
                   __cy_n__.summary AS summary,
                   user_stories AS user_stories, 
                   labels(__cy_n__)[0] AS type
            ORDER BY __cy_n__.file_name, __cy_n__.startLine
        """
        
        async with self._cypher_lock:
            results = await client.execute_queries([query])
        
        # DDLë§Œ ìˆëŠ” ê²½ìš° ë˜ëŠ” ë¶„ì„ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° None ë°˜í™˜ (ì˜¤ë¥˜ ëŒ€ì‹ )
        if not results or not results[0]:
            log_process("ANALYZE", "USER_STORY", "User Story ìƒì„± ìŠ¤í‚µ: ë¶„ì„ëœ í”„ë¡œì‹œì €/í•¨ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤", logging.INFO)
            return None
        
        filtered = [
            r for r in results[0]
            if r.get("summary") or (r.get("user_stories") and len(r["user_stories"]) > 0)
        ]
        
        if not filtered:
            return None
        
        log_process("ANALYZE", "USER_STORY", f"User Story ìƒì„± | ëŒ€ìƒ={len(filtered)}ê°œ í”„ë¡œì‹œì €")
        return generate_user_story_document(
            results=filtered,
            source_name="ROBO",
            source_type="DBMS í”„ë¡œì‹œì €/í•¨ìˆ˜",
        )

    # =========================================================================
    # DDL ì²˜ë¦¬
    # =========================================================================

    async def _run_ddl_phase(
        self,
        client: Neo4jClient,
        orchestrator: Any,
        stats: AnalysisStats,
    ) -> AsyncGenerator[bytes, None]:
        """DDL íŒŒì¼ ì²˜ë¦¬ - í…Œì´ë¸”/ì»¬ëŸ¼ ìŠ¤í‚¤ë§ˆ ìƒì„±"""
        ddl_files = self._list_ddl_files(orchestrator)
        
        if not ddl_files:
            yield self.emit_skip("DDL íŒŒì¼ ì—†ìŒ â†’ ìŠ¤í‚¤ë§ˆ ì²˜ë¦¬ ê±´ë„ˆëœ€")
            return
        
        ddl_count = len(ddl_files)
        yield emit_message("")
        yield self.emit_separator()
        yield self.emit_phase_header(0, "ğŸ“‹ DDL ìŠ¤í‚¤ë§ˆ ìˆ˜ì§‘", f"{ddl_count}ê°œ DDL")
        yield self.emit_separator()
        
        ddl_dir = orchestrator.dirs["ddl"]
        
        for idx, ddl_file in enumerate(ddl_files, 1):
            yield emit_message("")
            yield self.emit_file_start(idx, ddl_count, ddl_file)
            
            # íŒŒì¼ ë‹¨ìœ„ ì§„í–‰ë¥ : ê° íŒŒì¼ì´ (idx-1)/ddl_count ~ idx/ddl_count êµ¬ê°„ ì°¨ì§€
            file_base_progress = int(((idx - 1) / ddl_count) * 100)
            file_end_progress = int((idx / ddl_count) * 100)
            
            # _process_ddlì€ ì´ì œ AsyncGenerator - ë©”ì‹œì§€ì™€ ìµœì¢… ê²°ê³¼ë¥¼ yield
            ddl_graph = None
            ddl_stats = {"tables": 0, "columns": 0, "fks": 0}
            
            async for item in self._process_ddl(
                ddl_path=os.path.join(ddl_dir, ddl_file),
                client=client,
                file_name=ddl_file,
                orchestrator=orchestrator,
                emit_progress=True,
                file_base_progress=file_base_progress,
                file_end_progress=file_end_progress,
            ):
                if isinstance(item, tuple):
                    # ìµœì¢… ê²°ê³¼ (ddl_graph, ddl_stats)
                    ddl_graph, ddl_stats = item
                else:
                    # ì§„í–‰ ìƒí™© ë©”ì‹œì§€ (bytes)
                    yield item
            
            if ddl_stats["tables"]:
                yield emit_message(f"   âœ“ Table ë…¸ë“œ: {ddl_stats['tables']}ê°œ")
            if ddl_stats["columns"]:
                yield emit_message(f"   âœ“ Column ë…¸ë“œ: {ddl_stats['columns']}ê°œ")
            if ddl_stats["fks"]:
                yield emit_message(f"   âœ“ FK ê´€ê³„: {ddl_stats['fks']}ê°œ")
            
            # íŒŒì¼ ì™„ë£Œ ì‹œ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            yield emit_phase_event(0, "DDL ì²˜ë¦¬", "running", file_end_progress)
            
            stats.add_ddl_result(ddl_stats["tables"], ddl_stats["columns"], ddl_stats["fks"])
            
            if ddl_graph and (ddl_graph.get("Nodes") or ddl_graph.get("Relationships")):
                yield emit_data(
                    graph=ddl_graph,
                    line_number=0,
                    analysis_progress=0,
                    current_file=f"DDL-{ddl_file}",
                )
        
        yield emit_message("")
        yield emit_message("ğŸ“Š DDL ì²˜ë¦¬ ì™„ë£Œ:")
        yield emit_message(f"   â€¢ í…Œì´ë¸”: {stats.ddl_tables}ê°œ")
        yield emit_message(f"   â€¢ ì»¬ëŸ¼: {stats.ddl_columns}ê°œ")
        yield emit_message(f"   â€¢ FK: {stats.ddl_fks}ê°œ")

    def _list_ddl_files(self, orchestrator: Any) -> list[str]:
        """DDL íŒŒì¼ ëª©ë¡ ì¡°íšŒ
        
        DDL ë””ë ‰í† ë¦¬ê°€ ì—†ê±°ë‚˜ íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ê²½ê³  ì²˜ë¦¬, ì—ëŸ¬ ì•„ë‹˜)
        """
        ddl_dir = orchestrator.dirs.get("ddl", "")
        if not ddl_dir:
            log_process("ANALYZE", "DDL", "DDL ë””ë ‰í† ë¦¬ ì„¤ì • ì—†ìŒ - DDL ì²˜ë¦¬ ìƒëµ")
            return []
        if not os.path.isdir(ddl_dir):
            # DDL ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ê²½ê³ ë§Œ í•˜ê³  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            log_process("ANALYZE", "DDL", f"DDL ë””ë ‰í† ë¦¬ ì—†ìŒ: {ddl_dir} - DDL ì²˜ë¦¬ ìƒëµ")
            return []
        try:
            files = sorted(
                f for f in os.listdir(ddl_dir)
                if os.path.isfile(os.path.join(ddl_dir, f))
            )
            if not files:
                # DDL íŒŒì¼ì´ ì—†ìœ¼ë©´ ê²½ê³ ë§Œ í•˜ê³  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
                log_process("ANALYZE", "DDL", f"DDL ë””ë ‰í† ë¦¬ì— íŒŒì¼ ì—†ìŒ: {ddl_dir} - DDL ì²˜ë¦¬ ìƒëµ")
                return []
            log_process("ANALYZE", "DDL", f"DDL íŒŒì¼ ë°œê²¬: {len(files)}ê°œ")
            return files
        except OSError as e:
            log_process("ANALYZE", "DDL", f"DDL ë””ë ‰í† ë¦¬ ì½ê¸° ì‹¤íŒ¨: {ddl_dir} - {e}")
            return []

    def _apply_name_case(self, name: str, name_case: str) -> str:
        """ë©”íƒ€ë°ì´í„° ëŒ€ì†Œë¬¸ì ë³€í™˜ ì ìš©
        
        Args:
            name: ë³€í™˜í•  ì´ë¦„ (í…Œì´ë¸”ëª…, ì»¬ëŸ¼ëª…, ìŠ¤í‚¤ë§ˆëª… ë“±)
            name_case: ë³€í™˜ ì˜µì…˜ (original, uppercase, lowercase)
        
        Returns:
            ë³€í™˜ëœ ì´ë¦„
        """
        if not name:
            return name
        if name_case == "uppercase":
            return name.upper()
        elif name_case == "lowercase":
            return name.lower()
        return name  # original: ê·¸ëŒ€ë¡œ ë°˜í™˜

    async def _process_ddl(
        self,
        ddl_path: str,
        client: Neo4jClient,
        file_name: str,
        orchestrator: Any,
        emit_progress: bool = True,
        use_llm: bool = False,  # ê¸°ë³¸ê°’: ì •ê·œì‹ íŒŒì„œ ì‚¬ìš© (ë¹ ë¦„)
        file_base_progress: int = 0,  # íŒŒì¼ ì‹œì‘ ì§„í–‰ë¥ 
        file_end_progress: int = 100,  # íŒŒì¼ ì¢…ë£Œ ì§„í–‰ë¥ 
    ) -> AsyncGenerator[bytes | tuple[dict, dict], None]:
        """DDL íŒŒì¼ ì²˜ë¦¬ ë° í…Œì´ë¸”/ì»¬ëŸ¼ ë…¸ë“œ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°)
        
        Args:
            use_llm: Trueë©´ LLM ì‚¬ìš©, Falseë©´ ì •ê·œì‹ íŒŒì„œ ì‚¬ìš© (ê¸°ë³¸: False, ë¹ ë¥¸ íŒŒì‹±)
            file_base_progress: ì´ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘ ì‹œ ì „ì²´ ì§„í–‰ë¥  (0-100)
            file_end_progress: ì´ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ ì‹œ ì „ì²´ ì§„í–‰ë¥  (0-100)
        
        Yields:
            bytes: ì§„í–‰ ìƒí™© ë©”ì‹œì§€ (emit_message)
            tuple[dict, dict]: ìµœì¢… ê²°ê³¼ (ddl_graph, ddl_stats) - ë§ˆì§€ë§‰ì— í•œ ë²ˆë§Œ
        """
        import re
        ddl_stats = {"tables": 0, "columns": 0, "fks": 0}
        
        # ì§„í–‰ë¥  ë²”ìœ„ ê³„ì‚° (íŒŒì¼ ë‚´ì—ì„œ íŒŒì‹± 50%, ì €ì¥ 50% ë¹„ìœ¨)
        file_range = file_end_progress - file_base_progress
        parsing_end = file_base_progress + int(file_range * 0.5)
        saving_start = parsing_end
        saving_end = file_end_progress
        
        async with aiofiles.open(ddl_path, "r", encoding="utf-8") as f:
            ddl_content = await f.read()
        
        total_tokens = calculate_code_token(ddl_content)
        
        # ========================================
        # ì •ê·œì‹ íŒŒì„œ ì‚¬ìš© (ê¸°ë³¸ê°’ - ë¹ ë¦„)
        # ========================================
        if not use_llm:
            if emit_progress:
                yield emit_message(f"   âš¡ ì •ê·œì‹ íŒŒì„œ ì‚¬ìš© (ë¹ ë¥¸ ëª¨ë“œ)")
                yield emit_phase_event(
                    phase_num=0,
                    phase_name="DDL ì²˜ë¦¬",
                    status="in_progress",
                    progress=file_base_progress + int(file_range * 0.1),
                    details={"mode": "regex", "tokens": total_tokens}
                )
            
            try:
                # ì •ê·œì‹ íŒŒì„œë¡œ í•œ ë²ˆì— íŒŒì‹± (ë§¤ìš° ë¹ ë¦„)
                parsed = await asyncio.to_thread(regex_parse_ddl, ddl_content)
                all_parsed_results = parsed.get("analysis", [])
                
                table_count = len(all_parsed_results)
                if emit_progress:
                    # ì²˜ìŒ 5ê°œ í…Œì´ë¸”ëª… ë¯¸ë¦¬ë³´ê¸°
                    table_names = [t.get("table", {}).get("name", "?") for t in all_parsed_results[:5]]
                    preview = ", ".join(table_names)
                    if table_count > 5:
                        preview += f" ì™¸ {table_count - 5}ê°œ"
                    
                    yield emit_message(f"   âœ… íŒŒì‹± ì™„ë£Œ: {table_count}ê°œ í…Œì´ë¸” ({preview})")
                    yield emit_phase_event(
                        phase_num=0,
                        phase_name="DDL ì²˜ë¦¬",
                        status="in_progress",
                        progress=parsing_end,
                        details={"tables_parsed": table_count, "mode": "regex"}
                    )
                    
            except Exception as e:
                if emit_progress:
                    yield emit_message(f"   âŒ ì •ê·œì‹ íŒŒì‹± ì‹¤íŒ¨: {str(e)[:80]}")
                raise AnalysisError(f"DDL ì •ê·œì‹ íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        # ========================================
        # LLM íŒŒì„œ ì‚¬ìš© (use_llm=Trueì¸ ê²½ìš°)
        # ========================================
        else:
            # ëŒ€ìš©ëŸ‰ DDL ì²­í¬ ë¶„í• 
            ddl_chunks = split_ddl_into_chunks(ddl_content)
            chunk_count = len(ddl_chunks)
            
            if chunk_count > 1 and emit_progress:
                yield emit_message(f"   ğŸ“¦ ëŒ€ìš©ëŸ‰ DDL ë¶„í• : {total_tokens:,} í† í° â†’ {chunk_count}ê°œ ì²­í¬")
            
            loader = RuleLoader(target_lang="dbms")
            
            # CREATE TABLE íŒ¨í„´ (ì²­í¬ì—ì„œ í…Œì´ë¸”ëª… ì¶”ì¶œìš©)
            table_pattern = re.compile(
                r'CREATE\s+(?:TABLE|VIEW)\s+(?:IF\s+NOT\s+EXISTS\s+)?([\w\."]+)',
                re.IGNORECASE
            )
            
            # ì²­í¬ë³„ LLM í˜¸ì¶œ ë° ê²°ê³¼ ë³‘í•©
            all_parsed_results: List[Dict] = []
            total_tables_parsed = 0
            
            for chunk_idx, chunk in enumerate(ddl_chunks, 1):
                chunk_tokens = calculate_code_token(chunk)
                
                # ì²­í¬ì— í¬í•¨ëœ í…Œì´ë¸”ëª… ì¶”ì¶œ (ë¯¸ë¦¬ë³´ê¸°ìš©)
                tables_in_chunk_raw = table_pattern.findall(chunk)
                tables_preview = [t.replace('"', '').split('.')[-1] for t in tables_in_chunk_raw[:3]]
                preview_str = ", ".join(tables_preview)
                if len(tables_in_chunk_raw) > 3:
                    preview_str += f" ì™¸ {len(tables_in_chunk_raw) - 3}ê°œ"
                
                # ì§„í–‰ë¥  ê³„ì‚° (ì²­í¬ ê¸°ì¤€)
                progress_percent = int((chunk_idx - 1) / chunk_count * 100)
                
                if emit_progress:
                    yield emit_message(f"   ğŸ”„ [{chunk_idx}/{chunk_count}] íŒŒì‹± ì¤‘: {preview_str}")
                    yield emit_phase_event(
                        phase_num=0,
                        phase_name="DDL ì²˜ë¦¬",
                        status="in_progress",
                        progress=progress_percent,
                        details={"chunk": chunk_idx, "total_chunks": chunk_count, "current_tables": preview_str}
                    )
                
                try:
                    # LLM í˜¸ì¶œ (DDL íŒŒì‹±ìš© ë¹ ë¥¸ ëª¨ë¸)
                    chunk_parsed = await asyncio.to_thread(
                        loader.execute,
                        "ddl",
                        {"ddl_content": chunk, "locale": orchestrator.locale},
                        orchestrator.api_key,
                        model="gpt-4.1-mini",
                    )
                    tables_in_chunk = len(chunk_parsed.get("analysis", []))
                    all_parsed_results.extend(chunk_parsed.get("analysis", []))
                    total_tables_parsed += tables_in_chunk
                    
                    # íŒŒì‹±ëœ í…Œì´ë¸”ëª… í‘œì‹œ
                    parsed_table_names = [
                        t.get("table", {}).get("name", "?") 
                        for t in chunk_parsed.get("analysis", [])[:5]
                    ]
                    parsed_preview = ", ".join(parsed_table_names)
                    if tables_in_chunk > 5:
                        parsed_preview += f" ì™¸ {tables_in_chunk - 5}ê°œ"
                    
                    progress_percent = int(chunk_idx / chunk_count * 100)
                    
                    if emit_progress:
                        yield emit_message(f"   âœ… [{chunk_idx}/{chunk_count}] ì™„ë£Œ: {tables_in_chunk}ê°œ í…Œì´ë¸” ({parsed_preview})")
                        yield emit_phase_event(
                            phase_num=0,
                            phase_name="DDL ì²˜ë¦¬",
                            status="in_progress",
                            progress=progress_percent,
                            details={"chunk": chunk_idx, "total_chunks": chunk_count, "tables_parsed": total_tables_parsed}
                        )
                    
                except Exception as e:
                    if emit_progress:
                        yield emit_message(f"   âŒ [{chunk_idx}/{chunk_count}] ì‹¤íŒ¨: {str(e)[:80]}")
                    raise AnalysisError(f"DDL ì²­í¬ {chunk_idx} íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        # ë³‘í•©ëœ ê²°ê³¼ë¥¼ parsedë¡œ ì‚¬ìš©
        parsed = {"analysis": all_parsed_results}
        
        # db ì†ì„±ì€ DML ì²˜ë¦¬(ast_processor)ì™€ ì¼ê´€ì„±ì„ ìœ„í•´ ì†Œë¬¸ìë¡œ ë³€í™˜
        db_name = (orchestrator.target or 'postgres').lower()
        
        # ëŒ€ì†Œë¬¸ì ë³€í™˜ ì˜µì…˜
        name_case = getattr(orchestrator, 'name_case', 'original')

        # ===========================================
        # UNWIND ë°°ì¹˜ìš© ë°ì´í„° ìˆ˜ì§‘ (ê°œë³„ ì¿¼ë¦¬ ëŒ€ì‹ )
        # ===========================================
        schemas_data = []  # ìŠ¤í‚¤ë§ˆ ë°ì´í„°
        tables_data = []   # í…Œì´ë¸” ë°ì´í„°
        columns_data = []  # ì»¬ëŸ¼ ë°ì´í„°
        fks_data = []      # FK ê´€ê³„ ë°ì´í„°
        
        # ì¤‘ë³µ ë°©ì§€ìš© ì„¸íŠ¸
        seen_schemas = set()
        seen_tables = set()

        for table_info in parsed.get("analysis", []):
            table = table_info.get("table", {})
            columns = table_info.get("columns", [])
            foreign_keys = table_info.get("foreignKeys", [])
            primary_keys = [
                str(pk).strip().upper()
                for pk in (table_info.get("primaryKeys") or [])
                if pk
            ]

            # ì›ë³¸ ê°’ì—ì„œ ë”°ì˜´í‘œ ì œê±° í›„ ëŒ€ì†Œë¬¸ì ë³€í™˜ ì ìš©
            schema_raw = (table.get("schema") or "").strip()
            table_name_raw = (table.get("name") or "").strip()
            comment = (table.get("comment") or "").strip()
            table_type = (table.get("table_type") or "BASE TABLE").strip().upper()
            
            # parse_table_identifierë¡œ ë”°ì˜´í‘œ ì œê±° ë° ìŠ¤í‚¤ë§ˆ/í…Œì´ë¸” ë¶„ë¦¬
            qualified = f"{schema_raw}.{table_name_raw}" if schema_raw else table_name_raw
            parsed_schema, parsed_name, _ = parse_table_identifier(qualified)
            
            # name_case ì˜µì…˜ì— ë”°ë¼ ëŒ€ì†Œë¬¸ì ë³€í™˜ ì ìš©
            schema = self._apply_name_case(parsed_schema if parsed_schema else "public", name_case)
            parsed_name = self._apply_name_case(parsed_name, name_case)
            
            # DDLì—ì„œ ë°œê²¬ëœ ìŠ¤í‚¤ë§ˆ ìˆ˜ì§‘ (name_case ì ìš©ëœ ê°’ìœ¼ë¡œ ì €ì¥)
            if schema and schema.lower() != 'public':
                self._ddl_schemas.add(schema)
            
            # ìŠ¤í‚¤ë§ˆ ë°ì´í„° ìˆ˜ì§‘ (ì¤‘ë³µ ë°©ì§€)
            schema_key = (db_name, schema)
            if schema_key not in seen_schemas:
                seen_schemas.add(schema_key)
                schemas_data.append({
                    "db": db_name,
                    "name": schema
                })
            
            # í…Œì´ë¸” ë°ì´í„° ìˆ˜ì§‘ (ì¤‘ë³µ ë°©ì§€)
            table_key = (db_name, schema, parsed_name)
            if table_key not in seen_tables:
                seen_tables.add(table_key)
                tables_data.append({
                    "db": db_name,
                    "schema": schema,
                    "name": parsed_name,
                    "description": escape_for_cypher(comment),
                    "description_source": "ddl" if comment else "",
                    "table_type": table_type
                })
                ddl_stats["tables"] += 1
            
            # DDL ë©”íƒ€ë°ì´í„° ìºì‹œ ì €ì¥ (ë©”ëª¨ë¦¬)
            column_metadata = {}
            for col in columns:
                col_name_raw = (col.get("name") or "").strip()
                if not col_name_raw:
                    continue
                col_name = self._apply_name_case(col_name_raw, name_case)
                col_comment = (col.get("comment") or "").strip()
                column_metadata[col_name] = {
                    "description": col_comment,
                    "dtype": (col.get("dtype") or col.get("type") or "").strip(),
                    "nullable": col.get("nullable", True),
                }
            
            cache_key = (schema.lower(), parsed_name.lower())
            self._ddl_table_metadata[cache_key] = {
                "description": comment,
                "columns": column_metadata,
                "original_schema": schema,
                "original_name": parsed_name,
            }

            # ì»¬ëŸ¼ ë°ì´í„° ìˆ˜ì§‘
            for col in columns:
                col_name_raw = (col.get("name") or "").strip()
                if not col_name_raw:
                    continue
                
                col_name = self._apply_name_case(col_name_raw, name_case)
                col_type = (col.get("dtype") or col.get("type") or "").strip()
                col_nullable = col.get("nullable", True)
                col_comment = (col.get("comment") or "").strip()
                fqn = ".".join(filter(None, [schema, parsed_name, col_name])).lower()
                
                col_data = {
                    "fqn": escape_for_cypher(fqn),
                    "name": escape_for_cypher(col_name),
                    "dtype": escape_for_cypher(col_type),
                    "description": escape_for_cypher(col_comment),
                    "description_source": "ddl" if col_comment else "",
                    "nullable": col_nullable,
                    "table_db": db_name,
                    "table_schema": schema,
                    "table_name": parsed_name
                }
                if col_name_raw.upper() in primary_keys:
                    col_data["pk_constraint"] = f"{parsed_name}_pkey"
                
                columns_data.append(col_data)
                ddl_stats["columns"] += 1

            # FK ê´€ê³„ ë°ì´í„° ìˆ˜ì§‘
            for fk in foreign_keys:
                src_col_raw = (fk.get("column") or "").strip()
                ref = (fk.get("ref") or "").strip()
                if not src_col_raw or not ref or "." not in ref:
                    continue

                ref_table_part, ref_col_raw = ref.rsplit(".", 1)
                ref_schema_parsed, ref_table_raw, _ = parse_table_identifier(ref_table_part)
                ref_schema_final = self._apply_name_case(ref_schema_parsed or schema, name_case)
                ref_table = self._apply_name_case(ref_table_raw, name_case)
                src_col = self._apply_name_case(src_col_raw, name_case)
                ref_col = self._apply_name_case(ref_col_raw, name_case)

                fks_data.append({
                    "from_db": db_name,
                    "from_schema": schema,
                    "from_table": parsed_name,
                    "from_column": escape_for_cypher(src_col),
                    "to_db": db_name,
                    "to_schema": ref_schema_final or "",
                    "to_table": ref_table or "",
                    "to_column": escape_for_cypher(ref_col)
                })
                ddl_stats["fks"] += 1

        # ===========================================
        # UNWIND ë°°ì¹˜ ì‹¤í–‰ (7~8ë²ˆì˜ Neo4j í˜¸ì¶œë¡œ ì™„ë£Œ!)
        # ===========================================
        if emit_progress:
            yield emit_message(f"   ğŸ’¾ UNWIND ë°°ì¹˜ ì €ì¥ ì‹œì‘: {ddl_stats['tables']}ê°œ í…Œì´ë¸”, {ddl_stats['columns']}ê°œ ì»¬ëŸ¼, {ddl_stats['fks']}ê°œ FK")
            yield emit_phase_event(
                phase_num=0,
                phase_name="DDL ì²˜ë¦¬",
                status="in_progress",
                progress=saving_start,
                details={
                    "step": "unwind_batch",
                    "tables": ddl_stats['tables'],
                    "columns": ddl_stats['columns'],
                    "fks": ddl_stats['fks']
                }
            )
        
        all_nodes: dict = {}
        all_relationships: dict = {}
        
        # 1. ìŠ¤í‚¤ë§ˆ ë…¸ë“œ ìƒì„±
        if schemas_data:
            if emit_progress:
                yield emit_message(f"      ğŸ“¦ [1/6] ìŠ¤í‚¤ë§ˆ {len(schemas_data)}ê°œ ìƒì„± ì¤‘...")
            schema_query = """
            UNWIND $items AS item
            MERGE (__cy_s__:Schema {db: item.db, name: item.name})
            RETURN __cy_s__
            """
            async with self._cypher_lock:
                result = await client.run_batch_unwind(schema_query, schemas_data)
            for node in result.get("Nodes", []):
                all_nodes[node.get("Node ID")] = node
        
        # 2. í…Œì´ë¸” ë…¸ë“œ ìƒì„±
        if tables_data:
            if emit_progress:
                yield emit_message(f"      ğŸ“¦ [2/6] í…Œì´ë¸” {len(tables_data)}ê°œ ìƒì„± ì¤‘...")
            table_query = """
            UNWIND $items AS item
            MERGE (__cy_t__:Table {db: item.db, schema: item.schema, name: item.name})
            SET __cy_t__.description = item.description,
                __cy_t__.description_source = item.description_source,
                __cy_t__.table_type = item.table_type
            RETURN __cy_t__
            """
            async with self._cypher_lock:
                result = await client.run_batch_unwind(table_query, tables_data)
            for node in result.get("Nodes", []):
                all_nodes[node.get("Node ID")] = node
        
        # 3. í…Œì´ë¸”-ìŠ¤í‚¤ë§ˆ ê´€ê³„ ìƒì„±
        if tables_data:
            if emit_progress:
                yield emit_message(f"      ğŸ“¦ [3/6] í…Œì´ë¸”-ìŠ¤í‚¤ë§ˆ ê´€ê³„ {len(tables_data)}ê°œ ìƒì„± ì¤‘...")
            belongs_query = """
            UNWIND $items AS item
            MATCH (__cy_t__:Table {db: item.db, schema: item.schema, name: item.name})
            MATCH (__cy_s__:Schema {db: item.db, name: item.schema})
            MERGE (__cy_t__)-[__cy_r__:BELONGS_TO]->(__cy_s__)
            RETURN __cy_t__, __cy_r__, __cy_s__
            """
            async with self._cypher_lock:
                result = await client.run_batch_unwind(belongs_query, tables_data)
            for node in result.get("Nodes", []):
                all_nodes[node.get("Node ID")] = node
            for rel in result.get("Relationships", []):
                all_relationships[rel.get("Relationship ID")] = rel
        
        # 4. ì»¬ëŸ¼ ë…¸ë“œ ìƒì„±
        if columns_data:
            if emit_progress:
                yield emit_message(f"      ğŸ“¦ [4/6] ì»¬ëŸ¼ {len(columns_data)}ê°œ ìƒì„± ì¤‘...")
            column_query = """
            UNWIND $items AS item
            MERGE (__cy_c__:Column {fqn: item.fqn})
            SET __cy_c__.name = item.name,
                __cy_c__.dtype = item.dtype,
                __cy_c__.description = item.description,
                __cy_c__.description_source = item.description_source,
                __cy_c__.nullable = item.nullable,
                __cy_c__.pk_constraint = CASE WHEN item.pk_constraint IS NOT NULL THEN item.pk_constraint ELSE __cy_c__.pk_constraint END
            RETURN __cy_c__
            """
            async with self._cypher_lock:
                result = await client.run_batch_unwind(column_query, columns_data)
            for node in result.get("Nodes", []):
                all_nodes[node.get("Node ID")] = node
        
        # 5. í…Œì´ë¸”-ì»¬ëŸ¼ ê´€ê³„ ìƒì„±
        if columns_data:
            if emit_progress:
                yield emit_message(f"      ğŸ“¦ [5/6] í…Œì´ë¸”-ì»¬ëŸ¼ ê´€ê³„ {len(columns_data)}ê°œ ìƒì„± ì¤‘...")
            has_column_query = """
            UNWIND $items AS item
            MATCH (__cy_t__:Table {db: item.table_db, schema: item.table_schema, name: item.table_name})
            MATCH (__cy_c__:Column {fqn: item.fqn})
            MERGE (__cy_t__)-[__cy_r__:HAS_COLUMN]->(__cy_c__)
            RETURN __cy_t__, __cy_r__, __cy_c__
            """
            async with self._cypher_lock:
                result = await client.run_batch_unwind(has_column_query, columns_data)
            for node in result.get("Nodes", []):
                all_nodes[node.get("Node ID")] = node
            for rel in result.get("Relationships", []):
                all_relationships[rel.get("Relationship ID")] = rel
        
        # 6. FK ê´€ê³„ ìƒì„± (ì°¸ì¡° í…Œì´ë¸” MERGE + FK ê´€ê³„)
        if fks_data:
            if emit_progress:
                yield emit_message(f"      ğŸ“¦ [6/6] FK ê´€ê³„ {len(fks_data)}ê°œ ìƒì„± ì¤‘...")
            # ë¨¼ì € ì°¸ì¡° í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±
            ref_tables_query = """
            UNWIND $items AS item
            MERGE (__cy_rt__:Table {db: item.to_db, schema: item.to_schema, name: item.to_table})
            RETURN __cy_rt__
            """
            async with self._cypher_lock:
                result = await client.run_batch_unwind(ref_tables_query, fks_data)
            for node in result.get("Nodes", []):
                all_nodes[node.get("Node ID")] = node
            
            # FK ê´€ê³„ ìƒì„±
            fk_query = """
            UNWIND $items AS item
            MATCH (__cy_t__:Table {db: item.from_db, schema: item.from_schema, name: item.from_table})
            MATCH (__cy_rt__:Table {db: item.to_db, schema: item.to_schema, name: item.to_table})
            MERGE (__cy_t__)-[__cy_r__:FK_TO_TABLE {sourceColumn: item.from_column, targetColumn: item.to_column}]->(__cy_rt__)
            ON CREATE SET __cy_r__.type = 'many_to_one', __cy_r__.source = 'ddl'
            RETURN __cy_t__, __cy_r__, __cy_rt__
            """
            async with self._cypher_lock:
                result = await client.run_batch_unwind(fk_query, fks_data)
            for node in result.get("Nodes", []):
                all_nodes[node.get("Node ID")] = node
            for rel in result.get("Relationships", []):
                all_relationships[rel.get("Relationship ID")] = rel
        
        if emit_progress:
            yield emit_message(f"   âœ… UNWIND ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: {len(all_nodes)}ê°œ ë…¸ë“œ, {len(all_relationships)}ê°œ ê´€ê³„")
            yield emit_phase_event(
                phase_num=0,
                phase_name="DDL ì²˜ë¦¬",
                status="in_progress",
                progress=saving_end,
                details={
                    "step": "unwind_completed",
                    "nodes_created": len(all_nodes),
                    "relationships_created": len(all_relationships)
                }
            )
        
        result = {
            "Nodes": list(all_nodes.values()),
            "Relationships": list(all_relationships.values())
        }
        
        if emit_progress:
            yield emit_message(f"   âœ… Neo4j ì €ì¥ ì™„ë£Œ: {len(result['Nodes'])}ê°œ ë…¸ë“œ, {len(result['Relationships'])}ê°œ ê´€ê³„ ìƒì„±")
            yield emit_phase_event(
                phase_num=0,
                phase_name="DDL ì²˜ë¦¬",
                status="in_progress",
                progress=saving_end,
                details={
                    "step": "neo4j_saved",
                    "tables": ddl_stats['tables'],
                    "columns": ddl_stats['columns'],
                    "fks": ddl_stats['fks'],
                    "nodes_created": len(result['Nodes']),
                    "relationships_created": len(result['Relationships'])
                }
            )
        
        log_process("ANALYZE", "DDL", f"DDL ì²˜ë¦¬ ì™„ë£Œ: {file_name} (T:{ddl_stats['tables']}, C:{ddl_stats['columns']}, FK:{ddl_stats['fks']})")
        
        # ìµœì¢… ê²°ê³¼ë¥¼ íŠ¹ë³„í•œ í˜•íƒœë¡œ yield (tuple)
        yield (result, ddl_stats)

    # =========================================================================
    # ìŠ¤í‚¤ë§ˆ ê²°ì •
    # =========================================================================

    def _resolve_default_schema(self, directory: str, name_case: str = 'original') -> str:
        """íŒŒì¼ ê²½ë¡œì—ì„œ ê¸°ë³¸ ìŠ¤í‚¤ë§ˆë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
        
        ìš°ì„ ìˆœìœ„:
        1. ê²½ë¡œì˜ í´ë”ëª… ì¤‘ DDL ìŠ¤í‚¤ë§ˆì™€ ì¼ì¹˜í•˜ëŠ” ê²ƒ (ê¹Šì€ í´ë” ìš°ì„ )
        2. ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ë””ë ‰í† ë¦¬ëª… ì‚¬ìš©
        
        Args:
            directory: íŒŒì¼ì´ ìœ„ì¹˜í•œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            name_case: ëŒ€ì†Œë¬¸ì ë³€í™˜ ì˜µì…˜ (original, uppercase, lowercase)
        """
        if not directory:
            return self._apply_name_case("public", name_case)
        
        # ê²½ë¡œë¥¼ í´ë” ëª©ë¡ìœ¼ë¡œ ë¶„ë¦¬ (ê¹Šì€ ìˆœì„œëŒ€ë¡œ)
        parts = directory.replace("\\", "/").split("/")
        parts = [p for p in parts if p]  # ë¹ˆ ë¬¸ìì—´ ì œê±°
        
        if not parts:
            return self._apply_name_case("public", name_case)
        
        # DDL ìŠ¤í‚¤ë§ˆê°€ ìˆìœ¼ë©´ ë§¤ì¹­ ì‹œë„ (ê¹Šì€ í´ë”ë¶€í„°)
        # ëŒ€ì†Œë¬¸ì ë¬´ê´€ ë¹„êµ í›„, DDLì— ì €ì¥ëœ ì›ë³¸ ëŒ€ì†Œë¬¸ì ë°˜í™˜
        if self._ddl_schemas:
            ddl_schemas_lower_map = {s.lower(): s for s in self._ddl_schemas}
            for folder in reversed(parts):
                matched = ddl_schemas_lower_map.get(folder.lower())
                if matched:
                    return matched  # DDLì—ì„œ name_case ì ìš©ëœ ê°’ ê·¸ëŒ€ë¡œ ë°˜í™˜
        
        # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ë””ë ‰í† ë¦¬ëª…(ê°€ì¥ ê¹Šì€ í´ë”)ì— name_case ì ìš©
        return self._apply_name_case(parts[-1], name_case)

    # =========================================================================
    # íŒŒì¼ ë¡œë“œ
    # =========================================================================

    async def _load_all_files(
        self,
        file_names: list[tuple[str, str]],
        orchestrator: Any,
    ) -> List[FileAnalysisContext]:
        """ëª¨ë“  íŒŒì¼ì˜ ASTì™€ ì†ŒìŠ¤ì½”ë“œë¥¼ ë³‘ë ¬ë¡œ ë¡œë“œí•©ë‹ˆë‹¤."""
        
        async def load_single(directory: str, file_name: str) -> FileAnalysisContext:
            src_path = os.path.join(orchestrator.dirs["src"], directory, file_name)
            base_name = os.path.splitext(file_name)[0]
            ast_path = os.path.join(orchestrator.dirs["analysis"], directory, f"{base_name}.json")

            async with aiofiles.open(ast_path, "r", encoding="utf-8") as ast_file, \
                       aiofiles.open(src_path, "r", encoding="utf-8") as src_file:
                ast_content, source_lines = await asyncio.gather(
                    ast_file.read(),
                    src_file.readlines(),
                )
                return FileAnalysisContext(
                    directory=directory,
                    file_name=file_name,
                    ast_data=json.loads(ast_content),
                    source_lines=source_lines,
                )

        tasks = [load_single(d, f) for d, f in file_names]
        return await asyncio.gather(*tasks)

    # =========================================================================
    # Phase 1: AST ê·¸ë˜í”„ ìƒì„±
    # =========================================================================

    async def _run_phase1(
        self,
        contexts: List[FileAnalysisContext],
        client: Neo4jClient,
        orchestrator: Any,
        stats: AnalysisStats,
    ) -> AsyncGenerator[bytes, None]:
        """Phase 1: ëª¨ë“  íŒŒì¼ì˜ AST ê·¸ë˜í”„ë¥¼ ë³‘ë ¬ë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
        
        completed = 0
        total = len(contexts)
        results_queue: asyncio.Queue = asyncio.Queue()

        async def process_file(ctx: FileAnalysisContext):
            async with self._file_semaphore:
                try:
                    # name_case ì˜µì…˜ ê°€ì ¸ì˜¤ê¸°
                    name_case = getattr(orchestrator, 'name_case', 'original')
                    
                    # íŒŒì¼ ê²½ë¡œ ê¸°ë°˜ ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ ê²°ì • (name_case ì ìš©)
                    default_schema = self._resolve_default_schema(ctx.directory, name_case)
                    
                    processor = DbmsAstProcessor(
                        antlr_data=ctx.ast_data,
                        file_content="".join(ctx.source_lines),
                        directory=ctx.directory,
                        file_name=ctx.file_name,
                        api_key=orchestrator.api_key,
                        locale=orchestrator.locale,
                        dbms=orchestrator.target,
                        last_line=len(ctx.source_lines),
                        default_schema=default_schema,
                        ddl_table_metadata=self._ddl_table_metadata,
                        name_case=name_case,
                    )
                    ctx.processor = processor
                    
                    # ì •ì  ê·¸ë˜í”„ ìƒì„±
                    queries = processor.build_static_graph_queries()
                    
                    if queries:
                        all_nodes = {}
                        all_relationships = {}
                        async with self._cypher_lock:
                            async for batch_result in client.run_graph_query(queries):
                                for node in batch_result.get("Nodes", []):
                                    all_nodes[node["Node ID"]] = node
                                for rel in batch_result.get("Relationships", []):
                                    all_relationships[rel["Relationship ID"]] = rel
                        
                        graph = {"Nodes": list(all_nodes.values()), "Relationships": list(all_relationships.values())}
                        node_count = len(graph.get("Nodes", []))
                        rel_count = len(graph.get("Relationships", []))
                        
                        ctx.status = FileStatus.PH1_OK
                        await results_queue.put({
                            "type": "success",
                            "file": ctx.file_name,
                            "graph": graph,
                            "node_count": node_count,
                            "rel_count": rel_count,
                        })
                    else:
                        ctx.status = FileStatus.PH1_OK
                        await results_queue.put({
                            "type": "success",
                            "file": ctx.file_name,
                            "graph": {"Nodes": [], "Relationships": []},
                            "node_count": 0,
                            "rel_count": 0,
                        })
                        
                except Exception as e:
                    log_process("ANALYZE", "ERROR", f"Phase 1 ì˜¤ë¥˜ ({ctx.file_name}): {e}", logging.ERROR, e)
                    ctx.status = FileStatus.PH1_FAIL
                    ctx.error_message = str(e)[:100]
                    await results_queue.put({
                        "type": "error",
                        "file": ctx.file_name,
                        "message": str(e),
                    })
                    raise  # ì¦‰ì‹œ ì¤‘ë‹¨ - ë¶€ë¶„ ì‹¤íŒ¨ í—ˆìš© ì•ˆí•¨

        # ëª¨ë“  íŒŒì¼ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘
        tasks = [asyncio.create_task(process_file(ctx)) for ctx in contexts]

        # ê²°ê³¼ ìˆ˜ì‹  ë° ìŠ¤íŠ¸ë¦¬ë°
        while completed < total:
            result = await asyncio.wait_for(results_queue.get(), timeout=300.0)
            result_type = result.get("type", "")
            
            completed += 1
            stats.files_completed = completed
            
            # Phase 1 ì§„í–‰ë¥  ê³„ì‚° (0-50% ë²”ìœ„ ì‚¬ìš©)
            phase1_progress = int(completed / total * 50)
            
            if result_type == "error":
                yield emit_message(f"   âŒ [{completed}/{total}] {result['file']}: {result['message'][:50]}")
                stats.mark_file_failed(result['file'], "Phase1 ì‹¤íŒ¨")
                yield emit_phase_event(
                    phase_num=1,
                    phase_name="AST êµ¬ì¡° ë¶„ì„",
                    status="in_progress",
                    progress=phase1_progress,
                    details={"file": result['file'], "status": "failed", "completed": completed, "total": total}
                )
            else:
                stats.add_graph_result(result["graph"], is_static=True)
                
                graph = result["graph"]
                node_count = result.get("node_count", 0)
                rel_count = result.get("rel_count", 0)
                
                # ë…¸ë“œ íƒ€ì…ë³„ ìƒì„¸ ì§‘ê³„
                node_types = {}
                for node in graph.get("Nodes", []):
                    labels = node.get("Labels", [])
                    for label in labels:
                        node_types[label] = node_types.get(label, 0) + 1
                
                # ìƒì„¸ ë©”ì‹œì§€ ìƒì„±
                yield emit_message(f"   âœ“ [{completed}/{total}] {result['file']}")
                
                if node_types:
                    # ì£¼ìš” ë…¸ë“œ íƒ€ì… í‘œì‹œ
                    proc_count = node_types.get("PROCEDURE", 0) + node_types.get("FUNCTION", 0)
                    stmt_count = sum(v for k, v in node_types.items() if k in ["SELECT", "INSERT", "UPDATE", "DELETE", "MERGE"])
                    table_refs = node_types.get("Table", 0)
                    
                    detail_parts = []
                    if proc_count:
                        detail_parts.append(f"í”„ë¡œì‹œì €/í•¨ìˆ˜ {proc_count}ê°œ")
                    if stmt_count:
                        detail_parts.append(f"SQLë¬¸ {stmt_count}ê°œ")
                    if table_refs:
                        detail_parts.append(f"í…Œì´ë¸” ì°¸ì¡° {table_refs}ê°œ")
                    
                    if detail_parts:
                        yield emit_message(f"      â†’ {', '.join(detail_parts)}")
                    
                    # ê´€ê³„ ì •ë³´
                    if rel_count > 0:
                        yield emit_message(f"      â†’ ê´€ê³„ {rel_count}ê°œ ìƒì„± (FROM, WRITES, CALLS ë“±)")
                
                yield emit_phase_event(
                    phase_num=1,
                    phase_name="AST êµ¬ì¡° ë¶„ì„",
                    status="in_progress",
                    progress=phase1_progress,
                    details={
                        "file": result['file'],
                        "nodes": node_count,
                        "relationships": rel_count,
                        "completed": completed,
                        "total": total,
                        "node_types": node_types
                    }
                )
                
                yield emit_data(
                    graph=graph,
                    line_number=0,
                    analysis_progress=int(completed / total * 50),
                    current_file=result["file"],
                )

        # ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
        await asyncio.gather(*tasks, return_exceptions=True)

    # =========================================================================
    # Phase 2: LLM ë¶„ì„
    # =========================================================================

    async def _run_phase2(
        self,
        contexts: List[FileAnalysisContext],
        client: Neo4jClient,
        orchestrator: Any,
        stats: AnalysisStats,
    ) -> AsyncGenerator[bytes, None]:
        """Phase 2: Phase1 ì„±ê³µ íŒŒì¼ì˜ LLM ë¶„ì„ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        
        if not contexts:
            yield emit_message("   â„¹ï¸ ë¶„ì„ ëŒ€ìƒ íŒŒì¼ ì—†ìŒ")
            return
        
        completed = 0
        total = len(contexts)
        results_queue: asyncio.Queue = asyncio.Queue()

        async def analyze_file(ctx: FileAnalysisContext):
            async with self._file_semaphore:
                try:
                    if not ctx.processor:
                        raise AnalysisError(f"Phase 1ì—ì„œ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì‹¤íŒ¨: {ctx.file_name}")
                    
                    # LLM ë¶„ì„ ì‹¤í–‰ (íŠœí”Œ ë°˜í™˜: queries, failed_batch_count, failed_details)
                    analysis_queries, failed_batch_count, failed_details = await ctx.processor.run_llm_analysis()
                    
                    if analysis_queries:
                        all_nodes = {}
                        all_relationships = {}
                        async with self._cypher_lock:
                            async for batch_result in client.run_graph_query(analysis_queries):
                                for node in batch_result.get("Nodes", []):
                                    all_nodes[node["Node ID"]] = node
                                for rel in batch_result.get("Relationships", []):
                                    all_relationships[rel["Relationship ID"]] = rel
                                # ë°°ì¹˜ ì§„í–‰ë¥  ìŠ¤íŠ¸ë¦¬ë° (ê·¸ë˜í”„ ë°ì´í„° í¬í•¨)
                                await results_queue.put({
                                    "type": "batch_progress",
                                    "file": ctx.file_name,
                                    "batch": batch_result.get("batch", 0),
                                    "total_batches": batch_result.get("total_batches", 0),
                                    "graph": {
                                        "Nodes": batch_result.get("Nodes", []),
                                        "Relationships": batch_result.get("Relationships", []),
                                    },
                                })
                        
                        graph = {"Nodes": list(all_nodes.values()), "Relationships": list(all_relationships.values())}
                        ctx.status = FileStatus.PH2_OK
                        await results_queue.put({
                            "type": "success",
                            "file": ctx.file_name,
                            "graph": graph,
                            "query_count": len(analysis_queries),
                            "failed_batches": failed_batch_count,
                            "failed_details": failed_details,
                        })
                    else:
                        ctx.status = FileStatus.PH2_OK
                        await results_queue.put({
                            "type": "success",
                            "file": ctx.file_name,
                            "graph": {"Nodes": [], "Relationships": []},
                            "query_count": 0,
                            "failed_batches": failed_batch_count,
                        })
                    
                    # ë°°ì¹˜ ì‹¤íŒ¨ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ì¤‘ë‹¨ - ë¶€ë¶„ ì‹¤íŒ¨ í—ˆìš© ì•ˆí•¨
                    if failed_batch_count > 0:
                        raise AnalysisError(f"{ctx.file_name}: {failed_batch_count}ê°œ ë°°ì¹˜ ì‹¤íŒ¨")
                        
                except Exception as e:
                    log_process("ANALYZE", "ERROR", f"Phase 2 ì˜¤ë¥˜ ({ctx.file_name}): {e}", logging.ERROR, e)
                    ctx.status = FileStatus.PH2_FAIL
                    ctx.error_message = str(e)[:100]
                    await results_queue.put({
                        "type": "error",
                        "file": ctx.file_name,
                        "message": str(e),
                    })
                    raise  # ì¦‰ì‹œ ì¤‘ë‹¨ - ë¶€ë¶„ ì‹¤íŒ¨ í—ˆìš© ì•ˆí•¨

        # ëª¨ë“  íŒŒì¼ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘
        tasks = [asyncio.create_task(analyze_file(ctx)) for ctx in contexts]

        # ê²°ê³¼ ìˆ˜ì‹  ë° ìŠ¤íŠ¸ë¦¬ë°
        while completed < total:
            result = await asyncio.wait_for(results_queue.get(), timeout=600.0)
            result_type = result.get("type", "")
            
            # warningì€ ì¹´ìš´íŠ¸í•˜ì§€ ì•ŠìŒ (ì¶”ê°€ ì •ë³´ì¼ ë¿)
            if result_type == "warning":
                yield emit_message(f"   âš ï¸ {result['file']}: {result['message']}")
                continue
            
            # ë°°ì¹˜ ì§„í–‰ë¥ ì€ ì¹´ìš´íŠ¸í•˜ì§€ ì•ŠìŒ (ì¤‘ê°„ ì§„í–‰ ìƒíƒœ)
            if result_type == "batch_progress":
                batch = result.get("batch", 0)
                total_batches = result.get("total_batches", 0)
                graph = result.get("graph")
                yield emit_message(f"      ğŸ“¦ {result['file']}: ë°°ì¹˜ {batch}/{total_batches} ì €ì¥ ì™„ë£Œ")
                # ë°°ì¹˜ë³„ ê·¸ë˜í”„ ë°ì´í„° ì¦‰ì‹œ ì „ì†¡
                if graph:
                    yield emit_data(graph=graph)
                continue
            
            completed += 1
            
            # Phase 2 ì§„í–‰ë¥  ê³„ì‚° (50-100% ë²”ìœ„ ì‚¬ìš©)
            phase2_progress = 50 + int(completed / total * 50)
            
            if result_type == "error":
                yield emit_message(f"   âŒ [{completed}/{total}] {result['file']}: {result['message'][:50]}")
                stats.mark_file_failed(result['file'], "Phase2 ì‹¤íŒ¨")
                yield emit_phase_event(
                    phase_num=2,
                    phase_name="AI ë¶„ì„",
                    status="in_progress",
                    progress=phase2_progress,
                    details={"file": result['file'], "status": "failed", "completed": completed, "total": total}
                )
            else:
                stats.llm_batches_executed += 1
                graph = result["graph"]
                stats.add_graph_result(graph, is_static=False)
                
                # ë°°ì¹˜ ì‹¤íŒ¨ ì •ë³´ í‘œì‹œ
                failed_batches = result.get("failed_batches", 0)
                failed_details = result.get("failed_details", [])
                fail_info = f" (ë°°ì¹˜ {failed_batches}ê°œ ì‹¤íŒ¨)" if failed_batches > 0 else ""
                
                # ë¶„ì„ ê²°ê³¼ ìƒì„¸ ì§‘ê³„
                node_count = len(graph.get("Nodes", []))
                rel_count = len(graph.get("Relationships", []))
                
                # ì—…ë°ì´íŠ¸ëœ ë…¸ë“œ íƒ€ì…ë³„ ì§‘ê³„
                updated_types = {}
                for node in graph.get("Nodes", []):
                    labels = node.get("Labels", [])
                    for label in labels:
                        updated_types[label] = updated_types.get(label, 0) + 1
                
                yield emit_message(f"   âœ“ [{completed}/{total}] {result['file']} (ì¿¼ë¦¬ {result['query_count']}ê°œ){fail_info}")
                
                # LLM ë¶„ì„ ê²°ê³¼ ìƒì„¸ í‘œì‹œ
                if updated_types:
                    # ì£¼ìš” ì—…ë°ì´íŠ¸ í‘œì‹œ
                    summary_added = sum(1 for n in graph.get("Nodes", []) if n.get("Properties", {}).get("summary"))
                    table_desc_added = sum(1 for n in graph.get("Nodes", []) 
                                           if "Table" in (n.get("Labels") or []) 
                                           and n.get("Properties", {}).get("analyzed_description"))
                    
                    detail_parts = []
                    if summary_added:
                        detail_parts.append(f"ìš”ì•½ {summary_added}ê°œ ìƒì„±")
                    if table_desc_added:
                        detail_parts.append(f"í…Œì´ë¸” ì„¤ëª… {table_desc_added}ê°œ ë³´ê°•")
                    if rel_count:
                        detail_parts.append(f"ê´€ê³„ {rel_count}ê°œ ì—…ë°ì´íŠ¸")
                    
                    if detail_parts:
                        yield emit_message(f"      â†’ {', '.join(detail_parts)}")
                
                # ì‹¤íŒ¨ ìƒì„¸ ì •ë³´ ì¶œë ¥ (ìµœëŒ€ 3ê°œ)
                if failed_details:
                    stats.llm_batches_failed += len(failed_details)
                    for detail in failed_details[:3]:
                        yield emit_message(f"      âš ï¸ ë°°ì¹˜ #{detail['batch_id']} ({detail['node_ranges']}): {detail['error'][:50]}")
                
                yield emit_phase_event(
                    phase_num=2,
                    phase_name="AI ë¶„ì„",
                    status="in_progress",
                    progress=phase2_progress,
                    details={
                        "file": result['file'],
                        "queries": result['query_count'],
                        "nodes_updated": node_count,
                        "relationships_updated": rel_count,
                        "completed": completed,
                        "total": total
                    }
                )
                
                yield emit_data(
                    graph=graph,
                    line_number=0,
                    analysis_progress=phase2_progress,
                    current_file=result["file"],
                )

        # ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
        await asyncio.gather(*tasks, return_exceptions=True)

    # =========================================================================
    # Phase 4: ë²¡í„°ë¼ì´ì§• (ì„ë² ë”© ìƒì„±)
    # =========================================================================
    
    async def _run_vectorize_phase(
        self,
        client: Neo4jClient,
        orchestrator: Any,
        stats: AnalysisStats,
    ) -> AsyncGenerator[bytes, None]:
        """Phase 4: í…Œì´ë¸”/ì»¬ëŸ¼ ë²¡í„°ë¼ì´ì§• (ë°°ì¹˜ ìµœì í™”)
        
        Neo4jì— ì €ì¥ëœ í…Œì´ë¸”/ì»¬ëŸ¼ì˜ descriptionì„ ê¸°ë°˜ìœ¼ë¡œ ì„ë² ë”© ìƒì„±
        ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ ìµœì í™”
        """
        from openai import AsyncOpenAI
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        api_key = orchestrator.api_key or settings.openai_api_key
        if not api_key:
            yield emit_message("   âš ï¸ OpenAI API í‚¤ê°€ ì—†ì–´ ë²¡í„°ë¼ì´ì§•ì„ ê±´ë„ˆëœë‹ˆë‹¤")
            return
        
        openai_client = AsyncOpenAI(api_key=api_key)
        embedding_client = EmbeddingClient(openai_client)
        
        # ===========================================
        # í…Œì´ë¸” ë²¡í„°ë¼ì´ì§• (ë°°ì¹˜ ì²˜ë¦¬)
        # ===========================================
        yield emit_message("   ğŸ“Š [Phase 4-1] í…Œì´ë¸” ë²¡í„°ë¼ì´ì§• ì‹œì‘...")
        yield emit_phase_event(
            phase_num=4,
            phase_name="ë²¡í„°ë¼ì´ì§•",
            status="in_progress",
            progress=0,
            details={"step": "table_vectorizing"}
        )
        
        # descriptionê³¼ analyzed_descriptionì„ í•©ì³ì„œ ì„ë² ë”© ìƒì„± (ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ)
        table_query = """
        MATCH (__cy_t__:Table)
        WHERE (__cy_t__.vector IS NULL OR size(__cy_t__.vector) = 0)
          AND (__cy_t__.description IS NOT NULL OR __cy_t__.analyzed_description IS NOT NULL)
        RETURN elementId(__cy_t__) AS tid, 
               __cy_t__.name AS name,
               __cy_t__.schema AS schema,
               trim(
                 coalesce(__cy_t__.description, '') + 
                 CASE WHEN __cy_t__.analyzed_description IS NOT NULL AND __cy_t__.analyzed_description <> '' 
                      THEN ' | AI ë¶„ì„: ' + __cy_t__.analyzed_description 
                      ELSE '' 
                 END
               ) AS description
        ORDER BY __cy_t__.schema, __cy_t__.name
        """
        
        try:
            async with self._cypher_lock:
                result = await client.execute_queries([table_query])
            
            tables = result[0] if result and result[0] else []
            total_tables = len(tables)
            
            if total_tables == 0:
                yield emit_message("      â„¹ï¸ ë²¡í„°í™”í•  í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤")
            else:
                yield emit_message(f"      ğŸ“‹ ë²¡í„°í™” ëŒ€ìƒ: {total_tables}ê°œ í…Œì´ë¸”")
                
                # í…Œì´ë¸”ë„ ë°°ì¹˜ë¡œ ì²˜ë¦¬ (50ê°œì”©)
                batch_size = 50
                for batch_idx in range(0, total_tables, batch_size):
                    batch = tables[batch_idx:batch_idx + batch_size]
                    batch_num = batch_idx // batch_size + 1
                    total_batches = (total_tables + batch_size - 1) // batch_size
                    
                    # ìœ íš¨í•œ í…Œì´ë¸”ë§Œ í•„í„°ë§
                    valid_items = []
                    texts = []
                    for item in batch:
                        description = item.get("description", "") or ""
                        if not description:
                            continue
                        text = embedding_client.format_table_text(
                            table_name=item.get("name", ""),
                            description=description
                        )
                        texts.append(text)
                        valid_items.append(item)
                    
                    if not texts:
                        continue
                    
                    # ë°°ì¹˜ ì§„í–‰ ìƒí™© í‘œì‹œ
                    batch_progress = int(batch_idx / total_tables * 25)  # 0-25% ë²”ìœ„
                    yield emit_message(f"      ğŸ”„ [{batch_num}/{total_batches}] í…Œì´ë¸” {len(valid_items)}ê°œ ì„ë² ë”© ìƒì„± ì¤‘...")
                    yield emit_phase_event(
                        phase_num=4,
                        phase_name="ë²¡í„°ë¼ì´ì§•",
                        status="in_progress",
                        progress=batch_progress,
                        details={"step": "table_embedding", "batch": batch_num, "total_batches": total_batches}
                    )
                    
                    # ë°°ì¹˜ ì„ë² ë”© API í˜¸ì¶œ
                    vectors = await embedding_client.embed_batch(texts)
                    
                    # UNWIND ë°°ì¹˜ ì €ì¥ìš© ë°ì´í„° ìƒì„±
                    vector_updates = []
                    for item, vector in zip(valid_items, vectors):
                        if vector:
                            vector_updates.append({
                                "tid": item['tid'],
                                "vector": vector
                            })
                            stats.tables_vectorized += 1
                    
                    # UNWINDë¡œ í•œë²ˆì— ì €ì¥
                    if vector_updates:
                        update_query = """
                        UNWIND $items AS item
                        MATCH (__cy_t__) WHERE elementId(__cy_t__) = item.tid
                        SET __cy_t__.vector = item.vector
                        RETURN __cy_t__
                        """
                        async with self._cypher_lock:
                            await client.execute_with_params(update_query, {"items": vector_updates})
                        
                        yield emit_message(f"      âœ“ [{batch_num}/{total_batches}] {len(vector_updates)}ê°œ í…Œì´ë¸” ë²¡í„° ì €ì¥ ì™„ë£Œ")
                
                yield emit_message(f"   âœ… í…Œì´ë¸” ë²¡í„°ë¼ì´ì§• ì™„ë£Œ: {stats.tables_vectorized}ê°œ í…Œì´ë¸”")
            
        except Exception as e:
            yield emit_message(f"   âš ï¸ í…Œì´ë¸” ë²¡í„°ë¼ì´ì§• ì‹¤íŒ¨: {str(e)[:100]}")
        
        # ===========================================
        # ì»¬ëŸ¼ ë²¡í„°ë¼ì´ì§• (ë°°ì¹˜ ì²˜ë¦¬)
        # ===========================================
        yield emit_message("   ğŸ“Š [Phase 4-2] ì»¬ëŸ¼ ë²¡í„°ë¼ì´ì§• ì‹œì‘...")
        yield emit_phase_event(
            phase_num=4,
            phase_name="ë²¡í„°ë¼ì´ì§•",
            status="in_progress",
            progress=25,
            details={"step": "column_vectorizing"}
        )
        
        # descriptionê³¼ analyzed_descriptionì„ í•©ì³ì„œ ì„ë² ë”© ìƒì„± (ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ)
        column_query = """
        MATCH (__cy_t__:Table)-[:HAS_COLUMN]->(__cy_c__:Column)
        WHERE (__cy_c__.vector IS NULL OR size(__cy_c__.vector) = 0)
          AND (__cy_c__.description IS NOT NULL OR __cy_c__.analyzed_description IS NOT NULL)
        RETURN elementId(__cy_c__) AS cid,
               __cy_c__.name AS column_name,
               __cy_t__.name AS table_name,
               coalesce(__cy_c__.dtype, '') AS dtype,
               trim(
                 coalesce(__cy_c__.description, '') + 
                 CASE WHEN __cy_c__.analyzed_description IS NOT NULL AND __cy_c__.analyzed_description <> '' 
                      THEN ' | AI ë¶„ì„: ' + __cy_c__.analyzed_description 
                      ELSE '' 
                 END
               ) AS description
        ORDER BY __cy_t__.schema, __cy_t__.name, __cy_c__.name
        """
        
        try:
            async with self._cypher_lock:
                result = await client.execute_queries([column_query])
            
            columns = result[0] if result and result[0] else []
            total_columns = len(columns)
            
            if total_columns == 0:
                yield emit_message("      â„¹ï¸ ë²¡í„°í™”í•  ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤")
            else:
                yield emit_message(f"      ğŸ“‹ ë²¡í„°í™” ëŒ€ìƒ: {total_columns}ê°œ ì»¬ëŸ¼")
            
                # ë°°ì¹˜ ì²˜ë¦¬ (50ê°œì”©)
                batch_size = 50
                for i in range(0, total_columns, batch_size):
                    batch = columns[i:i + batch_size]
                    batch_num = i // batch_size + 1
                    total_batches = (total_columns + batch_size - 1) // batch_size
                    texts = []
                    
                    for item in batch:
                        text = embedding_client.format_column_text(
                            column_name=item.get("column_name", ""),
                            table_name=item.get("table_name", ""),
                            dtype=item.get("dtype", ""),
                            description=item.get("description", "")
                        )
                        texts.append(text)
                    
                    # ë°°ì¹˜ ì§„í–‰ ìƒí™© í‘œì‹œ
                    batch_progress = 25 + int(i / total_columns * 75)  # 25-100% ë²”ìœ„
                    yield emit_message(f"      ğŸ”„ [{batch_num}/{total_batches}] ì»¬ëŸ¼ {len(texts)}ê°œ ì„ë² ë”© ìƒì„± ì¤‘...")
                    yield emit_phase_event(
                        phase_num=4,
                        phase_name="ë²¡í„°ë¼ì´ì§•",
                        status="in_progress",
                        progress=batch_progress,
                        details={"step": "column_embedding", "batch": batch_num, "total_batches": total_batches, "done": i, "total": total_columns}
                    )
                    
                    vectors = await embedding_client.embed_batch(texts)
                    
                    # UNWIND ë°°ì¹˜ ì €ì¥ìš© ë°ì´í„° ìƒì„±
                    vector_updates = []
                    for item, vector in zip(batch, vectors):
                        if vector:
                            vector_updates.append({
                                "cid": item['cid'],
                                "vector": vector
                            })
                            stats.columns_vectorized += 1
                    
                    # UNWINDë¡œ í•œë²ˆì— ì €ì¥
                    if vector_updates:
                        update_query = """
                        UNWIND $items AS item
                        MATCH (__cy_c__) WHERE elementId(__cy_c__) = item.cid
                        SET __cy_c__.vector = item.vector
                        RETURN __cy_c__
                        """
                        async with self._cypher_lock:
                            await client.execute_with_params(update_query, {"items": vector_updates})
                        
                        yield emit_message(f"      âœ“ [{batch_num}/{total_batches}] {len(vector_updates)}ê°œ ì»¬ëŸ¼ ë²¡í„° ì €ì¥ ì™„ë£Œ")
                
                yield emit_message(f"   âœ… ì»¬ëŸ¼ ë²¡í„°ë¼ì´ì§• ì™„ë£Œ: {stats.columns_vectorized}ê°œ ì»¬ëŸ¼")
                yield emit_phase_event(
                    phase_num=4,
                    phase_name="ë²¡í„°ë¼ì´ì§•",
                    status="completed",
                    progress=100,
                    details={"tables_vectorized": stats.tables_vectorized, "columns_vectorized": stats.columns_vectorized}
                )
            
        except Exception as e:
            yield emit_message(f"   âš ï¸ ì»¬ëŸ¼ ë²¡í„°ë¼ì´ì§• ì‹¤íŒ¨: {str(e)[:100]}")

    # =========================================================================
    # ë¦¬ë‹ˆì§€ ë¶„ì„ (Phase 5)
    # =========================================================================

    async def _run_lineage_phase(
        self,
        client: Neo4jClient,
        orchestrator: Any,
        stats: AnalysisStats,
    ) -> AsyncGenerator[bytes, None]:
        """ETL íŒ¨í„´ ê°ì§€ ë° ë°ì´í„° ë¦¬ë‹ˆì§€ ê´€ê³„ ìƒì„±
        
        Stored Procedureê°€ ETL ì—­í• ì„ í•˜ëŠ”ì§€ ë¶„ì„í•˜ê³ ,
        Source í…Œì´ë¸” â†’ ETL â†’ Target í…Œì´ë¸” ê°„ ë°ì´í„° íë¦„ ê´€ê³„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        source_dir = orchestrator.dirs.get("source", "")
        
        if not source_dir or not os.path.exists(source_dir):
            yield emit_message("   â„¹ï¸ SP íŒŒì¼ ì—†ìŒ â†’ ë¦¬ë‹ˆì§€ ë¶„ì„ ê±´ë„ˆëœ€")
            return
        
        # SP íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        sql_files = []
        for root, _, files in os.walk(source_dir):
            for f in files:
                if f.endswith(".sql"):
                    sql_files.append(os.path.join(root, f))
        
        if not sql_files:
            yield emit_message("   â„¹ï¸ SP íŒŒì¼ ì—†ìŒ â†’ ë¦¬ë‹ˆì§€ ë¶„ì„ ê±´ë„ˆëœ€")
            return
        
        yield emit_message(f"   ğŸ” {len(sql_files)}ê°œ SP íŒŒì¼ì—ì„œ ETL íŒ¨í„´ ë¶„ì„...")
        
        # ë¦¬ë‹ˆì§€ ë¶„ì„ê¸° ìƒì„±
        lineage_analyzer = LineageAnalyzer(dbms="oracle")
        all_lineages: list[LineageInfo] = []
        
        # ê° SP íŒŒì¼ ë¶„ì„
        for idx, sql_file in enumerate(sql_files, 1):
            file_name = os.path.basename(sql_file)
            
            try:
                async with aiofiles.open(sql_file, "r", encoding="utf-8", errors="ignore") as f:
                    sql_content = await f.read()
                
                # ë¦¬ë‹ˆì§€ ë¶„ì„
                lineages = lineage_analyzer.analyze_sql_content(sql_content, file_name)
                
                # ETL íŒ¨í„´ì´ ê°ì§€ëœ ê²½ìš°ë§Œ ì €ì¥
                etl_lineages = [l for l in lineages if l.is_etl]
                if etl_lineages:
                    for l in etl_lineages:
                        l.file_name = file_name
                    all_lineages.extend(etl_lineages)
                    yield emit_message(
                        f"   âœ… {file_name}: ETL íŒ¨í„´ {len(etl_lineages)}ê°œ ê°ì§€"
                    )
                
            except Exception as e:
                log_process("LINEAGE", "ERROR", f"{file_name} ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        # ETL íŒ¨í„´ì´ ê°ì§€ëœ ê²½ìš° Neo4jì— ì €ì¥
        if all_lineages:
            yield emit_message(f"\n   ğŸ“Š ì´ {len(all_lineages)}ê°œ ETL íŒ¨í„´ â†’ Neo4j ì €ì¥...")
            
            try:
                # name_case ì˜µì…˜ ê°€ì ¸ì˜¤ê¸°
                name_case = getattr(orchestrator, "name_case", "original")
                
                result = await lineage_analyzer.save_lineage_to_neo4j(
                    client=client,
                    lineage_list=all_lineages,
                    file_name="",
                    name_case=name_case,
                )
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                if not hasattr(stats, 'etl_count'):
                    stats.etl_count = 0
                if not hasattr(stats, 'data_flows'):
                    stats.data_flows = 0
                
                stats.etl_count = result.get("etl_nodes", 0)
                stats.data_flows = result.get("data_flows", 0)
                
                yield emit_message(
                    f"   âœ… ë¦¬ë‹ˆì§€ ì €ì¥ ì™„ë£Œ: "
                    f"ETL í”„ë¡œì‹œì € {result.get('etl_nodes', 0)}ê°œ, "
                    f"ETL_READS {result.get('etl_reads', 0)}ê°œ, "
                    f"ETL_WRITES {result.get('etl_writes', 0)}ê°œ, "
                    f"DATA_FLOWS_TO {result.get('data_flows', 0)}ê°œ"
                )
                
            except Exception as e:
                yield emit_message(f"   âš ï¸ ë¦¬ë‹ˆì§€ ì €ì¥ ì‹¤íŒ¨: {str(e)[:100]}")
                log_process("LINEAGE", "ERROR", f"Neo4j ì €ì¥ ì‹¤íŒ¨: {e}")
        else:
            yield emit_message("   â„¹ï¸ ETL íŒ¨í„´ ì—†ìŒ â†’ ë¦¬ë‹ˆì§€ ê´€ê³„ ë¯¸ìƒì„±")
