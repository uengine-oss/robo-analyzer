"""DBMS ì½”ë“œ ë¶„ì„ ì „ëµ - PL/SQL, í”„ë¡œì‹œì €, í•¨ìˆ˜ ë“±

AST ê¸°ë°˜ PL/SQL ì½”ë“œ ë¶„ì„ â†’ Neo4j ê·¸ë˜í”„ ìƒì„±.

ë¶„ì„ íë¦„ (Frameworkì™€ ë™ì¼í•œ 2ë‹¨ê³„ + DDL):
1. [Phase 1] DDL ì²˜ë¦¬ + ëª¨ë“  íŒŒì¼ AST ê·¸ë˜í”„ ìƒì„± (ë³‘ë ¬)
2. [Phase 2] ëª¨ë“  íŒŒì¼ LLM ë¶„ì„ (ë³‘ë ¬)
3. [Phase 3] User Story ë¬¸ì„œ ìƒì„± (BaseStreamingAnalyzer ê³µí†µ)

Phase ë¡œì§ì€ ê° phase íŒŒì¼ì— ë¶„ë¦¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤:
- ddl_phase.py: DDL ì²˜ë¦¬ (Phase 0)
- ast_phase.py: AST ê·¸ë˜í”„ ìƒì„± (Phase 1)
- llm_phase.py: LLM ë¶„ì„ (Phase 2)
- vector_phase.py: ë²¡í„°ë¼ì´ì§• (Phase 4)
- lineage_phase.py: ë¦¬ë‹ˆì§€ ë¶„ì„ (Phase 5)
"""

import asyncio
import json
import logging
import os
from typing import Any, AsyncGenerator, Optional, List, Dict, Tuple

import aiofiles

from analyzer.neo4j_client import Neo4jClient
from analyzer.strategy.base_analyzer import BaseStreamingAnalyzer, AnalysisStats
from analyzer.strategy.base.file_context import FileStatus, FileAnalysisContext
from analyzer.pipeline_control import pipeline_controller, PipelinePhase
from config.settings import settings
from util.stream_event import (
    emit_message,
    emit_phase_event,
)
from util.text_utils import (
    log_process,
    generate_user_story_document,
)

# Phase íŒŒì¼ë“¤ì—ì„œ import
from analyzer.strategy.dbms.ddl_phase import run_ddl_phase
from analyzer.strategy.dbms.ast_phase import run_phase1
from analyzer.strategy.dbms.llm_phase import run_phase2
from analyzer.strategy.dbms.metadata_phase import run_metadata_phase
from analyzer.strategy.dbms.vector_phase import run_vectorize_phase
from analyzer.strategy.dbms.lineage_phase import run_lineage_phase


class DbmsAnalyzer(BaseStreamingAnalyzer):
    """DBMS ì½”ë“œ ë¶„ì„ ì „ëµ
    
    2ë‹¨ê³„ ë¶„ì„ + DDL ì²˜ë¦¬ (Frameworkì™€ ë™ì¼):
    - Phase 0: DDL ì²˜ë¦¬
    - Phase 1: ëª¨ë“  íŒŒì¼ AST ê·¸ë˜í”„ ìƒì„± (ë³‘ë ¬)
    - Phase 2: ëª¨ë“  íŒŒì¼ LLM ë¶„ì„ (ë³‘ë ¬) - Phase1 ì‹¤íŒ¨ íŒŒì¼ ì œì™¸
    - Phase 3: í…Œì´ë¸”/ì»¬ëŸ¼ ì„¤ëª… ë³´ê°•
    - Phase 4: ë²¡í„°ë¼ì´ì§•
    - Phase 5: ë¦¬ë‹ˆì§€ ë¶„ì„
    - User Story ë¬¸ì„œ ìƒì„± (ë¶€ëª¨ í´ë˜ìŠ¤ ê³µí†µ)
    """

    # =========================================================================
    # ì „ëµ ë©”íƒ€ë°ì´í„° (BaseStreamingAnalyzer êµ¬í˜„)
    # =========================================================================
    
    @property
    def strategy_name(self) -> str:
        return "DBMS"
    
    @property
    def strategy_emoji(self) -> str:
        return "ğŸ—„ï¸"
    
    @property
    def file_type_description(self) -> str:
        return "SQL íŒŒì¼"

    def __init__(self):
        self._cypher_lock = asyncio.Lock()
        self._file_semaphore: Optional[asyncio.Semaphore] = None
        self._ddl_schemas: set[str] = set()  # DDLì—ì„œ ìˆ˜ì§‘ëœ ìŠ¤í‚¤ë§ˆ Set
        # DDL ë©”íƒ€ë°ì´í„° ìºì‹œ: {(schema, table_name): {description, columns}}
        self._ddl_table_metadata: Dict[Tuple[str, str], Dict[str, Any]] = {}

    # =========================================================================
    # ë©”ì¸ íŒŒì´í”„ë¼ì¸ (BaseStreamingAnalyzer êµ¬í˜„)
    # =========================================================================

    async def run_pipeline(
        self,
        file_names: list[tuple[str, str]],
        client: Neo4jClient,
        orchestrator: Any,
        stats: AnalysisStats,
    ) -> AsyncGenerator[bytes, None]:
        """DBMS ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        íë¦„ (Frameworkì™€ ë™ì¼):
        1. DDL ì²˜ë¦¬ + íŒŒì¼ ë¡œë“œ (ë³‘ë ¬)
        2. Phase 1: ëª¨ë“  íŒŒì¼ AST ê·¸ë˜í”„ ìƒì„± (ë³‘ë ¬)
        3. Phase 2: ëª¨ë“  íŒŒì¼ LLM ë¶„ì„ (ë³‘ë ¬) - Phase1 ì‹¤íŒ¨ íŒŒì¼ ì œì™¸
        
        Note: User Story PhaseëŠ” ë¶€ëª¨ í´ë˜ìŠ¤ì—ì„œ ì²˜ë¦¬
        """
        total_files = len(file_names)
        self._file_semaphore = asyncio.Semaphore(settings.concurrency.file_concurrency)
        
        # íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì´ˆê¸°í™”
        pipeline_controller.reset()
        pipeline_state = pipeline_controller.get_state()

        # LLM ìºì‹œ ìƒíƒœ í‘œì‹œ
        if settings.llm.cache_enabled:
            cache_path = settings.llm.cache_db_path
            if not os.path.isabs(cache_path):
                cache_path = os.path.join(settings.path.base_dir, cache_path)
            cache_exists = os.path.exists(cache_path)
            cache_size = os.path.getsize(cache_path) if cache_exists else 0
            cache_size_str = f"{cache_size / 1024:.1f}KB" if cache_size < 1024*1024 else f"{cache_size / (1024*1024):.1f}MB"
            yield emit_message(f"ğŸ—„ï¸ LLM ìºì‹œ: í™œì„±í™” ({cache_size_str if cache_exists else 'ì‹ ê·œ'})")
        else:
            yield emit_message("ğŸ”„ LLM ìºì‹œ: ë¹„í™œì„±í™” (ë§¤ë²ˆ ìƒˆë¡œìš´ LLM í˜¸ì¶œ)")

        if total_files > 0:
            yield emit_message(f"âš¡ ë³‘ë ¬ ì²˜ë¦¬: íŒŒì¼ {settings.concurrency.file_concurrency}ê°œ ë™ì‹œ")

        # ========== Phase 0: DDL ì²˜ë¦¬ ==========
        pipeline_state.set_phase(PipelinePhase.DDL_PROCESSING, "DDL íŒŒì¼ ì²˜ë¦¬ ì¤‘", 0)
        yield emit_phase_event(0, "DDL ì²˜ë¦¬", "started", 0, {"canPause": True})
        
        async for chunk in run_ddl_phase(self, client, orchestrator, stats):
            yield chunk
        
        yield emit_phase_event(0, "DDL ì²˜ë¦¬", "completed", 100)
        
        # DDL í›„ ì¼ì‹œì •ì§€ ì²´í¬
        if not await pipeline_state.wait_if_paused():
            yield emit_message("â¹ï¸ íŒŒì´í”„ë¼ì¸ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
            pipeline_state.set_phase(PipelinePhase.CANCELLED)
            return

        # DDLë§Œ ìˆëŠ” ê²½ìš° (ì†ŒìŠ¤ íŒŒì¼ ì—†ìŒ) - Phase 1,2 ìŠ¤í‚µ
        if total_files == 0:
            yield emit_message("")
            yield emit_message("ğŸ“‹ DDL íŒŒì¼ë§Œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤ (ì†ŒìŠ¤ íŒŒì¼ ì—†ìŒ)")
            pipeline_state.set_phase(PipelinePhase.COMPLETED)
            return

        # ========== Phase 1: AST ê·¸ë˜í”„ ìƒì„± ==========
        pipeline_state.set_phase(PipelinePhase.AST_GENERATION, "AST êµ¬ì¡° ê·¸ë˜í”„ ìƒì„± ì¤‘", 0)
        yield emit_phase_event(1, "AST êµ¬ì¡° ìƒì„±", "started", 0, {"canPause": True})
        
        yield emit_message("")
        yield self.emit_separator()
        yield self.emit_phase_header(1, "ğŸ—ï¸ AST êµ¬ì¡° ê·¸ë˜í”„ ìƒì„±", f"{total_files}ê°œ íŒŒì¼ ë³‘ë ¬")
        yield self.emit_separator()

        contexts = await self._load_all_files(file_names, orchestrator)
        yield emit_message(f"   âœ“ {len(contexts)}ê°œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ")

        async for chunk in run_phase1(self, contexts, client, orchestrator, stats):
            yield chunk

        # Phase 1 ê²°ê³¼ ìš”ì•½
        ph1_ok_count = sum(1 for c in contexts if c.status == FileStatus.PH1_OK)
        ph1_fail_count = sum(1 for c in contexts if c.status == FileStatus.PH1_FAIL)
        
        yield emit_message("")
        yield self.emit_phase_complete(1, f"{stats.static_nodes_created}ê°œ ë…¸ë“œ ìƒì„±")
        yield emit_phase_event(1, "AST êµ¬ì¡° ìƒì„±", "completed", 100, {"nodes": stats.static_nodes_created})
        
        if ph1_fail_count > 0:
            yield self.emit_warning(f"Phase 1 ì‹¤íŒ¨: {ph1_fail_count}ê°œ íŒŒì¼ â†’ Phase 2 ìŠ¤í‚µ (í† í° ì ˆê°)")

        # Phase 1 í›„ ì¼ì‹œì •ì§€ ì²´í¬
        if not await pipeline_state.wait_if_paused():
            yield emit_message("â¹ï¸ íŒŒì´í”„ë¼ì¸ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
            pipeline_state.set_phase(PipelinePhase.CANCELLED)
            return

        # ========== Phase 2: LLM ë¶„ì„ ==========
        ph2_targets = [c for c in contexts if c.status == FileStatus.PH1_OK]
        
        pipeline_state.set_phase(PipelinePhase.LLM_ANALYSIS, "AI ë¶„ì„ ì¤‘", 0)
        yield emit_phase_event(2, "AI ë¶„ì„", "started", 0, {"canPause": True, "files": len(ph2_targets)})
        
        yield emit_message("")
        yield self.emit_separator()
        yield self.emit_phase_header(2, "ğŸ¤– AI ë¶„ì„", f"{len(ph2_targets)}ê°œ íŒŒì¼ ë³‘ë ¬")
        yield self.emit_separator()
        
        if ph1_fail_count > 0:
            yield emit_message(f"   â„¹ï¸ {ph1_fail_count}ê°œ íŒŒì¼ì€ Phase 1 ì‹¤íŒ¨ë¡œ ìŠ¤í‚µë¨ (í† í° ì ˆê°)")

        async for chunk in run_phase2(self, ph2_targets, client, orchestrator, stats):
            yield chunk

        yield emit_message("")
        yield self.emit_phase_complete(2, f"{stats.llm_batches_executed}ê°œ ë¶„ì„ ì™„ë£Œ")
        yield emit_phase_event(2, "AI ë¶„ì„", "completed", 100, {"batches": stats.llm_batches_executed})
        
        # Phase 2 í›„ ì¼ì‹œì •ì§€ ì²´í¬
        if not await pipeline_state.wait_if_paused():
            yield emit_message("â¹ï¸ íŒŒì´í”„ë¼ì¸ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
            pipeline_state.set_phase(PipelinePhase.CANCELLED)
            return
        
        # ========== Phase 3: í…Œì´ë¸”/ì»¬ëŸ¼ ì„¤ëª… ë³´ê°• ==========
        # Note: ì‹¤ì œ í…Œì´ë¸” ìš”ì•½ì€ Phase 2ì˜ run_llm_analysis ë‚´ì—ì„œ ì´ë¯¸ ìˆ˜í–‰ë¨
        # ì—¬ê¸°ì„œëŠ” ì§„í–‰ ìƒíƒœë§Œ í‘œì‹œ
        pipeline_state.set_phase(PipelinePhase.TABLE_ENRICHMENT, "í…Œì´ë¸”/ì»¬ëŸ¼ ì„¤ëª… ë³´ê°• ì¤‘", 0)
        yield emit_phase_event(3, "í…Œì´ë¸” ì„¤ëª… ë³´ê°•", "started", 0, {"canPause": True})
        yield self.emit_phase_header(3, "ğŸ“Š í…Œì´ë¸”/ì»¬ëŸ¼ ì„¤ëª… ë³´ê°•", "LLM ë¶„ì„ ê²°ê³¼ ì ìš©")
        
        # í…Œì´ë¸” ìš”ì•½ ê²°ê³¼ ì¹´ìš´íŠ¸ (ì´ë¯¸ Phase 2ì—ì„œ ìˆ˜í–‰ë¨)
        table_count = sum(
            1 for ctx in ph2_targets 
            if ctx.processor and hasattr(ctx.processor, '_table_summary_store') 
            and ctx.processor._table_summary_store
        )
        
        yield emit_message(f"   âœ… í…Œì´ë¸”/ì»¬ëŸ¼ ì„¤ëª… ë³´ê°• ì™„ë£Œ")
        yield self.emit_phase_complete(3, "ì„¤ëª… ë³´ê°• ì™„ë£Œ")
        yield emit_phase_event(3, "í…Œì´ë¸” ì„¤ëª… ë³´ê°•", "completed", 100)
        
        # Phase 3 í›„ ì¼ì‹œì •ì§€ ì²´í¬
        if not await pipeline_state.wait_if_paused():
            yield emit_message("â¹ï¸ íŒŒì´í”„ë¼ì¸ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
            pipeline_state.set_phase(PipelinePhase.CANCELLED)
            return

        # ========== Phase 3.5: ë©”íƒ€ë°ì´í„° ë³´ê°• (Text2SQL ê¸°ë°˜) ==========
        # Text2SQL APIë¥¼ í†µí•´ ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ í›„ LLMìœ¼ë¡œ ì„¤ëª… ìƒì„± + FK ì¶”ë¡ 
        yield emit_message("")
        yield self.emit_separator()
        yield self.emit_phase_header(3.5, "ğŸ“‹ ë©”íƒ€ë°ì´í„° ë³´ê°•", "Text2SQL ê¸°ë°˜ ì„¤ëª… ìƒì„±")
        yield self.emit_separator()
        
        async for chunk in run_metadata_phase(self, client, orchestrator, stats):
            yield chunk
        
        yield emit_message("")
        
        # Phase 3.5 í›„ ì¼ì‹œì •ì§€ ì²´í¬
        if not await pipeline_state.wait_if_paused():
            yield emit_message("â¹ï¸ íŒŒì´í”„ë¼ì¸ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
            pipeline_state.set_phase(PipelinePhase.CANCELLED)
            return

        # ========== Phase 4: ë²¡í„°ë¼ì´ì§• (ì„ë² ë”© ìƒì„±) ==========
        pipeline_state.set_phase(PipelinePhase.VECTORIZING, "í…Œì´ë¸”/ì»¬ëŸ¼ ë²¡í„°ë¼ì´ì§• ì¤‘", 0)
        yield emit_phase_event(4, "ë²¡í„°ë¼ì´ì§•", "started", 0, {"canPause": True})
        yield emit_message("")
        yield self.emit_separator()
        yield self.emit_phase_header(4, "ğŸ”¢ ë²¡í„°ë¼ì´ì§•", "ì„ë² ë”© ìƒì„±")
        yield self.emit_separator()
        
        async for chunk in run_vectorize_phase(self, client, orchestrator, stats):
            yield chunk
        
        yield emit_message("")
        yield self.emit_phase_complete(4, "ë²¡í„°ë¼ì´ì§• ì™„ë£Œ")
        yield emit_phase_event(4, "ë²¡í„°ë¼ì´ì§•", "completed", 100, {
            "tables_vectorized": stats.tables_vectorized,
            "columns_vectorized": stats.columns_vectorized
        })
        
        # Phase 4 í›„ ì¼ì‹œì •ì§€ ì²´í¬
        if not await pipeline_state.wait_if_paused():
            yield emit_message("â¹ï¸ íŒŒì´í”„ë¼ì¸ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
            pipeline_state.set_phase(PipelinePhase.CANCELLED)
            return
        
        # ========== Phase 5: ë¦¬ë‹ˆì§€ ë¶„ì„ (ETL íŒ¨í„´ ê°ì§€) ==========
        pipeline_state.set_phase(PipelinePhase.LINEAGE_ANALYSIS, "ë°ì´í„° ë¦¬ë‹ˆì§€ ë¶„ì„ ì¤‘", 0)
        yield emit_phase_event(5, "ë¦¬ë‹ˆì§€ ë¶„ì„", "started", 0, {"canPause": True})
        yield emit_message("")
        yield self.emit_separator()
        yield self.emit_phase_header(5, "ğŸ”— ë°ì´í„° ë¦¬ë‹ˆì§€ ë¶„ì„", "ETL íŒ¨í„´ ê°ì§€")
        yield self.emit_separator()
        
        async for chunk in run_lineage_phase(self, client, orchestrator, stats):
            yield chunk
        
        yield emit_message("")
        yield self.emit_phase_complete(5, "ë¦¬ë‹ˆì§€ ë¶„ì„ ì™„ë£Œ")
        yield emit_phase_event(5, "ë¦¬ë‹ˆì§€ ë¶„ì„", "completed", 100, {
            "etl_count": getattr(stats, 'etl_count', 0),
            "data_flows": getattr(stats, 'data_flows', 0)
        })
        
        # Phase 5 í›„ ì¼ì‹œì •ì§€ ì²´í¬
        if not await pipeline_state.wait_if_paused():
            yield emit_message("â¹ï¸ íŒŒì´í”„ë¼ì¸ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
            pipeline_state.set_phase(PipelinePhase.CANCELLED)
            return

    # =========================================================================
    # User Story ë¬¸ì„œ ìƒì„± (BaseStreamingAnalyzer êµ¬í˜„)
    # =========================================================================

    async def build_user_story_doc(
        self,
        client: Neo4jClient,
        orchestrator: Any,
    ) -> Optional[str]:
        """ë¶„ì„ëœ í”„ë¡œì‹œì €ì—ì„œ User Story ë¬¸ì„œ ìƒì„±"""
        query = """
            MATCH (__cy_n__)
            WHERE (__cy_n__:PROCEDURE OR __cy_n__:FUNCTION OR __cy_n__:TRIGGER)
              AND __cy_n__.summary IS NOT NULL
            OPTIONAL MATCH (__cy_n__)-[:HAS_USER_STORY]->(__cy_us__:UserStory)
            OPTIONAL MATCH (__cy_us__)-[:HAS_AC]->(__cy_ac__:AcceptanceCriteria)
            WITH __cy_n__, __cy_us__, collect(DISTINCT {
                id: __cy_ac__.id,
                title: __cy_ac__.title,
                given: __cy_ac__.given,
                when: __cy_ac__.when,
                then: __cy_ac__.then
            }) AS acceptance_criteria
            WITH __cy_n__, collect(DISTINCT {
                id: __cy_us__.id,
                role: __cy_us__.role,
                goal: __cy_us__.goal,
                benefit: __cy_us__.benefit,
                acceptance_criteria: acceptance_criteria
            }) AS user_stories
            RETURN __cy_n__.procedure_name AS name, 
                   __cy_n__.summary AS summary,
                   user_stories AS user_stories, 
                   labels(__cy_n__)[0] AS type
            ORDER BY __cy_n__.file_name, __cy_n__.startLine
        """
        
        async with self._cypher_lock:
            results = await client.execute_queries([query])
        
        # DDLë§Œ ìˆëŠ” ê²½ìš° ë˜ëŠ” ë¶„ì„ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° None ë°˜í™˜ (ì˜¤ë¥˜ ëŒ€ì‹ )
        if not results or not results[0]:
            log_process("ANALYZE", "USER_STORY", "User Story ìƒì„± ìŠ¤í‚µ: ë¶„ì„ëœ í”„ë¡œì‹œì €/í•¨ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤", logging.INFO)
            return None
        
        filtered = [
            r for r in results[0]
            if r.get("summary") or (r.get("user_stories") and len(r["user_stories"]) > 0)
        ]
        
        if not filtered:
            return None
        
        log_process("ANALYZE", "USER_STORY", f"User Story ìƒì„± | ëŒ€ìƒ={len(filtered)}ê°œ í”„ë¡œì‹œì €")
        return generate_user_story_document(
            results=filtered,
            source_name="ROBO",
            source_type="DBMS í”„ë¡œì‹œì €/í•¨ìˆ˜",
        )

    # =========================================================================
    # íŒŒì¼ ë¡œë“œ
    # =========================================================================

    async def _load_all_files(
        self,
        file_names: list[tuple[str, str]],
        orchestrator: Any,
    ) -> List[FileAnalysisContext]:
        """ëª¨ë“  íŒŒì¼ì˜ AST JSONì„ ë³‘ë ¬ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.
        
        source íŒŒì¼ì€ ë” ì´ìƒ ì½ì§€ ì•ŠìŠµë‹ˆë‹¤ - AST JSONì˜ code ì†ì„± ì‚¬ìš©.
        """
        
        async def load_single(directory: str, file_name: str) -> FileAnalysisContext:
            base_name = os.path.splitext(file_name)[0]
            ast_path = os.path.join(orchestrator.dirs["analysis"], directory, f"{base_name}.json")

            async with aiofiles.open(ast_path, "r", encoding="utf-8") as ast_file:
                ast_content = await ast_file.read()
                return FileAnalysisContext(
                    directory=directory,
                    file_name=file_name,
                    ast_data=json.loads(ast_content),
                )

        tasks = [load_single(d, f) for d, f in file_names]
        return await asyncio.gather(*tasks)
