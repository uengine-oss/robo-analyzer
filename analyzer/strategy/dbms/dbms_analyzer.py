"""DBMS ì½”ë“œ ë¶„ì„ ì „ëµ - PL/SQL, í”„ë¡œì‹œì €, í•¨ìˆ˜ ë“±

AST ê¸°ë°˜ PL/SQL ì½”ë“œ ë¶„ì„ â†’ Neo4j ê·¸ë˜í”„ ìƒì„±.

ë¶„ì„ íë¦„ (Frameworkì™€ ë™ì¼í•œ 2ë‹¨ê³„ + DDL):
1. [Phase 1] DDL ì²˜ë¦¬ + ëª¨ë“  íŒŒì¼ AST ê·¸ë˜í”„ ìƒì„± (ë³‘ë ¬)
2. [Phase 2] ëª¨ë“  íŒŒì¼ LLM ë¶„ì„ (ë³‘ë ¬)
3. [Phase 3] User Story ë¬¸ì„œ ìƒì„± (BaseStreamingAnalyzer ê³µí†µ)
"""

import asyncio
import json
import logging
import os
from typing import Any, AsyncGenerator, Optional, List, Dict, Tuple

import aiofiles

from analyzer.neo4j_client import Neo4jClient
from analyzer.strategy.base_analyzer import BaseStreamingAnalyzer, AnalysisStats
from analyzer.strategy.base.file_context import FileStatus, FileAnalysisContext
from analyzer.strategy.dbms.ast_processor import DbmsAstProcessor
from analyzer.pipeline_control import pipeline_controller, PipelinePhase
from config.settings import settings
from util.exception import AnalysisError
from util.rule_loader import RuleLoader
from util.utility_tool import escape_for_cypher
from util.stream_utils import (
    emit_data,
    emit_message,
    emit_phase_event,
    format_graph_result,
)
from util.utility_tool import (
    escape_for_cypher,
    log_process,
    parse_table_identifier,
    generate_user_story_document,
    split_ddl_into_chunks,
    calculate_code_token,
)
from util.embedding_client import EmbeddingClient


class DbmsAnalyzer(BaseStreamingAnalyzer):
    """DBMS ì½”ë“œ ë¶„ì„ ì „ëµ
    
    2ë‹¨ê³„ ë¶„ì„ + DDL ì²˜ë¦¬ (Frameworkì™€ ë™ì¼):
    - Phase 1: DDL ì²˜ë¦¬ + ëª¨ë“  íŒŒì¼ AST ê·¸ë˜í”„ ìƒì„± (ë³‘ë ¬)
    - Phase 2: ëª¨ë“  íŒŒì¼ LLM ë¶„ì„ (ë³‘ë ¬) - Phase1 ì‹¤íŒ¨ íŒŒì¼ ì œì™¸
    - Phase 3: User Story ë¬¸ì„œ ìƒì„± (ë¶€ëª¨ í´ë˜ìŠ¤ ê³µí†µ)
    """

    # =========================================================================
    # ì „ëµ ë©”íƒ€ë°ì´í„° (BaseStreamingAnalyzer êµ¬í˜„)
    # =========================================================================
    
    @property
    def strategy_name(self) -> str:
        return "DBMS"
    
    @property
    def strategy_emoji(self) -> str:
        return "ğŸ—„ï¸"
    
    @property
    def file_type_description(self) -> str:
        return "SQL íŒŒì¼"

    def __init__(self):
        self._cypher_lock = asyncio.Lock()
        self._file_semaphore: Optional[asyncio.Semaphore] = None
        self._ddl_schemas: set[str] = set()  # DDLì—ì„œ ìˆ˜ì§‘ëœ ìŠ¤í‚¤ë§ˆ Set
        # DDL ë©”íƒ€ë°ì´í„° ìºì‹œ: {(schema, table_name): {description, columns}}
        self._ddl_table_metadata: Dict[Tuple[str, str], Dict[str, Any]] = {}

    # =========================================================================
    # ë©”ì¸ íŒŒì´í”„ë¼ì¸ (BaseStreamingAnalyzer êµ¬í˜„)
    # =========================================================================

    async def run_pipeline(
        self,
        file_names: list[tuple[str, str]],
        client: Neo4jClient,
        orchestrator: Any,
        stats: AnalysisStats,
    ) -> AsyncGenerator[bytes, None]:
        """DBMS ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        íë¦„ (Frameworkì™€ ë™ì¼):
        1. DDL ì²˜ë¦¬ + íŒŒì¼ ë¡œë“œ (ë³‘ë ¬)
        2. Phase 1: ëª¨ë“  íŒŒì¼ AST ê·¸ë˜í”„ ìƒì„± (ë³‘ë ¬)
        3. Phase 2: ëª¨ë“  íŒŒì¼ LLM ë¶„ì„ (ë³‘ë ¬) - Phase1 ì‹¤íŒ¨ íŒŒì¼ ì œì™¸
        
        Note: User Story PhaseëŠ” ë¶€ëª¨ í´ë˜ìŠ¤ì—ì„œ ì²˜ë¦¬
        """
        total_files = len(file_names)
        self._file_semaphore = asyncio.Semaphore(settings.concurrency.file_concurrency)
        
        # íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì´ˆê¸°í™”
        pipeline_state = pipeline_controller.get_state()

        # LLM ìºì‹œ ìƒíƒœ í‘œì‹œ
        if settings.llm.cache_enabled:
            cache_path = settings.llm.cache_db_path
            if not os.path.isabs(cache_path):
                cache_path = os.path.join(settings.path.base_dir, cache_path)
            cache_exists = os.path.exists(cache_path)
            cache_size = os.path.getsize(cache_path) if cache_exists else 0
            cache_size_str = f"{cache_size / 1024:.1f}KB" if cache_size < 1024*1024 else f"{cache_size / (1024*1024):.1f}MB"
            yield emit_message(f"ğŸ—„ï¸ LLM ìºì‹œ: í™œì„±í™” ({cache_size_str if cache_exists else 'ì‹ ê·œ'})")
        else:
            yield emit_message("ğŸ”„ LLM ìºì‹œ: ë¹„í™œì„±í™” (ë§¤ë²ˆ ìƒˆë¡œìš´ LLM í˜¸ì¶œ)")

        if total_files > 0:
            yield emit_message(f"âš¡ ë³‘ë ¬ ì²˜ë¦¬: íŒŒì¼ {settings.concurrency.file_concurrency}ê°œ ë™ì‹œ")

        # ========== Phase 0: DDL ì²˜ë¦¬ ==========
        pipeline_state.set_phase(PipelinePhase.DDL_PROCESSING, "DDL íŒŒì¼ ì²˜ë¦¬ ì¤‘", 0)
        yield emit_phase_event(0, "DDL ì²˜ë¦¬", "started", 0, {"canPause": True})
        
        async for chunk in self._run_ddl_phase(client, orchestrator, stats):
            yield chunk
        
        yield emit_phase_event(0, "DDL ì²˜ë¦¬", "completed", 100)
        
        # DDL í›„ ì¼ì‹œì •ì§€ ì²´í¬
        if not await pipeline_state.wait_if_paused():
            yield emit_message("â¹ï¸ íŒŒì´í”„ë¼ì¸ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
            pipeline_state.set_phase(PipelinePhase.CANCELLED)
            return

        # DDLë§Œ ìˆëŠ” ê²½ìš° (ì†ŒìŠ¤ íŒŒì¼ ì—†ìŒ) - Phase 1,2 ìŠ¤í‚µ
        if total_files == 0:
            yield emit_message("")
            yield emit_message("ğŸ“‹ DDL íŒŒì¼ë§Œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤ (ì†ŒìŠ¤ íŒŒì¼ ì—†ìŒ)")
            pipeline_state.set_phase(PipelinePhase.COMPLETED)
            return

        # ========== Phase 1: AST ê·¸ë˜í”„ ìƒì„± ==========
        pipeline_state.set_phase(PipelinePhase.AST_GENERATION, "AST êµ¬ì¡° ê·¸ë˜í”„ ìƒì„± ì¤‘", 0)
        yield emit_phase_event(1, "AST êµ¬ì¡° ìƒì„±", "started", 0, {"canPause": True})
        
        yield emit_message("")
        yield self.emit_separator()
        yield self.emit_phase_header(1, "ğŸ—ï¸ AST êµ¬ì¡° ê·¸ë˜í”„ ìƒì„±", f"{total_files}ê°œ íŒŒì¼ ë³‘ë ¬")
        yield self.emit_separator()

        contexts = await self._load_all_files(file_names, orchestrator)
        yield emit_message(f"   âœ“ {len(contexts)}ê°œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ")

        async for chunk in self._run_phase1(contexts, client, orchestrator, stats):
            yield chunk

        # Phase 1 ê²°ê³¼ ìš”ì•½
        ph1_ok_count = sum(1 for c in contexts if c.status == FileStatus.PH1_OK)
        ph1_fail_count = sum(1 for c in contexts if c.status == FileStatus.PH1_FAIL)
        
        yield emit_message("")
        yield self.emit_phase_complete(1, f"{stats.static_nodes_created}ê°œ ë…¸ë“œ ìƒì„±")
        yield emit_phase_event(1, "AST êµ¬ì¡° ìƒì„±", "completed", 100, {"nodes": stats.static_nodes_created})
        
        if ph1_fail_count > 0:
            yield self.emit_warning(f"Phase 1 ì‹¤íŒ¨: {ph1_fail_count}ê°œ íŒŒì¼ â†’ Phase 2 ìŠ¤í‚µ (í† í° ì ˆê°)")

        # Phase 1 í›„ ì¼ì‹œì •ì§€ ì²´í¬
        if not await pipeline_state.wait_if_paused():
            yield emit_message("â¹ï¸ íŒŒì´í”„ë¼ì¸ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
            pipeline_state.set_phase(PipelinePhase.CANCELLED)
            return

        # ========== Phase 2: LLM ë¶„ì„ ==========
        ph2_targets = [c for c in contexts if c.status == FileStatus.PH1_OK]
        
        pipeline_state.set_phase(PipelinePhase.LLM_ANALYSIS, "AI ë¶„ì„ ì¤‘", 0)
        yield emit_phase_event(2, "AI ë¶„ì„", "started", 0, {"canPause": True, "files": len(ph2_targets)})
        
        yield emit_message("")
        yield self.emit_separator()
        yield self.emit_phase_header(2, "ğŸ¤– AI ë¶„ì„", f"{len(ph2_targets)}ê°œ íŒŒì¼ ë³‘ë ¬")
        yield self.emit_separator()
        
        if ph1_fail_count > 0:
            yield emit_message(f"   â„¹ï¸ {ph1_fail_count}ê°œ íŒŒì¼ì€ Phase 1 ì‹¤íŒ¨ë¡œ ìŠ¤í‚µë¨ (í† í° ì ˆê°)")

        async for chunk in self._run_phase2(ph2_targets, client, orchestrator, stats):
            yield chunk

        yield emit_message("")
        yield self.emit_phase_complete(2, f"{stats.llm_batches_executed}ê°œ ë¶„ì„ ì™„ë£Œ")
        yield emit_phase_event(2, "AI ë¶„ì„", "completed", 100, {"batches": stats.llm_batches_executed})
        
        # Phase 2 í›„ ì¼ì‹œì •ì§€ ì²´í¬
        if not await pipeline_state.wait_if_paused():
            yield emit_message("â¹ï¸ íŒŒì´í”„ë¼ì¸ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
            pipeline_state.set_phase(PipelinePhase.CANCELLED)
            return
        
        # ========== Phase 3: í…Œì´ë¸”/ì»¬ëŸ¼ ì„¤ëª… ë³´ê°• ==========
        # Note: ì‹¤ì œ í…Œì´ë¸” ìš”ì•½ì€ Phase 2ì˜ run_llm_analysis ë‚´ì—ì„œ ì´ë¯¸ ìˆ˜í–‰ë¨
        # ì—¬ê¸°ì„œëŠ” ì§„í–‰ ìƒíƒœë§Œ í‘œì‹œ
        pipeline_state.set_phase(PipelinePhase.TABLE_ENRICHMENT, "í…Œì´ë¸”/ì»¬ëŸ¼ ì„¤ëª… ë³´ê°• ì¤‘", 0)
        yield emit_phase_event(3, "í…Œì´ë¸” ì„¤ëª… ë³´ê°•", "started", 0, {"canPause": True})
        yield self.emit_phase_header(3, "ğŸ“Š í…Œì´ë¸”/ì»¬ëŸ¼ ì„¤ëª… ë³´ê°•", "LLM ë¶„ì„ ê²°ê³¼ ì ìš©")
        
        # í…Œì´ë¸” ìš”ì•½ ê²°ê³¼ ì¹´ìš´íŠ¸ (ì´ë¯¸ Phase 2ì—ì„œ ìˆ˜í–‰ë¨)
        table_count = sum(
            1 for ctx in ph2_targets 
            if ctx.processor and hasattr(ctx.processor, '_table_summary_store') 
            and ctx.processor._table_summary_store
        )
        
        yield emit_message(f"   âœ… í…Œì´ë¸”/ì»¬ëŸ¼ ì„¤ëª… ë³´ê°• ì™„ë£Œ")
        yield self.emit_phase_complete(3, "ì„¤ëª… ë³´ê°• ì™„ë£Œ")
        yield emit_phase_event(3, "í…Œì´ë¸” ì„¤ëª… ë³´ê°•", "completed", 100)
        
        # Phase 3 í›„ ì¼ì‹œì •ì§€ ì²´í¬
        if not await pipeline_state.wait_if_paused():
            yield emit_message("â¹ï¸ íŒŒì´í”„ë¼ì¸ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
            pipeline_state.set_phase(PipelinePhase.CANCELLED)
            return

        # ========== Phase 4: ë²¡í„°ë¼ì´ì§• (ì„ë² ë”© ìƒì„±) ==========
        pipeline_state.set_phase(PipelinePhase.VECTORIZING, "í…Œì´ë¸”/ì»¬ëŸ¼ ë²¡í„°ë¼ì´ì§• ì¤‘", 0)
        yield emit_phase_event(4, "ë²¡í„°ë¼ì´ì§•", "started", 0, {"canPause": True})
        yield emit_message("")
        yield self.emit_separator()
        yield self.emit_phase_header(4, "ğŸ”¢ ë²¡í„°ë¼ì´ì§•", "ì„ë² ë”© ìƒì„±")
        yield self.emit_separator()
        
        async for chunk in self._run_vectorize_phase(client, orchestrator, stats):
            yield chunk
        
        yield emit_message("")
        yield self.emit_phase_complete(4, "ë²¡í„°ë¼ì´ì§• ì™„ë£Œ")
        yield emit_phase_event(4, "ë²¡í„°ë¼ì´ì§•", "completed", 100, {
            "tables_vectorized": stats.tables_vectorized,
            "columns_vectorized": stats.columns_vectorized
        })
        
        # Phase 4 í›„ ì¼ì‹œì •ì§€ ì²´í¬
        if not await pipeline_state.wait_if_paused():
            yield emit_message("â¹ï¸ íŒŒì´í”„ë¼ì¸ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
            pipeline_state.set_phase(PipelinePhase.CANCELLED)
            return

    # =========================================================================
    # User Story ë¬¸ì„œ ìƒì„± (BaseStreamingAnalyzer êµ¬í˜„)
    # =========================================================================

    async def build_user_story_doc(
        self,
        client: Neo4jClient,
        orchestrator: Any,
    ) -> Optional[str]:
        """ë¶„ì„ëœ í”„ë¡œì‹œì €ì—ì„œ User Story ë¬¸ì„œ ìƒì„±"""
        query = """
            MATCH (n)
            WHERE (n:PROCEDURE OR n:FUNCTION OR n:TRIGGER)
              AND n.summary IS NOT NULL
            OPTIONAL MATCH (n)-[:HAS_USER_STORY]->(us:UserStory)
            OPTIONAL MATCH (us)-[:HAS_AC]->(ac:AcceptanceCriteria)
            WITH n, us, collect(DISTINCT {
                id: ac.id,
                title: ac.title,
                given: ac.given,
                when: ac.when,
                then: ac.then
            }) AS acceptance_criteria
            WITH n, collect(DISTINCT {
                id: us.id,
                role: us.role,
                goal: us.goal,
                benefit: us.benefit,
                acceptance_criteria: acceptance_criteria
            }) AS user_stories
            RETURN n.procedure_name AS name, 
                   n.summary AS summary,
                   user_stories AS user_stories, 
                   labels(n)[0] AS type
            ORDER BY n.file_name, n.startLine
        """
        
        async with self._cypher_lock:
            results = await client.execute_queries([query])
        
        # DDLë§Œ ìˆëŠ” ê²½ìš° ë˜ëŠ” ë¶„ì„ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° None ë°˜í™˜ (ì˜¤ë¥˜ ëŒ€ì‹ )
        if not results or not results[0]:
            log_process("ANALYZE", "USER_STORY", "User Story ìƒì„± ìŠ¤í‚µ: ë¶„ì„ëœ í”„ë¡œì‹œì €/í•¨ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤", logging.INFO)
            return None
        
        filtered = [
            r for r in results[0]
            if r.get("summary") or (r.get("user_stories") and len(r["user_stories"]) > 0)
        ]
        
        if not filtered:
            return None
        
        log_process("ANALYZE", "USER_STORY", f"User Story ìƒì„± | ëŒ€ìƒ={len(filtered)}ê°œ í”„ë¡œì‹œì €")
        return generate_user_story_document(
            results=filtered,
            source_name="ROBO",
            source_type="DBMS í”„ë¡œì‹œì €/í•¨ìˆ˜",
        )

    # =========================================================================
    # DDL ì²˜ë¦¬
    # =========================================================================

    async def _run_ddl_phase(
        self,
        client: Neo4jClient,
        orchestrator: Any,
        stats: AnalysisStats,
    ) -> AsyncGenerator[bytes, None]:
        """DDL íŒŒì¼ ì²˜ë¦¬ - í…Œì´ë¸”/ì»¬ëŸ¼ ìŠ¤í‚¤ë§ˆ ìƒì„±"""
        ddl_files = self._list_ddl_files(orchestrator)
        
        if not ddl_files:
            yield self.emit_skip("DDL íŒŒì¼ ì—†ìŒ â†’ ìŠ¤í‚¤ë§ˆ ì²˜ë¦¬ ê±´ë„ˆëœ€")
            return
        
        ddl_count = len(ddl_files)
        yield emit_message("")
        yield self.emit_separator()
        yield self.emit_phase_header(0, "ğŸ“‹ DDL ìŠ¤í‚¤ë§ˆ ìˆ˜ì§‘", f"{ddl_count}ê°œ DDL")
        yield self.emit_separator()
        
        ddl_dir = orchestrator.dirs["ddl"]
        
        for idx, ddl_file in enumerate(ddl_files, 1):
            yield emit_message("")
            yield self.emit_file_start(idx, ddl_count, ddl_file)
            
            ddl_graph, ddl_stats = await self._process_ddl(
                ddl_path=os.path.join(ddl_dir, ddl_file),
                client=client,
                file_name=ddl_file,
                orchestrator=orchestrator,
            )
            
            if ddl_stats["tables"]:
                yield emit_message(f"   âœ“ Table ë…¸ë“œ: {ddl_stats['tables']}ê°œ")
            if ddl_stats["columns"]:
                yield emit_message(f"   âœ“ Column ë…¸ë“œ: {ddl_stats['columns']}ê°œ")
            if ddl_stats["fks"]:
                yield emit_message(f"   âœ“ FK ê´€ê³„: {ddl_stats['fks']}ê°œ")
            
            stats.add_ddl_result(ddl_stats["tables"], ddl_stats["columns"], ddl_stats["fks"])
            
            if ddl_graph and (ddl_graph.get("Nodes") or ddl_graph.get("Relationships")):
                yield emit_data(
                    graph=ddl_graph,
                    line_number=0,
                    analysis_progress=0,
                    current_file=f"DDL-{ddl_file}",
                )
        
        yield emit_message("")
        yield emit_message("ğŸ“Š DDL ì²˜ë¦¬ ì™„ë£Œ:")
        yield emit_message(f"   â€¢ í…Œì´ë¸”: {stats.ddl_tables}ê°œ")
        yield emit_message(f"   â€¢ ì»¬ëŸ¼: {stats.ddl_columns}ê°œ")
        yield emit_message(f"   â€¢ FK: {stats.ddl_fks}ê°œ")

    def _list_ddl_files(self, orchestrator: Any) -> list[str]:
        """DDL íŒŒì¼ ëª©ë¡ ì¡°íšŒ
        
        DDL ë””ë ‰í† ë¦¬ê°€ ì—†ê±°ë‚˜ íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ê²½ê³  ì²˜ë¦¬, ì—ëŸ¬ ì•„ë‹˜)
        """
        ddl_dir = orchestrator.dirs.get("ddl", "")
        if not ddl_dir:
            log_process("ANALYZE", "DDL", "DDL ë””ë ‰í† ë¦¬ ì„¤ì • ì—†ìŒ - DDL ì²˜ë¦¬ ìƒëµ")
            return []
        if not os.path.isdir(ddl_dir):
            # DDL ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ê²½ê³ ë§Œ í•˜ê³  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            log_process("ANALYZE", "DDL", f"DDL ë””ë ‰í† ë¦¬ ì—†ìŒ: {ddl_dir} - DDL ì²˜ë¦¬ ìƒëµ")
            return []
        try:
            files = sorted(
                f for f in os.listdir(ddl_dir)
                if os.path.isfile(os.path.join(ddl_dir, f))
            )
            if not files:
                # DDL íŒŒì¼ì´ ì—†ìœ¼ë©´ ê²½ê³ ë§Œ í•˜ê³  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
                log_process("ANALYZE", "DDL", f"DDL ë””ë ‰í† ë¦¬ì— íŒŒì¼ ì—†ìŒ: {ddl_dir} - DDL ì²˜ë¦¬ ìƒëµ")
                return []
            log_process("ANALYZE", "DDL", f"DDL íŒŒì¼ ë°œê²¬: {len(files)}ê°œ")
            return files
        except OSError as e:
            log_process("ANALYZE", "DDL", f"DDL ë””ë ‰í† ë¦¬ ì½ê¸° ì‹¤íŒ¨: {ddl_dir} - {e}")
            return []

    def _apply_name_case(self, name: str, name_case: str) -> str:
        """ë©”íƒ€ë°ì´í„° ëŒ€ì†Œë¬¸ì ë³€í™˜ ì ìš©
        
        Args:
            name: ë³€í™˜í•  ì´ë¦„ (í…Œì´ë¸”ëª…, ì»¬ëŸ¼ëª…, ìŠ¤í‚¤ë§ˆëª… ë“±)
            name_case: ë³€í™˜ ì˜µì…˜ (original, uppercase, lowercase)
        
        Returns:
            ë³€í™˜ëœ ì´ë¦„
        """
        if not name:
            return name
        if name_case == "uppercase":
            return name.upper()
        elif name_case == "lowercase":
            return name.lower()
        return name  # original: ê·¸ëŒ€ë¡œ ë°˜í™˜

    async def _process_ddl(
        self,
        ddl_path: str,
        client: Neo4jClient,
        file_name: str,
        orchestrator: Any,
    ) -> tuple[dict, dict]:
        """DDL íŒŒì¼ ì²˜ë¦¬ ë° í…Œì´ë¸”/ì»¬ëŸ¼ ë…¸ë“œ ìƒì„±
        
        ëŒ€ìš©ëŸ‰ DDL íŒŒì¼ì˜ ê²½ìš° CREATE TABLE ë‹¨ìœ„ë¡œ ì²­í¬ ë¶„í• í•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        ê° ì²­í¬ì—ëŠ” CREATE TABLE, COMMENT ON, ALTER TABLE êµ¬ë¬¸ì´ í•¨ê»˜ í¬í•¨ë©ë‹ˆë‹¤.
        """
        ddl_stats = {"tables": 0, "columns": 0, "fks": 0}
        
        async with aiofiles.open(ddl_path, "r", encoding="utf-8") as f:
            ddl_content = await f.read()
        
        # ëŒ€ìš©ëŸ‰ DDL ì²­í¬ ë¶„í• 
        ddl_chunks = split_ddl_into_chunks(ddl_content)
        total_tokens = calculate_code_token(ddl_content)
        chunk_count = len(ddl_chunks)
        
        if chunk_count > 1:
            log_process("DDL", "CHUNK", f"ğŸ“¦ ëŒ€ìš©ëŸ‰ DDL ë¶„í• : {total_tokens:,} í† í° â†’ {chunk_count}ê°œ ì²­í¬")
        
        loader = RuleLoader(target_lang="dbms")
        
        # ì²­í¬ë³„ LLM í˜¸ì¶œ ë° ê²°ê³¼ ë³‘í•©
        all_parsed_results: List[Dict] = []
        for chunk_idx, chunk in enumerate(ddl_chunks, 1):
            chunk_tokens = calculate_code_token(chunk)
            if chunk_count > 1:
                log_process("DDL", "CHUNK", f"  ì²­í¬ {chunk_idx}/{chunk_count} ì²˜ë¦¬ ì¤‘ ({chunk_tokens:,} í† í°)")
            
            try:
                # LLM í˜¸ì¶œì„ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬ (I/O ë¸”ë¡œí‚¹ ë°©ì§€)
                import asyncio
                chunk_parsed = await asyncio.to_thread(
                    loader.execute,
                    "ddl",
                    {"ddl_content": chunk, "locale": orchestrator.locale},
                    orchestrator.api_key,
                )
                tables_in_chunk = len(chunk_parsed.get("analysis", []))
                all_parsed_results.extend(chunk_parsed.get("analysis", []))
                
                if chunk_count > 1:
                    log_process("DDL", "CHUNK", f"  âœ… ì²­í¬ {chunk_idx} ì™„ë£Œ: {tables_in_chunk}ê°œ í…Œì´ë¸” íŒŒì‹±")
            except Exception as e:
                log_process("DDL", "ERROR", f"  âŒ ì²­í¬ {chunk_idx} ì‹¤íŒ¨: {str(e)[:100]}")
                raise AnalysisError(f"DDL ì²­í¬ {chunk_idx} íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        # ë³‘í•©ëœ ê²°ê³¼ë¥¼ parsedë¡œ ì‚¬ìš©
        parsed = {"analysis": all_parsed_results}
        
        queries = []
        # db ì†ì„±ì€ DML ì²˜ë¦¬(ast_processor)ì™€ ì¼ê´€ì„±ì„ ìœ„í•´ ì†Œë¬¸ìë¡œ ë³€í™˜
        common = {
            "db": (orchestrator.target or 'postgres').lower(),
        }
        
        # ëŒ€ì†Œë¬¸ì ë³€í™˜ ì˜µì…˜
        name_case = getattr(orchestrator, 'name_case', 'original')

        for table_info in parsed.get("analysis", []):
            table = table_info.get("table", {})
            columns = table_info.get("columns", [])
            foreign_keys = table_info.get("foreignKeys", [])
            primary_keys = [
                str(pk).strip().upper()
                for pk in (table_info.get("primaryKeys") or [])
                if pk
            ]

            # ì›ë³¸ ê°’ì—ì„œ ë”°ì˜´í‘œ ì œê±° í›„ ëŒ€ì†Œë¬¸ì ë³€í™˜ ì ìš©
            schema_raw = (table.get("schema") or "").strip()
            table_name_raw = (table.get("name") or "").strip()
            comment = (table.get("comment") or "").strip()
            table_type = (table.get("table_type") or "BASE TABLE").strip().upper()
            
            # parse_table_identifierë¡œ ë”°ì˜´í‘œ ì œê±° ë° ìŠ¤í‚¤ë§ˆ/í…Œì´ë¸” ë¶„ë¦¬
            qualified = f"{schema_raw}.{table_name_raw}" if schema_raw else table_name_raw
            parsed_schema, parsed_name, _ = parse_table_identifier(qualified)
            
            # parse_table_identifierê°€ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ë¯€ë¡œ, ì—¬ê¸°ì„œ ë‹¤ì‹œ ëŒ€ì†Œë¬¸ì ë³€í™˜ ì ìš©
            schema = self._apply_name_case(parsed_schema if parsed_schema else "public", name_case)
            parsed_name = self._apply_name_case(parsed_name, name_case)
            
            # DDLì—ì„œ ë°œê²¬ëœ ìŠ¤í‚¤ë§ˆ ìˆ˜ì§‘ (ë‚´ë¶€ ë¹„êµìš©ìœ¼ë¡œ ì†Œë¬¸ì ì €ì¥)
            if schema and schema.lower() != 'public':
                self._ddl_schemas.add(schema.lower())

            # Table ë…¸ë“œ ìƒì„± (MERGE í‚¤: db, schema, name ì‚¬ìš©)
            # ê°™ì€ ìŠ¤í‚¤ë§ˆ/í…Œì´ë¸”ëª…ì´ë©´ ê°™ì€ ë…¸ë“œë¡œ ì·¨ê¸‰í•´ì•¼ í•¨
            merge_key = {
                "db": common["db"],
                "schema": schema,
                "name": parsed_name
            }
            merge_str = ", ".join(f"`{k}`: '{v}'" for k, v in merge_key.items())
            
            column_metadata = {}
            for col in columns:
                col_name_raw = (col.get("name") or "").strip()
                if not col_name_raw:
                    continue
                col_name = self._apply_name_case(col_name_raw, name_case)
                col_comment = (col.get("comment") or "").strip()
                column_metadata[col_name] = {
                    "description": col_comment,
                    "dtype": (col.get("dtype") or col.get("type") or "").strip(),
                    "nullable": col.get("nullable", True),
                }
            
            set_props = {
                **common,
                "description": escape_for_cypher(comment),
                "description_source": "ddl" if comment else "",  # DDLì—ì„œ ì¶”ì¶œëœ ì„¤ëª…
                "table_type": table_type,
            }
            set_str = ", ".join(f"t.`{k}` = '{v}'" for k, v in set_props.items())
            
            # Schema ë…¸ë“œ ìƒì„± (ìŠ¤í‚¤ë§ˆê°€ ì—†ìœ¼ë©´ 'public' ì‚¬ìš©)
            # ëŒ€ì†Œë¬¸ì ë³€í™˜ì€ ì´ë¯¸ schema ë³€ìˆ˜ì— ì ìš©ë¨
            schema_name = schema if schema else self._apply_name_case('public', name_case)
            schema_merge = {
                "db": common["db"],
                "name": schema_name,  # ëŒ€ì†Œë¬¸ì ë³€í™˜ì´ ì´ë¯¸ ì ìš©ë¨
            }
            schema_merge_str = ", ".join(f"`{k}`: '{v}'" for k, v in schema_merge.items())
            queries.append(f"MERGE (s:Schema {{{schema_merge_str}}}) RETURN s")
            
            # Table ë…¸ë“œ ìƒì„± ë° Schemaì— BELONGS_TO ê´€ê³„ ì—°ê²°
            queries.append(f"MERGE (t:Table {{{merge_str}}}) SET {set_str} RETURN t")
            queries.append(
                f"MATCH (t:Table {{{merge_str}}})\n"
                f"MATCH (s:Schema {{{schema_merge_str}}})\n"
                f"MERGE (t)-[r:BELONGS_TO]->(s) RETURN t, r, s"
            )
            ddl_stats["tables"] += 1
            
            # DDL ë©”íƒ€ë°ì´í„° ìºì‹œ ì €ì¥ (ë©”ëª¨ë¦¬)
            # í‚¤ëŠ” ì†Œë¬¸ìë¡œ ì €ì¥í•˜ì—¬ ëŒ€ì†Œë¬¸ì ë¬´ê´€í•˜ê²Œ ì¡°íšŒ ê°€ëŠ¥
            # ì›ë³¸ ëŒ€ì†Œë¬¸ìë„ í•¨ê»˜ ì €ì¥í•˜ì—¬ SP ë¶„ì„ì—ì„œ DDLê³¼ ë™ì¼í•œ ëŒ€ì†Œë¬¸ì ì‚¬ìš©
            table_key = (schema.lower(), parsed_name.lower())
            self._ddl_table_metadata[table_key] = {
                "description": comment,
                "columns": column_metadata,
                "original_schema": schema,  # DDLì—ì„œ ì‚¬ìš©í•œ ì›ë³¸ ìŠ¤í‚¤ë§ˆëª…
                "original_name": parsed_name,  # DDLì—ì„œ ì‚¬ìš©í•œ ì›ë³¸ í…Œì´ë¸”ëª…
            }

            # Column ë…¸ë“œ ìƒì„±
            for col in columns:
                col_name_raw = (col.get("name") or "").strip()
                if not col_name_raw:
                    continue
                
                # ëŒ€ì†Œë¬¸ì ë³€í™˜ ì ìš©
                col_name = self._apply_name_case(col_name_raw, name_case)
                
                col_type = (col.get("dtype") or col.get("type") or "").strip()
                col_nullable = col.get("nullable", True)
                col_comment = (col.get("comment") or "").strip()
                fqn = ".".join(filter(None, [schema, parsed_name, col_name])).lower()
                escaped_fqn = escape_for_cypher(fqn)

                col_merge = {"fqn": escaped_fqn}
                col_merge_str = ", ".join(f"`{k}`: '{v}'" for k, v in col_merge.items())
                col_set = {
                    "name": escape_for_cypher(col_name),
                    "dtype": escape_for_cypher(col_type),
                    "description": escape_for_cypher(col_comment),
                    "description_source": "ddl" if col_comment else "",  # DDLì—ì„œ ì¶”ì¶œëœ ì„¤ëª…
                    "nullable": "true" if col_nullable else "false",
                    "fqn": escaped_fqn,
                }
                if col_name_raw.upper() in primary_keys:  # PK ì²´í¬ëŠ” ì›ë³¸ ëŒ€ë¬¸ìë¡œ
                    col_set["pk_constraint"] = f"{parsed_name}_pkey"
                
                col_set_str = ", ".join(f"c.`{k}` = '{v}'" for k, v in col_set.items())
                queries.append(f"MERGE (c:Column {{{col_merge_str}}}) SET {col_set_str} RETURN c")
                queries.append(
                    f"MATCH (t:Table {{{merge_str}}})\n"
                    f"MATCH (c:Column {{{col_merge_str}}})\n"
                    f"MERGE (t)-[r:HAS_COLUMN]->(c) RETURN t, r, c"
                )
                ddl_stats["columns"] += 1

            # FK ê´€ê³„ ìƒì„± - ê° FK ë§¤í•‘ë§ˆë‹¤ ë³„ë„ì˜ FK_TO_TABLE ê´€ê³„ ìƒì„±
            # ì†ì„±: sourceColumn, targetColumn, type, source
            # source='ddl': DDLì—ì„œ ì¶”ì¶œ (ì‹¤ì„  í‘œì‹œ)
            for fk in foreign_keys:
                src_col_raw = (fk.get("column") or "").strip()
                ref = (fk.get("ref") or "").strip()
                if not src_col_raw or not ref or "." not in ref:
                    continue

                ref_table_part, ref_col_raw = ref.rsplit(".", 1)
                ref_schema_parsed, ref_table_raw, _ = parse_table_identifier(ref_table_part)
                ref_schema_final = self._apply_name_case(ref_schema_parsed or schema, name_case)
                ref_table = self._apply_name_case(ref_table_raw, name_case)
                
                # ì»¬ëŸ¼ëª…ì—ë„ ëŒ€ì†Œë¬¸ì ë³€í™˜ ì ìš©
                src_col = self._apply_name_case(src_col_raw, name_case)
                ref_col = self._apply_name_case(ref_col_raw, name_case)

                # ì°¸ì¡° í…Œì´ë¸” MERGE (ìŠ¤í‚¤ë§ˆ/ì´ë¦„ìœ¼ë¡œë§Œ ë§¤ì¹­)
                ref_table_merge = {
                    "db": common["db"],
                    "schema": ref_schema_final or "",
                    "name": ref_table or ""
                }
                ref_merge_str = ", ".join(f"`{k}`: '{v}'" for k, v in ref_table_merge.items())
                queries.append(f"MERGE (rt:Table {{{ref_merge_str}}}) RETURN rt")
                
                escaped_src_col = escape_for_cypher(src_col)
                escaped_tgt_col = escape_for_cypher(ref_col)
                
                queries.append(
                    f"MATCH (t:Table {{{merge_str}}})\n"
                    f"MATCH (rt:Table {{{ref_merge_str}}})\n"
                    f"MERGE (t)-[r:FK_TO_TABLE {{sourceColumn: '{escaped_src_col}', targetColumn: '{escaped_tgt_col}'}}]->(rt)\n"
                    f"ON CREATE SET r.type = 'many_to_one', r.source = 'ddl'\n"
                    f"RETURN t, r, rt"
                )
                ddl_stats["fks"] += 1

        async with self._cypher_lock:
            result = await client.run_graph_query(queries)
        
        log_process("ANALYZE", "DDL", f"DDL ì²˜ë¦¬ ì™„ë£Œ: {file_name} (T:{ddl_stats['tables']}, C:{ddl_stats['columns']}, FK:{ddl_stats['fks']})")
        return result, ddl_stats

    # =========================================================================
    # ìŠ¤í‚¤ë§ˆ ê²°ì •
    # =========================================================================

    def _resolve_default_schema(self, directory: str) -> str:
        """íŒŒì¼ ê²½ë¡œì—ì„œ ê¸°ë³¸ ìŠ¤í‚¤ë§ˆë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
        
        ìš°ì„ ìˆœìœ„:
        1. ê²½ë¡œì˜ í´ë”ëª… ì¤‘ DDL ìŠ¤í‚¤ë§ˆì™€ ì¼ì¹˜í•˜ëŠ” ê²ƒ (ê¹Šì€ í´ë” ìš°ì„ )
        2. ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ 'public'
        """
        if not directory or not self._ddl_schemas:
            return "public"
        
        # ê²½ë¡œë¥¼ í´ë” ëª©ë¡ìœ¼ë¡œ ë¶„ë¦¬ (ê¹Šì€ ìˆœì„œëŒ€ë¡œ)
        parts = directory.replace("\\", "/").split("/")
        parts = [p.lower() for p in parts if p]
        
        # ê¹Šì€ í´ë”ë¶€í„° ë§¤ì¹­ (ì—­ìˆœ ìˆœíšŒ)
        for folder in reversed(parts):
            if folder in self._ddl_schemas:
                return folder
        
        return "public"

    # =========================================================================
    # íŒŒì¼ ë¡œë“œ
    # =========================================================================

    async def _load_all_files(
        self,
        file_names: list[tuple[str, str]],
        orchestrator: Any,
    ) -> List[FileAnalysisContext]:
        """ëª¨ë“  íŒŒì¼ì˜ ASTì™€ ì†ŒìŠ¤ì½”ë“œë¥¼ ë³‘ë ¬ë¡œ ë¡œë“œí•©ë‹ˆë‹¤."""
        
        async def load_single(directory: str, file_name: str) -> FileAnalysisContext:
            src_path = os.path.join(orchestrator.dirs["src"], directory, file_name)
            base_name = os.path.splitext(file_name)[0]
            ast_path = os.path.join(orchestrator.dirs["analysis"], directory, f"{base_name}.json")

            async with aiofiles.open(ast_path, "r", encoding="utf-8") as ast_file, \
                       aiofiles.open(src_path, "r", encoding="utf-8") as src_file:
                ast_content, source_lines = await asyncio.gather(
                    ast_file.read(),
                    src_file.readlines(),
                )
                return FileAnalysisContext(
                    directory=directory,
                    file_name=file_name,
                    ast_data=json.loads(ast_content),
                    source_lines=source_lines,
                )

        tasks = [load_single(d, f) for d, f in file_names]
        return await asyncio.gather(*tasks)

    # =========================================================================
    # Phase 1: AST ê·¸ë˜í”„ ìƒì„±
    # =========================================================================

    async def _run_phase1(
        self,
        contexts: List[FileAnalysisContext],
        client: Neo4jClient,
        orchestrator: Any,
        stats: AnalysisStats,
    ) -> AsyncGenerator[bytes, None]:
        """Phase 1: ëª¨ë“  íŒŒì¼ì˜ AST ê·¸ë˜í”„ë¥¼ ë³‘ë ¬ë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
        
        completed = 0
        total = len(contexts)
        results_queue: asyncio.Queue = asyncio.Queue()

        async def process_file(ctx: FileAnalysisContext):
            async with self._file_semaphore:
                try:
                    # íŒŒì¼ ê²½ë¡œ ê¸°ë°˜ ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ ê²°ì •
                    default_schema = self._resolve_default_schema(ctx.directory)
                    
                    processor = DbmsAstProcessor(
                        antlr_data=ctx.ast_data,
                        file_content="".join(ctx.source_lines),
                        directory=ctx.directory,
                        file_name=ctx.file_name,
                        api_key=orchestrator.api_key,
                        locale=orchestrator.locale,
                        dbms=orchestrator.target,
                        last_line=len(ctx.source_lines),
                        default_schema=default_schema,
                        ddl_table_metadata=self._ddl_table_metadata,
                        name_case=getattr(orchestrator, 'name_case', 'original'),
                    )
                    ctx.processor = processor
                    
                    # ì •ì  ê·¸ë˜í”„ ìƒì„±
                    queries = processor.build_static_graph_queries()
                    
                    if queries:
                        async with self._cypher_lock:
                            graph = await client.run_graph_query(queries)
                        
                        node_count = len(graph.get("Nodes", []))
                        rel_count = len(graph.get("Relationships", []))
                        
                        ctx.status = FileStatus.PH1_OK
                        await results_queue.put({
                            "type": "success",
                            "file": ctx.file_name,
                            "graph": graph,
                            "node_count": node_count,
                            "rel_count": rel_count,
                        })
                    else:
                        ctx.status = FileStatus.PH1_OK
                        await results_queue.put({
                            "type": "success",
                            "file": ctx.file_name,
                            "graph": {"Nodes": [], "Relationships": []},
                            "node_count": 0,
                            "rel_count": 0,
                        })
                        
                except Exception as e:
                    log_process("ANALYZE", "ERROR", f"Phase 1 ì˜¤ë¥˜ ({ctx.file_name}): {e}", logging.ERROR, e)
                    ctx.status = FileStatus.PH1_FAIL
                    ctx.error_message = str(e)[:100]
                    await results_queue.put({
                        "type": "error",
                        "file": ctx.file_name,
                        "message": str(e),
                    })
                    raise  # ì¦‰ì‹œ ì¤‘ë‹¨ - ë¶€ë¶„ ì‹¤íŒ¨ í—ˆìš© ì•ˆí•¨

        # ëª¨ë“  íŒŒì¼ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘
        tasks = [asyncio.create_task(process_file(ctx)) for ctx in contexts]

        # ê²°ê³¼ ìˆ˜ì‹  ë° ìŠ¤íŠ¸ë¦¬ë°
        while completed < total:
            result = await asyncio.wait_for(results_queue.get(), timeout=300.0)
            completed += 1
            stats.files_completed = completed
            
            if result["type"] == "error":
                yield emit_message(f"   âŒ [{completed}/{total}] {result['file']}: {result['message'][:50]}")
                stats.mark_file_failed(result['file'], "Phase1 ì‹¤íŒ¨")
            else:
                stats.add_graph_result(result["graph"], is_static=True)
                
                graph = result["graph"]
                graph_msg = format_graph_result(graph)
                
                yield emit_message(f"   âœ“ [{completed}/{total}] {result['file']}")
                if graph_msg:
                    for line in graph_msg.split("\n")[:3]:
                        yield emit_message(f"      {line}")
                
                yield emit_data(
                    graph=graph,
                    line_number=0,
                    analysis_progress=int(completed / total * 50),
                    current_file=result["file"],
                )

        # ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
        await asyncio.gather(*tasks, return_exceptions=True)

    # =========================================================================
    # Phase 2: LLM ë¶„ì„
    # =========================================================================

    async def _run_phase2(
        self,
        contexts: List[FileAnalysisContext],
        client: Neo4jClient,
        orchestrator: Any,
        stats: AnalysisStats,
    ) -> AsyncGenerator[bytes, None]:
        """Phase 2: Phase1 ì„±ê³µ íŒŒì¼ì˜ LLM ë¶„ì„ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        
        if not contexts:
            yield emit_message("   â„¹ï¸ ë¶„ì„ ëŒ€ìƒ íŒŒì¼ ì—†ìŒ")
            return
        
        completed = 0
        total = len(contexts)
        results_queue: asyncio.Queue = asyncio.Queue()

        async def analyze_file(ctx: FileAnalysisContext):
            async with self._file_semaphore:
                try:
                    if not ctx.processor:
                        raise AnalysisError(f"Phase 1ì—ì„œ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì‹¤íŒ¨: {ctx.file_name}")
                    
                    # LLM ë¶„ì„ ì‹¤í–‰ (íŠœí”Œ ë°˜í™˜: queries, failed_batch_count, failed_details)
                    analysis_queries, failed_batch_count, failed_details = await ctx.processor.run_llm_analysis()
                    
                    if analysis_queries:
                        async with self._cypher_lock:
                            graph = await client.run_graph_query(analysis_queries)
                        
                        ctx.status = FileStatus.PH2_OK
                        await results_queue.put({
                            "type": "success",
                            "file": ctx.file_name,
                            "graph": graph,
                            "query_count": len(analysis_queries),
                            "failed_batches": failed_batch_count,
                            "failed_details": failed_details,  # ìƒì„¸ ì •ë³´ ì¶”ê°€
                        })
                    else:
                        ctx.status = FileStatus.PH2_OK
                        await results_queue.put({
                            "type": "success",
                            "file": ctx.file_name,
                            "graph": {"Nodes": [], "Relationships": []},
                            "query_count": 0,
                            "failed_batches": failed_batch_count,
                        })
                    
                    # ë°°ì¹˜ ì‹¤íŒ¨ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ì¤‘ë‹¨ - ë¶€ë¶„ ì‹¤íŒ¨ í—ˆìš© ì•ˆí•¨
                    if failed_batch_count > 0:
                        raise AnalysisError(f"{ctx.file_name}: {failed_batch_count}ê°œ ë°°ì¹˜ ì‹¤íŒ¨")
                        
                except Exception as e:
                    log_process("ANALYZE", "ERROR", f"Phase 2 ì˜¤ë¥˜ ({ctx.file_name}): {e}", logging.ERROR, e)
                    ctx.status = FileStatus.PH2_FAIL
                    ctx.error_message = str(e)[:100]
                    await results_queue.put({
                        "type": "error",
                        "file": ctx.file_name,
                        "message": str(e),
                    })
                    raise  # ì¦‰ì‹œ ì¤‘ë‹¨ - ë¶€ë¶„ ì‹¤íŒ¨ í—ˆìš© ì•ˆí•¨

        # ëª¨ë“  íŒŒì¼ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘
        tasks = [asyncio.create_task(analyze_file(ctx)) for ctx in contexts]

        # ê²°ê³¼ ìˆ˜ì‹  ë° ìŠ¤íŠ¸ë¦¬ë°
        while completed < total:
            result = await asyncio.wait_for(results_queue.get(), timeout=600.0)
            result_type = result.get("type", "")
            
            # warningì€ ì¹´ìš´íŠ¸í•˜ì§€ ì•ŠìŒ (ì¶”ê°€ ì •ë³´ì¼ ë¿)
            if result_type == "warning":
                yield emit_message(f"   âš ï¸ {result['file']}: {result['message']}")
                continue
            
            completed += 1
            
            if result_type == "error":
                yield emit_message(f"   âŒ [{completed}/{total}] {result['file']}: {result['message'][:50]}")
                stats.mark_file_failed(result['file'], "Phase2 ì‹¤íŒ¨")
            else:
                stats.llm_batches_executed += 1
                graph = result["graph"]
                stats.add_graph_result(graph, is_static=False)
                
                # ë°°ì¹˜ ì‹¤íŒ¨ ì •ë³´ í‘œì‹œ
                failed_batches = result.get("failed_batches", 0)
                failed_details = result.get("failed_details", [])
                fail_info = f" (ë°°ì¹˜ {failed_batches}ê°œ ì‹¤íŒ¨)" if failed_batches > 0 else ""
                
                graph_msg = format_graph_result(graph)
                yield emit_message(f"   âœ“ [{completed}/{total}] {result['file']} (ì¿¼ë¦¬ {result['query_count']}ê°œ){fail_info}")
                if graph_msg:
                    for line in graph_msg.split("\n")[:3]:
                        yield emit_message(f"      {line}")
                
                # ì‹¤íŒ¨ ìƒì„¸ ì •ë³´ ì¶œë ¥ (ìµœëŒ€ 3ê°œ)
                if failed_details:
                    stats.llm_batches_failed += len(failed_details)
                    for detail in failed_details[:3]:
                        yield emit_message(f"      âš ï¸ ë°°ì¹˜ #{detail['batch_id']} ({detail['node_ranges']}): {detail['error'][:50]}")
                
                yield emit_data(
                    graph=graph,
                    line_number=0,
                    analysis_progress=50 + int(completed / total * 50),
                    current_file=result["file"],
                )

        # ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
        await asyncio.gather(*tasks, return_exceptions=True)

    # =========================================================================
    # Phase 4: ë²¡í„°ë¼ì´ì§• (ì„ë² ë”© ìƒì„±)
    # =========================================================================
    
    async def _run_vectorize_phase(
        self,
        client: Neo4jClient,
        orchestrator: Any,
        stats: AnalysisStats,
    ) -> AsyncGenerator[bytes, None]:
        """Phase 4: í…Œì´ë¸”/ì»¬ëŸ¼ ë²¡í„°ë¼ì´ì§•
        
        Neo4jì— ì €ì¥ëœ í…Œì´ë¸”/ì»¬ëŸ¼ì˜ descriptionì„ ê¸°ë°˜ìœ¼ë¡œ ì„ë² ë”© ìƒì„±
        """
        from openai import AsyncOpenAI
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        api_key = orchestrator.api_key or settings.openai_api_key
        if not api_key:
            yield emit_message("   âš ï¸ OpenAI API í‚¤ê°€ ì—†ì–´ ë²¡í„°ë¼ì´ì§•ì„ ê±´ë„ˆëœë‹ˆë‹¤")
            return
        
        openai_client = AsyncOpenAI(api_key=api_key)
        embedding_client = EmbeddingClient(openai_client)
        
        # í…Œì´ë¸” ë²¡í„°ë¼ì´ì§•
        yield emit_message("   ğŸ“Š í…Œì´ë¸” ë²¡í„°ë¼ì´ì§• ì¤‘...")
        
        table_query = """
        MATCH (t:Table)
        WHERE (t.vector IS NULL OR size(t.vector) = 0)
          AND (t.description IS NOT NULL OR t.analyzed_description IS NOT NULL)
        RETURN elementId(t) AS tid, 
               t.name AS name,
               t.schema AS schema,
               coalesce(t.description, t.analyzed_description, '') AS description
        ORDER BY t.schema, t.name
        """
        
        try:
            async with self._cypher_lock:
                result = await client.execute_queries([table_query])
            
            tables = result[0] if result and result[0] else []
            
            for item in tables:
                description = item.get("description", "") or ""
                if not description:
                    continue
                
                text = embedding_client.format_table_text(
                    table_name=item.get("name", ""),
                    description=description
                )
                vector = await embedding_client.embed_text(text)
                
                if vector:
                    set_query = f"""
                    MATCH (t)
                    WHERE elementId(t) = '{item['tid']}'
                    SET t.vector = {vector}
                    """
                    async with self._cypher_lock:
                        await client.execute_queries([set_query])
                    stats.tables_vectorized += 1
            
            yield emit_message(f"   âœ… í…Œì´ë¸” {stats.tables_vectorized}ê°œ ë²¡í„°ë¼ì´ì§• ì™„ë£Œ")
            
        except Exception as e:
            yield emit_message(f"   âš ï¸ í…Œì´ë¸” ë²¡í„°ë¼ì´ì§• ì‹¤íŒ¨: {str(e)[:100]}")
        
        # ì»¬ëŸ¼ ë²¡í„°ë¼ì´ì§•
        yield emit_message("   ğŸ“Š ì»¬ëŸ¼ ë²¡í„°ë¼ì´ì§• ì¤‘...")
        
        column_query = """
        MATCH (t:Table)-[:HAS_COLUMN]->(c:Column)
        WHERE (c.vector IS NULL OR size(c.vector) = 0)
          AND c.description IS NOT NULL AND c.description <> ''
        RETURN elementId(c) AS cid,
               c.name AS column_name,
               t.name AS table_name,
               coalesce(c.dtype, '') AS dtype,
               c.description AS description
        ORDER BY t.schema, t.name, c.name
        """
        
        try:
            async with self._cypher_lock:
                result = await client.execute_queries([column_query])
            
            columns = result[0] if result and result[0] else []
            
            # ë°°ì¹˜ ì²˜ë¦¬
            batch_size = 50
            for i in range(0, len(columns), batch_size):
                batch = columns[i:i + batch_size]
                texts = []
                
                for item in batch:
                    text = embedding_client.format_column_text(
                        column_name=item.get("column_name", ""),
                        table_name=item.get("table_name", ""),
                        dtype=item.get("dtype", ""),
                        description=item.get("description", "")
                    )
                    texts.append(text)
                
                vectors = await embedding_client.embed_batch(texts)
                
                for item, vector in zip(batch, vectors):
                    if vector:
                        set_query = f"""
                        MATCH (c)
                        WHERE elementId(c) = '{item['cid']}'
                        SET c.vector = {vector}
                        """
                        async with self._cypher_lock:
                            await client.execute_queries([set_query])
                        stats.columns_vectorized += 1
                
                yield emit_message(f"   ... ì»¬ëŸ¼ {min(i + batch_size, len(columns))}/{len(columns)} ì²˜ë¦¬ ì¤‘")
            
            yield emit_message(f"   âœ… ì»¬ëŸ¼ {stats.columns_vectorized}ê°œ ë²¡í„°ë¼ì´ì§• ì™„ë£Œ")
            
        except Exception as e:
            yield emit_message(f"   âš ï¸ ì»¬ëŸ¼ ë²¡í„°ë¼ì´ì§• ì‹¤íŒ¨: {str(e)[:100]}")
