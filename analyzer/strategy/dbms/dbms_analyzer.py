"""DBMS ì½”ë“œ ë¶„ì„ ì „ëµ - PL/SQL, í”„ë¡œì‹œì €, í•¨ìˆ˜ ë“±

AST ê¸°ë°˜ PL/SQL ì½”ë“œ ë¶„ì„ â†’ Neo4j ê·¸ë˜í”„ ìƒì„±.

ë¶„ì„ íë¦„ (ì´ì¤‘ ë³‘ë ¬):
1. [Phase 0] DDL ì²˜ë¦¬ (í…Œì´ë¸”/ì»¬ëŸ¼ ìŠ¤í‚¤ë§ˆ)
2. [Phase 1] íŒŒì¼ë³„ ë³‘ë ¬(5ê°œ)ë¡œ AST ê·¸ë˜í”„ ìƒì„±
3. [Phase 2] íŒŒì¼ë³„ ë³‘ë ¬(5ê°œ) + ì²­í¬ë³„ ë³‘ë ¬ë¡œ LLM ë¶„ì„
4. [Phase 3] í”„ë¡œì‹œì € ìš”ì•½ ë° User Story ìƒì„±
"""

import asyncio
import json
import logging
import os
from typing import Any, AsyncGenerator

import aiofiles

from analyzer.neo4j_client import Neo4jClient
from analyzer.parallel_executor import AnalysisTask, ParallelExecutor, ChunkBatcher
from analyzer.strategy.base_analyzer import AnalyzerStrategy
from analyzer.strategy.dbms.ast_processor import DbmsAstProcessor
from config.settings import settings
from util.rule_loader import RuleLoader
from util.stream_utils import (
    emit_data,
    emit_error,
    emit_message,
    format_graph_result,
)
from util.utility_tool import (
    escape_for_cypher,
    parse_table_identifier,
    generate_user_story_document,
)


class DbmsAnalyzer(AnalyzerStrategy):
    """DBMS ì½”ë“œ ë¶„ì„ ì „ëµ
    
    í”„ë¡œì‹œì €/í•¨ìˆ˜ ë¶„ì„ìš© ê·¸ë˜í”„ êµ¬ì¶•:
    - PROCEDURE, FUNCTION, TRIGGER ë…¸ë“œ
    - Table, Column ë…¸ë“œ
    - FROM, WRITES, CALL ê´€ê³„
    - Variable ë…¸ë“œ
    """

    async def analyze(
        self,
        file_names: list[tuple[str, str]],
        orchestrator: Any,
        **kwargs,
    ) -> AsyncGenerator[bytes, None]:
        """íŒŒì¼ ëª©ë¡ì„ ë¶„ì„í•˜ì—¬ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤."""
        client = Neo4jClient()
        event_queue_from = asyncio.Queue()
        event_queue_to = asyncio.Queue()
        total_files = len(file_names)

        try:
            yield emit_message("ğŸš€ DBMS ì½”ë“œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤")
            yield emit_message(f"ğŸ“¦ í”„ë¡œì íŠ¸: {orchestrator.project_name}")
            yield emit_message(f"ğŸ“Š ë¶„ì„ ëŒ€ìƒ: {total_files}ê°œ SQL íŒŒì¼")
            
            await client.ensure_constraints()
            yield emit_message("ğŸ”Œ Neo4j ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì™„ë£Œ")

            # ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ í™•ì¸
            if await client.check_nodes_exist(orchestrator.user_id, file_names):
                yield emit_message("ğŸ”„ ì´ì „ ë¶„ì„ ê²°ê³¼ ë°œê²¬ â†’ ì¦ë¶„ ì—…ë°ì´íŠ¸ ëª¨ë“œ")
            else:
                yield emit_message("ğŸ†• ìƒˆë¡œìš´ ë¶„ì„ ì‹œì‘")

            # ========== DDL ì²˜ë¦¬ ==========
            ddl_files = self._list_ddl_files(orchestrator)
            if ddl_files:
                ddl_count = len(ddl_files)
                yield emit_message("")
                yield emit_message("â”" * 42)
                yield emit_message(f"ğŸ“‹ [1ë‹¨ê³„] í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ìˆ˜ì§‘ ({ddl_count}ê°œ DDL)")
                yield emit_message("â”" * 42)
                
                ddl_dir = orchestrator.dirs["ddl"]
                total_tables = 0
                total_columns = 0
                total_fks = 0
                
                for idx, ddl_file in enumerate(ddl_files, 1):
                    yield emit_message("")
                    yield emit_message(f"ğŸ“„ [{idx}/{ddl_count}] {ddl_file}")
                    
                    ddl_graph, stats = await self._process_ddl(
                        ddl_path=os.path.join(ddl_dir, ddl_file),
                        client=client,
                        file_name=ddl_file,
                        orchestrator=orchestrator,
                    )
                    
                    if stats["tables"]:
                        yield emit_message(f"   âœ“ Table ë…¸ë“œ: {stats['tables']}ê°œ")
                        total_tables += stats["tables"]
                    if stats["columns"]:
                        yield emit_message(f"   âœ“ Column ë…¸ë“œ: {stats['columns']}ê°œ")
                        total_columns += stats["columns"]
                    if stats["fks"]:
                        yield emit_message(f"   âœ“ FK ê´€ê³„: {stats['fks']}ê°œ")
                        total_fks += stats["fks"]
                    
                    if ddl_graph and (ddl_graph.get("Nodes") or ddl_graph.get("Relationships")):
                        yield emit_data(
                            graph=ddl_graph,
                            line_number=0,
                            analysis_progress=0,
                            current_file=f"DDL-{ddl_file}",
                        )
                
                yield emit_message("")
                yield emit_message("ğŸ“Š DDL ì²˜ë¦¬ ì™„ë£Œ:")
                yield emit_message(f"   â€¢ í…Œì´ë¸”: {total_tables}ê°œ")
                yield emit_message(f"   â€¢ ì»¬ëŸ¼: {total_columns}ê°œ")
                yield emit_message(f"   â€¢ FK: {total_fks}ê°œ")
            else:
                yield emit_message("â„¹ï¸ DDL íŒŒì¼ ì—†ìŒ â†’ ìŠ¤í‚¤ë§ˆ ì²˜ë¦¬ ê±´ë„ˆëœ€")

            # ========== ì†ŒìŠ¤ íŒŒì¼ ë¶„ì„ ==========
            yield emit_message("")
            yield emit_message("â”" * 42)
            yield emit_message(f"ğŸ” [2ë‹¨ê³„] í”„ë¡œì‹œì €/í•¨ìˆ˜ ë¶„ì„ ({total_files}ê°œ íŒŒì¼)")
            yield emit_message("â”" * 42)

            for file_idx, (directory, file_name) in enumerate(file_names, 1):
                yield emit_message("")
                yield emit_message(f"ğŸ“„ [{file_idx}/{total_files}] {file_name}")
                if directory:
                    yield emit_message(f"   ğŸ“ ë””ë ‰í† ë¦¬: {directory}")
                
                async for chunk in self._analyze_file(
                    directory, file_name, file_names, client,
                    event_queue_from, event_queue_to, orchestrator,
                ):
                    yield chunk

            # ========== User Story ìƒì„± ==========
            yield emit_message("")
            yield emit_message("â”" * 42)
            yield emit_message("ğŸ“ [3ë‹¨ê³„] User Story ë¬¸ì„œ ìƒì„±")
            yield emit_message("â”" * 42)
            
            user_story_doc = await self._create_user_story_doc(client, orchestrator)
            if user_story_doc:
                yield emit_data(
                    graph={"Nodes": [], "Relationships": []},
                    line_number=0,
                    analysis_progress=100,
                    current_file="user_stories.md",
                    user_story_document=user_story_doc,
                    event_type="user_story_document",
                )
                yield emit_message("   âœ“ User Story ë¬¸ì„œ ìƒì„± ì™„ë£Œ")
            else:
                yield emit_message("   â„¹ï¸ ì¶”ì¶œí•  User Story ì—†ìŒ")
            
            yield emit_message("")
            yield emit_message("â”" * 42)
            yield emit_message("âœ… ëª¨ë“  ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            yield emit_message("â”" * 42)
            
        finally:
            await client.close()

    def _get_rule_loader(self) -> RuleLoader:
        """DBMS ê·œì¹™ ë¡œë” ë°˜í™˜"""
        return RuleLoader(target_lang="dbms")

    def _list_ddl_files(self, orchestrator: Any) -> list[str]:
        """DDL íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        ddl_dir = orchestrator.dirs.get("ddl", "")
        if not ddl_dir:
            logging.debug("[ANALYZE] DDL ë””ë ‰í† ë¦¬ ì„¤ì • ì—†ìŒ - ê±´ë„ˆëœ€")
            return []
        if not os.path.isdir(ddl_dir):
            logging.debug("[ANALYZE] DDL ë””ë ‰í† ë¦¬ ì—†ìŒ: %s - ê±´ë„ˆëœ€", ddl_dir)
            return []
        try:
            files = sorted(
                f for f in os.listdir(ddl_dir)
                if os.path.isfile(os.path.join(ddl_dir, f))
            )
            logging.info("[ANALYZE] DDL íŒŒì¼ ë°œê²¬: %dê°œ", len(files))
            return files
        except OSError as e:
            logging.warning("[ANALYZE] DDL ë””ë ‰í† ë¦¬ ì½ê¸° ì‹¤íŒ¨: %s | error=%s", ddl_dir, e)
            return []

    async def _load_file_assets(
        self,
        orchestrator: Any,
        directory: str,
        file_name: str,
    ) -> tuple[dict, list[str]]:
        """ì†ŒìŠ¤ íŒŒì¼ê³¼ AST JSON ë¡œë“œ"""
        src_path = os.path.join(orchestrator.dirs["src"], directory, file_name)
        base_name = os.path.splitext(file_name)[0]
        ast_path = os.path.join(orchestrator.dirs["analysis"], directory, f"{base_name}.json")

        async with aiofiles.open(ast_path, "r", encoding="utf-8") as ast_file, \
                   aiofiles.open(src_path, "r", encoding="utf-8") as src_file:
            ast_data, source_lines = await asyncio.gather(
                ast_file.read(),
                src_file.readlines(),
            )
            return json.loads(ast_data), source_lines

    async def _process_ddl(
        self,
        ddl_path: str,
        client: Neo4jClient,
        file_name: str,
        orchestrator: Any,
    ) -> tuple[dict, dict]:
        """DDL íŒŒì¼ ì²˜ë¦¬ ë° í…Œì´ë¸”/ì»¬ëŸ¼ ë…¸ë“œ ìƒì„±"""
        stats = {"tables": 0, "columns": 0, "fks": 0}
        
        async with aiofiles.open(ddl_path, "r", encoding="utf-8") as f:
            ddl_content = await f.read()
        
        loader = self._get_rule_loader()
        parsed = loader.execute(
            "ddl",
            {"ddl_content": ddl_content, "locale": orchestrator.locale},
            orchestrator.api_key,
        )
        
        queries = []
        common = {
            "user_id": orchestrator.user_id,
            "db": orchestrator.target,
            "project_name": orchestrator.project_name,
        }

        for table_info in parsed.get("analysis", []):
            table = table_info.get("table", {})
            columns = table_info.get("columns", [])
            foreign_keys = table_info.get("foreignKeys", [])
            primary_keys = [
                str(pk).strip().upper()
                for pk in (table_info.get("primaryKeys") or [])
                if pk
            ]

            schema_raw = (table.get("schema") or "").strip()
            table_name = (table.get("name") or "").strip()
            comment = (table.get("comment") or "").strip()
            table_type = (table.get("table_type") or "BASE TABLE").strip().upper()
            
            qualified = f"{schema_raw}.{table_name}" if schema_raw else table_name
            parsed_schema, parsed_name, _ = parse_table_identifier(qualified)
            schema = parsed_schema or ""

            # Table ë…¸ë“œ ìƒì„±
            merge_key = {**common, "schema": schema, "name": parsed_name}
            merge_str = ", ".join(f"`{k}`: '{v}'" for k, v in merge_key.items())
            
            detail_lines = [f"ì„¤ëª…: {comment}" if comment else "ì„¤ëª…: ", "", "ì£¼ìš” ì»¬ëŸ¼:"]
            for col in columns:
                col_name = (col.get("name") or "").strip()
                if not col_name:
                    continue
                col_comment = (col.get("comment") or "").strip()
                detail_lines.append(f"   {col_name}: {col_comment}" if col_comment else f"   {col_name}: ")
            
            set_props = {
                **common,
                "description": escape_for_cypher(comment),
                "table_type": table_type,
                "detailDescription": escape_for_cypher("\n".join(detail_lines)),
            }
            set_str = ", ".join(f"t.`{k}` = '{v}'" for k, v in set_props.items())
            queries.append(f"MERGE (t:Table {{{merge_str}}}) SET {set_str} RETURN t")
            stats["tables"] += 1

            # Column ë…¸ë“œ ìƒì„±
            for col in columns:
                col_name = (col.get("name") or "").strip()
                if not col_name:
                    continue
                
                col_type = (col.get("dtype") or col.get("type") or "").strip()
                col_nullable = col.get("nullable", True)
                col_comment = (col.get("comment") or "").strip()
                fqn = ".".join(filter(None, [schema, parsed_name, col_name])).lower()

                col_merge = {"user_id": orchestrator.user_id, "fqn": fqn, "project_name": orchestrator.project_name}
                col_merge_str = ", ".join(f"`{k}`: '{v}'" for k, v in col_merge.items())
                col_set = {
                    "name": escape_for_cypher(col_name),
                    "dtype": escape_for_cypher(col_type),
                    "description": escape_for_cypher(col_comment),
                    "nullable": "true" if col_nullable else "false",
                    "project_name": orchestrator.project_name,
                    "fqn": fqn,
                }
                if col_name.upper() in primary_keys:
                    col_set["pk_constraint"] = f"{parsed_name}_pkey"
                
                col_set_str = ", ".join(f"c.`{k}` = '{v}'" for k, v in col_set.items())
                queries.append(f"MERGE (c:Column {{{col_merge_str}}}) SET {col_set_str} RETURN c")
                queries.append(
                    f"MATCH (t:Table {{{merge_str}}})\n"
                    f"MATCH (c:Column {{{col_merge_str}}})\n"
                    f"MERGE (t)-[r:HAS_COLUMN]->(c) RETURN t, r, c"
                )
                stats["columns"] += 1

            # FK ê´€ê³„ ìƒì„±
            for fk in foreign_keys:
                src_col = (fk.get("column") or "").strip()
                ref = (fk.get("ref") or "").strip()
                if not src_col or not ref or "." not in ref:
                    continue

                ref_table_part, ref_col = ref.rsplit(".", 1)
                ref_schema, ref_table, _ = parse_table_identifier(ref_table_part)
                ref_schema = ref_schema or schema

                ref_table_merge = {**common, "schema": ref_schema or "", "name": ref_table or ""}
                ref_merge_str = ", ".join(f"`{k}`: '{v}'" for k, v in ref_table_merge.items())
                queries.append(f"MERGE (rt:Table {{{ref_merge_str}}}) RETURN rt")
                queries.append(
                    f"MATCH (t:Table {{{merge_str}}})\n"
                    f"MATCH (rt:Table {{{ref_merge_str}}})\n"
                    f"MERGE (t)-[r:FK_TO_TABLE]->(rt) RETURN t, r, rt"
                )
                stats["fks"] += 1

        result = await client.run_graph_query(queries)
        logging.info("DDL ì²˜ë¦¬ ì™„ë£Œ: %s (T:%d, C:%d, FK:%d)", 
                    file_name, stats["tables"], stats["columns"], stats["fks"])
        return result, stats

    async def _analyze_file(
        self,
        directory: str,
        file_name: str,
        all_files: list[tuple[str, str]],
        client: Neo4jClient,
        event_queue_from: asyncio.Queue,
        event_queue_to: asyncio.Queue,
        orchestrator: Any,
    ) -> AsyncGenerator[bytes, None]:
        """ë‹¨ì¼ íŒŒì¼ ë¶„ì„"""
        current_file = f"{directory}/{file_name}" if directory else file_name

        ast_data, source_lines = await self._load_file_assets(
            orchestrator, directory, file_name
        )
        last_line = len(source_lines)
        source_raw = "".join(source_lines)

        analyzer = DbmsAstProcessor(
            antlr_data=ast_data,
            file_content=source_raw,
            send_queue=event_queue_from,
            receive_queue=event_queue_to,
            last_line=last_line,
            directory=directory,
            file_name=file_name,
            user_id=orchestrator.user_id,
            api_key=orchestrator.api_key,
            locale=orchestrator.locale,
            dbms=orchestrator.target,
            project_name=orchestrator.project_name,
        )
        analysis_task = asyncio.create_task(analyzer.run())

        analyzed_blocks = 0
        static_blocks = 0
        total_llm_batches = 0
        total_nodes = 0
        total_rels = 0

        while True:
            event = await event_queue_from.get()
            event_type = event.get("type")

            if event_type == "end_analysis":
                yield emit_message(f"   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
                yield emit_message(f"   ğŸ“Š íŒŒì¼ ë¶„ì„ ì™„ë£Œ: {file_name}")
                yield emit_message(f"      â€¢ ì •ì  ë¸”ë¡: {static_blocks}ê°œ")
                yield emit_message(f"      â€¢ AI ë¶„ì„ ë¸”ë¡: {analyzed_blocks}ê°œ")
                yield emit_message(f"      â€¢ ìƒì„±ëœ ë…¸ë“œ: {total_nodes}ê°œ")
                yield emit_message(f"      â€¢ ìƒì„±ëœ ê´€ê³„: {total_rels}ê°œ")
                
                yield emit_data(
                    graph={"Nodes": [], "Relationships": []},
                    line_number=last_line,
                    analysis_progress=100,
                    current_file=current_file,
                )
                break

            if event_type == "error":
                error_msg = event.get("message", f"ë¶„ì„ ì‹¤íŒ¨: {file_name}")
                logging.error("ë¶„ì„ ì‹¤íŒ¨: %s - %s", file_name, error_msg)
                yield emit_message(f"   âŒ ì˜¤ë¥˜ ë°œìƒ: {error_msg}")
                yield emit_error(error_msg)
                return

            next_line = event.get("line_number", 0)
            progress = self.calc_progress(next_line, last_line)

            if event_type == "static_graph":
                static_blocks += 1
                queries = event.get("query_data", [])
                graph = await client.run_graph_query(queries)
                
                total_nodes += len(graph.get("Nodes", []))
                total_rels += len(graph.get("Relationships", []))
                
                if static_blocks == 1:
                    yield emit_message("   ğŸ—ï¸ [Phase 1] ì½”ë“œ êµ¬ì¡° ìƒì„± ì¤‘...")
                
                node_info = event.get("node_info", {})
                if node_info:
                    yield emit_message(
                        f"      â†’ {node_info.get('type', 'Unknown')} ë…¸ë“œ: "
                        f"{node_info.get('name', '')} (Line {node_info.get('start_line', 0)})"
                    )
                
                yield emit_data(
                    graph=graph,
                    line_number=next_line,
                    analysis_progress=progress,
                    current_file=current_file,
                )
                await event_queue_to.put({"type": "process_completed"})
                continue

            if event_type == "static_complete":
                yield emit_message(f"   âœ“ Phase 1 ì™„ë£Œ: êµ¬ì¡° ë…¸ë“œ {static_blocks}ê°œ ìƒì„±")
                await event_queue_to.put({"type": "process_completed"})
                continue

            if event_type == "llm_start":
                total_llm_batches = event.get("total_batches", 0)
                yield emit_message(f"   ğŸ¤– [Phase 2] AI ë¶„ì„ ì‹œì‘ ({total_llm_batches}ê°œ ë¸”ë¡)")
                await event_queue_to.put({"type": "process_completed"})
                continue

            if event_type == "analysis_code":
                analyzed_blocks += 1
                queries = event.get("query_data", [])
                graph = await client.run_graph_query(queries)
                
                total_nodes += len(graph.get("Nodes", []))
                total_rels += len(graph.get("Relationships", []))
                
                # ê²°ê³¼ ë©”ì‹œì§€í™”
                graph_msg = format_graph_result(graph)
                if graph_msg:
                    yield emit_message(f"      [{analyzed_blocks}/{total_llm_batches}] ë¶„ì„ ì™„ë£Œ")
                    for line in graph_msg.split("\n"):
                        yield emit_message(f"      {line}")
                
                yield emit_data(
                    graph=graph,
                    line_number=next_line,
                    analysis_progress=progress,
                    current_file=current_file,
                )
                await event_queue_to.put({"type": "process_completed"})

        await analysis_task

    async def _create_user_story_doc(
        self,
        client: Neo4jClient,
        orchestrator: Any,
    ) -> str:
        """ë¶„ì„ëœ í”„ë¡œì‹œì €ì—ì„œ User Story ë¬¸ì„œ ìƒì„±"""
        try:
            query = f"""
                MATCH (n)
                WHERE (n:PROCEDURE OR n:FUNCTION OR n:TRIGGER)
                  AND n.user_id = '{escape_for_cypher(orchestrator.user_id)}'
                  AND n.project_name = '{escape_for_cypher(orchestrator.project_name)}'
                  AND n.summary IS NOT NULL
                OPTIONAL MATCH (n)-[:HAS_USER_STORY]->(us:UserStory)
                OPTIONAL MATCH (us)-[:HAS_AC]->(ac:AcceptanceCriteria)
                WITH n, 
                     collect(DISTINCT {{
                         id: us.id,
                         role: us.role,
                         goal: us.goal,
                         benefit: us.benefit,
                         acceptance_criteria: collect(DISTINCT {{
                             id: ac.id,
                             title: ac.title,
                             given: ac.given,
                             when: ac.when,
                             then: ac.then
                         }})
                     }}) AS user_stories
                RETURN n.procedure_name AS name, 
                       n.summary AS summary,
                       user_stories AS user_stories, 
                       labels(n)[0] AS type
                ORDER BY n.file_name, n.startLine
            """
            
            results = await client.execute_queries([query])
            
            if not results or not results[0]:
                logging.info("[ANALYZE] User Story ìƒì„± ëŒ€ìƒ ì—†ìŒ (ì¿¼ë¦¬ ê²°ê³¼ ì—†ìŒ)")
                return ""
            
            filtered = [
                r for r in results[0]
                if r.get("summary") or (r.get("user_stories") and len(r["user_stories"]) > 0)
            ]
            
            if not filtered:
                logging.info("[ANALYZE] User Story ìƒì„± ëŒ€ìƒ ì—†ìŒ (ìš”ì•½ ì—†ëŠ” í”„ë¡œì‹œì €ë§Œ ì¡´ì¬)")
                return ""
            
            logging.info("[ANALYZE] User Story ìƒì„± | ëŒ€ìƒ=%dê°œ í”„ë¡œì‹œì €", len(filtered))
            return generate_user_story_document(
                results=filtered,
                source_name=orchestrator.project_name,
                source_type="DBMS í”„ë¡œì‹œì €/í•¨ìˆ˜",
            )
            
        except Exception as exc:
            # User Story ìƒì„± ì‹¤íŒ¨ëŠ” ì „ì²´ ë¶„ì„ì„ ì¤‘ë‹¨í•˜ì§€ ì•ŠìŒ (ë¶€ë¶„ ì‹¤íŒ¨ í—ˆìš©)
            logging.error("[ANALYZE] User Story ë¬¸ì„œ ìƒì„± ì‹¤íŒ¨ | error=%s", exc, exc_info=True)
            return ""

