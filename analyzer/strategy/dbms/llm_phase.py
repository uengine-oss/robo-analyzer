"""LLM ë¶„ì„ Phase (Phase 2) - DBMS

dbms_analyzer.pyì—ì„œ ë¶„ë¦¬ëœ Phase 2 ë¡œì§ì…ë‹ˆë‹¤.
ëª¨ë“  ë¡œì§ì€ 100% ë³´ì¡´ë˜ë©°, ìœ„ì¹˜ë§Œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.

í•µì‹¬ ë¡œì§(summary ì²­í¬ ë¶„ì„, User Story ìƒì„±, í…Œì´ë¸” ìš”ì•½)ì€
ast_processor.pyì˜ run_llm_analysis() ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë©ë‹ˆë‹¤.
"""

import asyncio
import logging
from typing import Any, AsyncGenerator, List

from analyzer.neo4j_client import Neo4jClient
from analyzer.strategy.base_analyzer import AnalysisStats
from analyzer.strategy.base.file_context import FileStatus, FileAnalysisContext
from util.stream_event import (
    emit_data,
    emit_message,
    emit_phase_event,
)
from util.text_utils import log_process


async def run_phase2(
    analyzer: Any,
    contexts: List[FileAnalysisContext],
    client: Neo4jClient,
    orchestrator: Any,
    stats: AnalysisStats,
) -> AsyncGenerator[bytes, None]:
    """Phase 2: Phase1 ì„±ê³µ íŒŒì¼ì˜ LLM ë¶„ì„ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.
    
    í•µì‹¬: ctx.processor.run_llm_analysis()ë¥¼ í˜¸ì¶œí•˜ì—¬ ëª¨ë“  LLM ë¶„ì„ ìˆ˜í–‰
    - summary ì²­í¬ ë¶„í• /í†µí•© (_process_unit_summaries)
    - User Story ìƒì„± (analyze_user_story)
    - í…Œì´ë¸” ìš”ì•½ (_finalize_table_summaries)
    
    ìœ„ ë¡œì§ë“¤ì€ ëª¨ë‘ ast_processor.pyì— ìˆìœ¼ë©°, ì´ í•¨ìˆ˜ì—ì„œëŠ” í˜¸ì¶œë§Œ ë‹´ë‹¹.
    
    Args:
        analyzer: DbmsAnalyzer ì¸ìŠ¤í„´ìŠ¤ (ê³µìœ  ìƒíƒœ ì ‘ê·¼ìš©)
        contexts: Phase 1 ì„±ê³µ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        client: Neo4j í´ë¼ì´ì–¸íŠ¸
        orchestrator: ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
        stats: ë¶„ì„ í†µê³„
    """
    if not contexts:
        yield emit_message("   â„¹ï¸ ë¶„ì„ ëŒ€ìƒ íŒŒì¼ ì—†ìŒ")
        return
    
    completed = 0
    total = len(contexts)
    results_queue: asyncio.Queue = asyncio.Queue()

    async def analyze_file(ctx: FileAnalysisContext):
        async with analyzer._file_semaphore:
            try:
                if not ctx.processor:
                    raise RuntimeError(f"Phase 1ì—ì„œ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì‹¤íŒ¨: {ctx.file_name}")
                
                # LLM ë¶„ì„ ì‹¤í–‰ (íŠœí”Œ ë°˜í™˜: queries, failed_batch_count, failed_details)
                # ì´ í˜¸ì¶œ ë‚´ì—ì„œ summary ì²­í¬ ë¶„ì„, User Story ìƒì„±, í…Œì´ë¸” ìš”ì•½ì´ ëª¨ë‘ ì²˜ë¦¬ë¨
                analysis_queries, failed_batch_count, failed_details = await ctx.processor.run_llm_analysis()
                
                if analysis_queries:
                    all_nodes = {}
                    all_relationships = {}
                    async with analyzer._cypher_lock:
                        async for batch_result in client.run_graph_query(analysis_queries):
                            for node in batch_result.get("Nodes", []):
                                all_nodes[node["Node ID"]] = node
                            for rel in batch_result.get("Relationships", []):
                                all_relationships[rel["Relationship ID"]] = rel
                            # ë°°ì¹˜ ì§„í–‰ë¥  ìŠ¤íŠ¸ë¦¬ë° (ê·¸ë˜í”„ ë°ì´í„° í¬í•¨)
                            await results_queue.put({
                                "type": "batch_progress",
                                "file": ctx.file_name,
                                "batch": batch_result.get("batch", 0),
                                "total_batches": batch_result.get("total_batches", 0),
                                "graph": {
                                    "Nodes": batch_result.get("Nodes", []),
                                    "Relationships": batch_result.get("Relationships", []),
                                },
                            })
                    
                    graph = {"Nodes": list(all_nodes.values()), "Relationships": list(all_relationships.values())}
                    ctx.status = FileStatus.PH2_OK
                    await results_queue.put({
                        "type": "success",
                        "file": ctx.file_name,
                        "graph": graph,
                        "query_count": len(analysis_queries),
                        "failed_batches": failed_batch_count,
                        "failed_details": failed_details,
                    })
                else:
                    ctx.status = FileStatus.PH2_OK
                    await results_queue.put({
                        "type": "success",
                        "file": ctx.file_name,
                        "graph": {"Nodes": [], "Relationships": []},
                        "query_count": 0,
                        "failed_batches": failed_batch_count,
                    })
                
                # ë°°ì¹˜ ì‹¤íŒ¨ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ì¤‘ë‹¨ - ë¶€ë¶„ ì‹¤íŒ¨ í—ˆìš© ì•ˆí•¨
                if failed_batch_count > 0:
                    raise RuntimeError(f"{ctx.file_name}: {failed_batch_count}ê°œ ë°°ì¹˜ ì‹¤íŒ¨")
                    
            except Exception as e:
                log_process("ANALYZE", "ERROR", f"Phase 2 ì˜¤ë¥˜ ({ctx.file_name}): {e}", logging.ERROR, e)
                ctx.status = FileStatus.PH2_FAIL
                ctx.error_message = str(e)[:100]
                await results_queue.put({
                    "type": "error",
                    "file": ctx.file_name,
                    "message": str(e),
                })
                raise  # ì¦‰ì‹œ ì¤‘ë‹¨ - ë¶€ë¶„ ì‹¤íŒ¨ í—ˆìš© ì•ˆí•¨

    # ëª¨ë“  íŒŒì¼ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘
    tasks = [asyncio.create_task(analyze_file(ctx)) for ctx in contexts]

    # ê²°ê³¼ ìˆ˜ì‹  ë° ìŠ¤íŠ¸ë¦¬ë°
    while completed < total:
        result = await asyncio.wait_for(results_queue.get(), timeout=600.0)
        result_type = result.get("type", "")
        
        # warningì€ ì¹´ìš´íŠ¸í•˜ì§€ ì•ŠìŒ (ì¶”ê°€ ì •ë³´ì¼ ë¿)
        if result_type == "warning":
            yield emit_message(f"   âš ï¸ {result['file']}: {result['message']}")
            continue
        
        # ë°°ì¹˜ ì§„í–‰ë¥ ì€ ì¹´ìš´íŠ¸í•˜ì§€ ì•ŠìŒ (ì¤‘ê°„ ì§„í–‰ ìƒíƒœ)
        if result_type == "batch_progress":
            batch = result.get("batch", 0)
            total_batches = result.get("total_batches", 0)
            graph = result.get("graph")
            yield emit_message(f"      ğŸ“¦ {result['file']}: ë°°ì¹˜ {batch}/{total_batches} ì €ì¥ ì™„ë£Œ")
            # ë°°ì¹˜ë³„ ê·¸ë˜í”„ ë°ì´í„° ì¦‰ì‹œ ì „ì†¡
            if graph:
                yield emit_data(graph=graph)
            continue
        
        completed += 1
        
        # Phase 2 ì§„í–‰ë¥  ê³„ì‚° (50-100% ë²”ìœ„ ì‚¬ìš©)
        phase2_progress = 50 + int(completed / total * 50)
        
        if result_type == "error":
            yield emit_message(f"   âŒ [{completed}/{total}] {result['file']}: {result['message'][:50]}")
            stats.mark_file_failed(result['file'], "Phase2 ì‹¤íŒ¨")
            yield emit_phase_event(
                phase_num=2,
                phase_name="AI ë¶„ì„",
                status="in_progress",
                progress=phase2_progress,
                details={"file": result['file'], "status": "failed", "completed": completed, "total": total}
            )
        else:
            stats.llm_batches_executed += 1
            graph = result["graph"]
            stats.add_graph_result(graph, is_static=False)
            
            # ë°°ì¹˜ ì‹¤íŒ¨ ì •ë³´ í‘œì‹œ
            failed_batches = result.get("failed_batches", 0)
            failed_details = result.get("failed_details", [])
            fail_info = f" (ë°°ì¹˜ {failed_batches}ê°œ ì‹¤íŒ¨)" if failed_batches > 0 else ""
            
            # ë¶„ì„ ê²°ê³¼ ìƒì„¸ ì§‘ê³„
            node_count = len(graph.get("Nodes", []))
            rel_count = len(graph.get("Relationships", []))
            
            # ì—…ë°ì´íŠ¸ëœ ë…¸ë“œ íƒ€ì…ë³„ ì§‘ê³„
            updated_types = {}
            for node in graph.get("Nodes", []):
                labels = node.get("Labels", [])
                for label in labels:
                    updated_types[label] = updated_types.get(label, 0) + 1
            
            yield emit_message(f"   âœ“ [{completed}/{total}] {result['file']} (ì¿¼ë¦¬ {result['query_count']}ê°œ){fail_info}")
            
            # LLM ë¶„ì„ ê²°ê³¼ ìƒì„¸ í‘œì‹œ
            if updated_types:
                # ì£¼ìš” ì—…ë°ì´íŠ¸ í‘œì‹œ
                summary_added = sum(1 for n in graph.get("Nodes", []) if n.get("Properties", {}).get("summary"))
                table_desc_added = sum(1 for n in graph.get("Nodes", []) 
                                       if "Table" in (n.get("Labels") or []) 
                                       and n.get("Properties", {}).get("analyzed_description"))
                
                detail_parts = []
                if summary_added:
                    detail_parts.append(f"ìš”ì•½ {summary_added}ê°œ ìƒì„±")
                if table_desc_added:
                    detail_parts.append(f"í…Œì´ë¸” ì„¤ëª… {table_desc_added}ê°œ ë³´ê°•")
                if rel_count:
                    detail_parts.append(f"ê´€ê³„ {rel_count}ê°œ ì—…ë°ì´íŠ¸")
                
                if detail_parts:
                    yield emit_message(f"      â†’ {', '.join(detail_parts)}")
            
            # ì‹¤íŒ¨ ìƒì„¸ ì •ë³´ ì¶œë ¥ (ìµœëŒ€ 3ê°œ)
            if failed_details:
                stats.llm_batches_failed += len(failed_details)
                for detail in failed_details[:3]:
                    yield emit_message(f"      âš ï¸ ë°°ì¹˜ #{detail['batch_id']} ({detail['node_ranges']}): {detail['error'][:50]}")
            
            yield emit_phase_event(
                phase_num=2,
                phase_name="AI ë¶„ì„",
                status="in_progress",
                progress=phase2_progress,
                details={
                    "file": result['file'],
                    "queries": result['query_count'],
                    "nodes_updated": node_count,
                    "relationships_updated": rel_count,
                    "completed": completed,
                    "total": total
                }
            )
            
            yield emit_data(
                graph=graph,
                line_number=0,
                analysis_progress=phase2_progress,
                current_file=result["file"],
            )

    # ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
    await asyncio.gather(*tasks, return_exceptions=True)

