"""ê³µí†µ AST í”„ë¡œì„¸ì„œ ë² ì´ìŠ¤ í´ë˜ìŠ¤

í…œí”Œë¦¿ ë©”ì„œë“œ íŒ¨í„´ìœ¼ë¡œ ê³µí†µ íŒŒì´í”„ë¼ì¸ì„ ì •ì˜í•©ë‹ˆë‹¤.

íŒŒì´í”„ë¼ì¸:
- Phase 1: build_static_graph_queries() - ì •ì  ê·¸ë˜í”„ ì¿¼ë¦¬ ìƒì„±
- Phase 1.5: _generate_parent_contexts() - ë¶€ëª¨ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
- Phase 2: run_llm_analysis() - LLM ë¶„ì„

ì „ëµë³„ êµ¬í˜„ í•„ìš”:
- _collect_nodes(): AST ìˆ˜ì§‘
- _run_preprocessing(): ì„ í–‰ ì²˜ë¦¬
- _invoke_llm(): LLM í˜¸ì¶œ
- _build_analysis_queries(): ë¶„ì„ ê²°ê³¼ ì¿¼ë¦¬ ë³€í™˜
- _process_unit_summaries(): ë‹¨ìœ„ ìš”ì•½ ì²˜ë¦¬
"""

from __future__ import annotations

import asyncio
import logging
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional, Tuple, Set

from config.settings import settings
from util.exception import AnalysisError
from util.utility_tool import escape_for_cypher, calculate_code_token, log_process
from analyzer.pipeline_control import pipeline_controller

from analyzer.strategy.base.statement_node import StatementNode
from analyzer.strategy.base.batch import AnalysisBatch, BatchPlanner

# ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¤ëŠ” ìƒìˆ˜
MAX_CONCURRENCY = settings.concurrency.max_concurrency


class BaseAstProcessor(ABC):
    """AST ì²˜ë¦¬ ë° LLM ë¶„ì„ ê³µí†µ íŒŒì´í”„ë¼ì¸
    
    2ë‹¨ê³„ ë¶„ì„ ì§€ì›:
    - Phase 1: build_static_graph_queries() - ì •ì  ê·¸ë˜í”„ ì¿¼ë¦¬ ìƒì„±
    - Phase 1.5: _generate_parent_contexts() - ë¶€ëª¨ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    - Phase 2: run_llm_analysis() - LLM ë¶„ì„ í›„ ì—…ë°ì´íŠ¸ ì¿¼ë¦¬ ìƒì„±
    
    ì „ëµë³„ë¡œ ì˜¤ë²„ë¼ì´ë“œí•´ì•¼ í•˜ëŠ” ë©”ì„œë“œ:
    - _collect_nodes(): AST ìˆ˜ì§‘
    - _build_static_node_queries(): ì •ì  ë…¸ë“œ ì¿¼ë¦¬ ìƒì„±
    - _build_relationship_queries(): ê´€ê³„ ì¿¼ë¦¬ ìƒì„±
    - _run_preprocessing(): ì„ í–‰ ì²˜ë¦¬
    - _invoke_llm(): LLM í˜¸ì¶œ
    - _build_analysis_queries(): ë¶„ì„ ê²°ê³¼ ì¿¼ë¦¬ ë³€í™˜
    - _process_unit_summaries(): ë‹¨ìœ„ ìš”ì•½ ì²˜ë¦¬
    - _extract_parent_context(): ë¶€ëª¨ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
    - _get_excluded_context_types(): ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì œì™¸ íƒ€ì…
    - _get_unit_info_dict(): ë‹¨ìœ„(í”„ë¡œì‹œì €/í´ë˜ìŠ¤) ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """

    def __init__(
        self,
        antlr_data: dict,
        file_content: str,
        directory: str,
        file_name: str,
        api_key: str,
        locale: str,
        last_line: int,
    ):
        """ê³µí†µ ì´ˆê¸°í™”"""
        self.antlr_data = antlr_data
        self.file_content = file_content
        self.last_line = last_line
        
        # Windows ê²½ë¡œ êµ¬ë¶„ì(\\)ë¥¼ /ë¡œ ë³€í™˜í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€
        normalized_dir = directory.replace('\\', '/') if directory else ''
        self.directory = normalized_dir
        self.file_name = file_name
        self.api_key = api_key
        self.locale = locale
        
        # full_directory: ë””ë ‰í† ë¦¬ + íŒŒì¼ëª… (Neo4j directory ì†ì„±ìœ¼ë¡œ ì‚¬ìš©)
        self.full_directory = f"{normalized_dir}/{file_name}" if normalized_dir else file_name

        self.node_base_props = (
            f"directory: '{escape_for_cypher(self.full_directory)}', file_name: '{file_name}'"
        )
        
        self.max_workers = MAX_CONCURRENCY
        self.file_last_line = last_line
        
        # AST ìˆ˜ì§‘ ê²°ê³¼ ìºì‹œ (Phase 1ì—ì„œ ìˆ˜ì§‘, Phase 2ì—ì„œ ì‚¬ìš©)
        self._nodes: Optional[List[StatementNode]] = None
        self._unit_info: Optional[Dict[str, Any]] = None  # í”„ë¡œì‹œì €/í´ë˜ìŠ¤ ì •ë³´

    # =========================================================================
    # ì¶”ìƒ ë©”ì„œë“œ - ì „ëµë³„ êµ¬í˜„ í•„ìš”
    # =========================================================================
    
    @abstractmethod
    def _collect_nodes(self) -> Tuple[List[StatementNode], Dict[str, Any]]:
        """ASTë¥¼ ìˆ˜ì§‘í•˜ì—¬ ë…¸ë“œ ë¦¬ìŠ¤íŠ¸ì™€ ë‹¨ìœ„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            (ë…¸ë“œ ë¦¬ìŠ¤íŠ¸, ë‹¨ìœ„ ì •ë³´ ë”•ì…”ë„ˆë¦¬)
            - DBMS: (nodes, procedures)
            - Framework: (nodes, classes)
        """
        raise NotImplementedError

    @abstractmethod
    def _build_static_node_queries(self, node: StatementNode) -> List[str]:
        """ì •ì  ë…¸ë“œ ìƒì„± ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        raise NotImplementedError

    @abstractmethod
    def _build_relationship_queries(self) -> List[str]:
        """ì •ì  ê´€ê³„ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        raise NotImplementedError

    @abstractmethod
    async def _run_preprocessing(self) -> List[str]:
        """ì„ í–‰ ì²˜ë¦¬ (ë³€ìˆ˜/ìƒì†/í•„ë“œ/ë©”ì„œë“œ ë¶„ì„) í›„ ì¿¼ë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            ì„ í–‰ ì²˜ë¦¬ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        raise NotImplementedError

    @abstractmethod
    async def _invoke_llm(self, batch: AnalysisBatch) -> Any:
        """LLMì„ í˜¸ì¶œí•˜ì—¬ ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            LLM ë¶„ì„ ê²°ê³¼ (ì „ëµë³„ í˜•ì‹)
        """
        raise NotImplementedError

    @abstractmethod
    def _build_analysis_queries(
        self, 
        batch: AnalysisBatch, 
        llm_result: Any,
        unit_summary_store: Optional[Dict[str, Dict[str, str]]] = None,
    ) -> List[str]:
        """LLM ë¶„ì„ ê²°ê³¼ë¥¼ ì¿¼ë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Args:
            batch: ë¶„ì„ ë°°ì¹˜
            llm_result: LLM ë¶„ì„ ê²°ê³¼
            unit_summary_store: ë‹¨ìœ„ë³„ summary ì €ì¥ì†Œ (í”„ë¡œì‹œì €/í´ë˜ìŠ¤)
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ì—…ë°ì´íŠ¸ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        raise NotImplementedError

    @abstractmethod
    async def _process_unit_summaries(
        self, 
        unit_summary_store: Dict[str, Dict[str, str]]
    ) -> List[str]:
        """ë‹¨ìœ„(í”„ë¡œì‹œì €/í´ë˜ìŠ¤)ë³„ summaryë¥¼ ì²˜ë¦¬í•˜ì—¬ ì¿¼ë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            unit_summary_store: ë‹¨ìœ„ë³„ summary ì €ì¥ì†Œ
            
        Returns:
            ë‹¨ìœ„ ìš”ì•½ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        raise NotImplementedError

    @abstractmethod
    async def _extract_parent_context(
        self, 
        skeleton_code: str, 
        ancestor_context: str
    ) -> str:
        """ë¶€ëª¨ ë…¸ë“œì˜ ìŠ¤ì¼ˆë ˆí†¤ ì½”ë“œì—ì„œ í•µì‹¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            skeleton_code: ìì‹ êµ¬ê°„ì´ .... ë¡œ ì••ì¶•ëœ ë¶€ëª¨ ì½”ë“œ
            ancestor_context: ì¡°ìƒ ë…¸ë“œë“¤ì˜ ëˆ„ì  ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            í•µì‹¬ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
        """
        raise NotImplementedError

    @abstractmethod
    def _get_excluded_context_types(self) -> Set[str]:
        """ì»¨í…ìŠ¤íŠ¸ ìƒì„±ì—ì„œ ì œì™¸í•  ë…¸ë“œ íƒ€ì…ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            ì œì™¸í•  ë…¸ë“œ íƒ€ì… Set
            - DBMS: PROCEDURE_TYPES
            - Framework: CLASS_TYPES
        """
        raise NotImplementedError

    def _use_dml_ranges(self) -> bool:
        """ë°°ì¹˜ ê³„íš ì‹œ DML ë²”ìœ„ë¥¼ í¬í•¨í• ì§€ ì—¬ë¶€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        ê¸°ë³¸ê°’: False (DML ë¶„ì„ ë¶ˆí•„ìš”)
        DBMSì—ì„œë§Œ Trueë¡œ ì˜¤ë²„ë¼ì´ë“œí•©ë‹ˆë‹¤.
        
        Returns:
            True: DML ë¶„ì„ í•„ìš” (DBMS)
            False: DML ë¶„ì„ ë¶ˆí•„ìš” (ê¸°ë³¸ê°’)
        """
        return False

    # =========================================================================
    # Phase 1: ì •ì  ê·¸ë˜í”„ ì¿¼ë¦¬ ìƒì„±
    # =========================================================================
    
    def build_static_graph_queries(self) -> List[str]:
        """[Phase 1] ASTë¥¼ ìˆ˜ì§‘í•˜ê³  ì •ì  ê·¸ë˜í”„ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Returns:
            ì •ì  ë…¸ë“œ ë° ê´€ê³„ ìƒì„± ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        log_process("ANALYZE", "PHASE1", f"ğŸ—ï¸ {self.full_directory} ì •ì  ê·¸ë˜í”„ ìƒì„±")
        
        # AST ìˆ˜ì§‘
        self._nodes, self._unit_info = self._collect_nodes()
        
        if not self._nodes:
            log_process("ANALYZE", "PHASE1", f"âš ï¸ {self.full_directory}: ë¶„ì„ ëŒ€ìƒ ë…¸ë“œ ì—†ìŒ")
            return []
        
        # ì •ì  ë…¸ë“œ ì¿¼ë¦¬ ìƒì„±
        queries: List[str] = []
        for node in self._nodes:
            queries.extend(self._build_static_node_queries(node))
        
        # ê´€ê³„ ì¿¼ë¦¬ ìƒì„±
        queries.extend(self._build_relationship_queries())
        
        log_process("ANALYZE", "PHASE1", f"âœ… {self.full_directory}: {len(queries)}ê°œ ì¿¼ë¦¬ ìƒì„±")
        return queries

    # =========================================================================
    # Phase 1.5: ë¶€ëª¨ ì»¨í…ìŠ¤íŠ¸ ìƒì„± (Top-down)
    # =========================================================================
    
    async def _generate_parent_contexts(self) -> None:
        """ë¶€ëª¨ ë…¸ë“œë“¤ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ Top-down ìˆœì„œë¡œ ìƒì„±í•©ë‹ˆë‹¤.
        
        ì²˜ë¦¬ íë¦„:
        1. ë¶€ëª¨ ë…¸ë“œë“¤ì„ ê¹Šì´ ìˆœìœ¼ë¡œ ì •ë ¬ (ì–•ì€ ê²ƒ ë¨¼ì €)
        2. ê° ë¶€ëª¨ì— ëŒ€í•´ ìŠ¤ì¼ˆë ˆí†¤ + ì¡°ìƒ ì»¨í…ìŠ¤íŠ¸ â†’ LLM â†’ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
        3. ì¶”ì¶œëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë…¸ë“œì— ì €ì¥
        4. context_ready_event ì„¤ì •
        """
        if not self._nodes:
            return
        
        excluded_types = self._get_excluded_context_types()
        
        # ì»¨í…ìŠ¤íŠ¸ ìƒì„±ì´ í•„ìš”í•œ ë¶€ëª¨ ë…¸ë“œë“¤ ìˆ˜ì§‘
        parent_nodes = [
            node for node in self._nodes
            if node.needs_context_generation(excluded_types)
        ]
        
        if not parent_nodes:
            log_process("ANALYZE", "CONTEXT", "â­ï¸ ì»¨í…ìŠ¤íŠ¸ ìƒì„±ì´ í•„ìš”í•œ ë¶€ëª¨ ë…¸ë“œ ì—†ìŒ")
            # ëª¨ë“  ë…¸ë“œì˜ context_ready_event ì„¤ì •
            for node in self._nodes:
                node.context_ready_event.set()
            return
        
        # ê¹Šì´ ê³„ì‚° í•¨ìˆ˜
        def get_depth(node: StatementNode) -> int:
            depth = 0
            current = node.parent
            while current:
                depth += 1
                current = current.parent
            return depth
        
        # ê¹Šì´ ìˆœìœ¼ë¡œ ì •ë ¬ (ì–•ì€ ê²ƒ ë¨¼ì € â†’ Top-down ë³´ì¥)
        parent_nodes.sort(key=get_depth)
        
        log_process("ANALYZE", "CONTEXT", f"ğŸ”„ ë¶€ëª¨ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘: {len(parent_nodes)}ê°œ ë…¸ë“œ")
        
        # ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ (ê¹Šì´ ìˆœì„œ ë³´ì¥)
        # ê°™ì€ ê¹Šì´ì˜ ë…¸ë“œë“¤ì€ ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥
        current_depth = -1
        current_batch: List[StatementNode] = []
        
        async def process_context_batch(batch: List[StatementNode]) -> None:
            """ê°™ì€ ê¹Šì´ì˜ ë…¸ë“œë“¤ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬"""
            semaphore = asyncio.Semaphore(min(self.max_workers, len(batch)))
            
            async def process_one(node: StatementNode) -> None:
                async with semaphore:
                    try:
                        # ë¶€ëª¨ì˜ context_ready_event ëŒ€ê¸° (ìˆìœ¼ë©´)
                        if node.parent and node.parent.needs_context_generation(excluded_types):
                            await node.parent.context_ready_event.wait()
                        
                        # ìŠ¤ì¼ˆë ˆí†¤ ì½”ë“œ ìƒì„±
                        skeleton = node.get_skeleton_code()
                        
                        # ì¡°ìƒ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
                        ancestor_ctx = node.get_ancestor_context()
                        
                        # LLM í˜¸ì¶œí•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        context = await self._extract_parent_context(skeleton, ancestor_ctx)
                        
                        node.context = context
                        log_process("ANALYZE", "CONTEXT", f"âœ… ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ: {node.node_type}[{node.start_line}~{node.end_line}]")
                    except Exception as e:
                        log_process("ANALYZE", "CONTEXT", f"âŒ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨ (ì¹˜ëª…ì ): {node.node_type}[{node.start_line}]: {e}", logging.ERROR)
                        # ì»¨í…ìŠ¤íŠ¸ ì—†ì´ ë¶„ì„í•˜ë©´ ë³„ì¹­ í•´ì„ ì˜¤ë¥˜ ë“±ìœ¼ë¡œ ê²°ê³¼ê°€ ì—‰ë§ì´ ë¨
                        # ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œì„œ ì‹¤íŒ¨ë¥¼ ëª…í™•íˆ í‘œì‹œ
                        raise
                    finally:
                        node.context_ready_event.set()
            
            await asyncio.gather(*[process_one(n) for n in batch])
        
        # ê¹Šì´ë³„ë¡œ ë°°ì¹˜ ì²˜ë¦¬
        for node in parent_nodes:
            depth = get_depth(node)
            if depth != current_depth:
                # ì´ì „ ê¹Šì´ ë°°ì¹˜ ì²˜ë¦¬
                if current_batch:
                    await process_context_batch(current_batch)
                current_depth = depth
                current_batch = [node]
            else:
                current_batch.append(node)
        
        # ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬
        if current_batch:
            await process_context_batch(current_batch)
        
        # ì»¨í…ìŠ¤íŠ¸ ìƒì„± ë¶ˆí•„ìš”í•œ ë…¸ë“œë“¤ë„ context_ready_event ì„¤ì •
        for node in self._nodes:
            if not node.context_ready_event.is_set():
                node.context_ready_event.set()
        
        log_process("ANALYZE", "CONTEXT", f"âœ… ë¶€ëª¨ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ: {len(parent_nodes)}ê°œ")

    # =========================================================================
    # Phase 2: LLM ë¶„ì„
    # =========================================================================
    
    async def run_llm_analysis(self) -> Tuple[List[str], int, List[Dict[str, Any]]]:
        """[Phase 2] LLM ë¶„ì„ì„ ì‹¤í–‰í•˜ê³  ì—…ë°ì´íŠ¸ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        ì¤‘ìš”: ìì‹â†’ë¶€ëª¨ ìš”ì•½ ì˜ì¡´ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•´ completion_event ê¸°ë°˜ ëŒ€ê¸°
        - ë¶€ëª¨ ë…¸ë“œëŠ” ìì‹ ë…¸ë“œì˜ completion_eventë¥¼ ê¸°ë‹¤ë¦° í›„ ì‹¤í–‰
        - leaf ë…¸ë“œëŠ” ë°”ë¡œ ì‹¤í–‰, parent ë…¸ë“œëŠ” ìì‹ ì™„ë£Œ í›„ ì‹¤í–‰
        
        Returns:
            (ë¶„ì„ ê²°ê³¼ ì—…ë°ì´íŠ¸ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸, ì‹¤íŒ¨í•œ ë°°ì¹˜ ìˆ˜, ì‹¤íŒ¨ ìƒì„¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸)
        """
        if self._nodes is None:
            raise AnalysisError(f"Phase 1ì´ ë¨¼ì € ì‹¤í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤: {self.file_name}")
        
        log_process("ANALYZE", "PHASE2", f"ğŸ¤– {self.full_directory} LLM ë¶„ì„ ì‹œì‘")
        
        all_queries: List[str] = []
        failed_batch_count = 0
        all_failed_details: List[Dict[str, Any]] = []
        
        # ì„ í–‰ ì²˜ë¦¬
        preprocessing_queries = await self._run_preprocessing()
        all_queries.extend(preprocessing_queries)
        
        # Phase 1.5: ë¶€ëª¨ ì»¨í…ìŠ¤íŠ¸ ìƒì„± (Top-down)
        await self._generate_parent_contexts()
        
        # ë°°ì¹˜ ë¶„ì„
        planner = BatchPlanner()
        batches = planner.plan(self._nodes, include_dml_ranges=self._use_dml_ranges())
        
        if not batches:
            log_process("ANALYZE", "PHASE2", f"âš ï¸ {self.full_directory}: ë¶„ì„ ëŒ€ìƒ ë°°ì¹˜ ì—†ìŒ")
            return all_queries, 0, []
        
        log_process("ANALYZE", "PHASE2", f"ğŸ“Š ë°°ì¹˜ {len(batches)}ê°œ (completion_event ê¸°ë°˜ ì˜ì¡´ì„± ë³´ì¥)")
        
        # ë‹¨ìœ„ë³„ summary ìˆ˜ì§‘ìš© ì €ì¥ì†Œ (ë°°ì¹˜ ì²˜ë¦¬ ì „ì— ì´ˆê¸°í™”)
        unit_summary_store: Dict[str, Dict[str, str]] = {
            key: {} for key in (self._unit_info or {})
        }
        
        async def process_batch(batch: AnalysisBatch, semaphore: asyncio.Semaphore) -> Tuple[List[str], Dict[str, Any]]:
            """ë°°ì¹˜ ì²˜ë¦¬ í›„ ì¿¼ë¦¬ì™€ ë¶„ì„ ê²°ê³¼ ë°˜í™˜. ë…¸ë“œì— summaryë„ ì„¤ì •.
            
            í•µì‹¬: ë¶€ëª¨ ë…¸ë“œëŠ” ìì‹ completion_eventë¥¼ ê¸°ë‹¤ë¦° í›„ ì‹¤í–‰ë¨
            â†’ ê¹Šì´ ê³„ì‚° ì—†ì´ ìì—°ìŠ¤ëŸ½ê²Œ leaf â†’ parent ìˆœì„œ ë³´ì¥
            
            ì¤‘ìš”: 
            - try/finallyë¡œ completion_event.set()ì„ ë³´ì¥í•˜ì—¬ ë°ë“œë½ ë°©ì§€
            - ìì‹ ì¤‘ ok=Falseê°€ ìˆìœ¼ë©´ ë¶€ëª¨ë„ ok=False (ë¶ˆì™„ì „ ìš”ì•½ ì „íŒŒ)
            """
            async with semaphore:
                # ë°°ì¹˜ ì‹œì‘ ì „ ì¼ì‹œì •ì§€/ì¤‘ë‹¨ ì²´í¬
                if not await pipeline_controller.check_continue():
                    raise AnalysisError("íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨ë¨")
                
                try:
                    # 1. ë°°ì¹˜ ë‚´ ëª¨ë“  ë…¸ë“œì˜ ìì‹ ì™„ë£Œ ë° ë¶€ëª¨ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ë¥¼ ê¸°ë‹¤ë¦¼
                    for node in batch.nodes:
                        # ë¶€ëª¨ ì»¨í…ìŠ¤íŠ¸ê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°
                        if node.parent:
                            await node.parent.context_ready_event.wait()
                        
                        if node.has_children:
                            for child in node.children:
                                await child.completion_event.wait()
                                # ìì‹ ì¤‘ í•˜ë‚˜ë¼ë„ ì‹¤íŒ¨í•˜ë©´ ë¶€ëª¨ë„ ë¶ˆì™„ì „
                                if not child.ok:
                                    node.ok = False
                    
                    log_process("ANALYZE", "LLM", f"ë°°ì¹˜ #{batch.batch_id} ì²˜ë¦¬ ì¤‘ ({len(batch.nodes)}ê°œ ë…¸ë“œ)")
                    llm_result = await self._invoke_llm(batch)
                    
                    # 2. ë…¸ë“œì— summary ì„¤ì • (ì „ëµë³„ ê²°ê³¼ í˜•ì‹ì— ë”°ë¼)
                    self._apply_summary_to_nodes(batch, llm_result)
                    
                    queries = self._build_analysis_queries(batch, llm_result, unit_summary_store)
                    return queries, {"batch": batch, "result": llm_result}
                except Exception:
                    # ë°°ì¹˜ ì‹¤íŒ¨ ì‹œ ëª¨ë“  ë…¸ë“œë¥¼ ok=Falseë¡œ ë§ˆí‚¹
                    for node in batch.nodes:
                        node.ok = False
                    raise
                finally:
                    # 3. ë¬´ì¡°ê±´ completion_event ì„¤ì • (ì‹¤íŒ¨í•´ë„ ë¶€ëª¨ê°€ ëŒ€ê¸°í•˜ì§€ ì•Šë„ë¡)
                    for node in batch.nodes:
                        node.completion_event.set()
        
        def collect_results(batch_results: list, batches_list: List[AnalysisBatch], level_name: str) -> Tuple[int, List[Dict[str, Any]]]:
            """ë°°ì¹˜ ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•˜ê³  (ì‹¤íŒ¨ ìˆ˜, ì‹¤íŒ¨ ìƒì„¸ ì •ë³´) ë°˜í™˜."""
            nonlocal all_queries
            fail_count = 0
            failed_details: List[Dict[str, Any]] = []
            
            for i, result in enumerate(batch_results):
                if isinstance(result, Exception):
                    batch = batches_list[i] if i < len(batches_list) else None
                    batch_id = batch.batch_id if batch else i
                    node_ranges = ", ".join(f"L{n.start_line}-{n.end_line}" for n in batch.nodes) if batch else "unknown"
                    error_msg = str(result)[:100]  # ìµœëŒ€ 100ì
                    
                    log_process("ANALYZE", "ERROR", f"[{level_name}] ë°°ì¹˜ #{batch_id} ì‹¤íŒ¨ ({node_ranges}): {error_msg}", logging.ERROR)
                    fail_count += 1
                    failed_details.append({
                        "batch_id": batch_id,
                        "node_ranges": node_ranges,
                        "error": error_msg
                    })
                else:
                    queries, _ = result
                    all_queries.extend(queries)
            return fail_count, failed_details
        
        # ëª¨ë“  ë°°ì¹˜ ë³‘ë ¬ ì‹¤í–‰ (completion_eventê°€ ìˆœì„œ ë³´ì¥)
        semaphore = asyncio.Semaphore(min(self.max_workers, len(batches)))
        batch_results = await asyncio.gather(
            *[process_batch(b, semaphore) for b in batches],
            return_exceptions=True
        )
        fail_count, failed_details = collect_results(batch_results, batches, "LLM")
        failed_batch_count += fail_count
        all_failed_details.extend(failed_details)
        
        # ë‹¨ìœ„ë³„ summary ì²˜ë¦¬ (í”„ë¡œì‹œì €/í´ë˜ìŠ¤ê°€ ì—†ì–´ë„ í…Œì´ë¸” ì„¤ëª… ë³´ê°• ë“± í›„ì²˜ë¦¬ ìˆ˜í–‰)
        unit_queries = await self._process_unit_summaries(unit_summary_store)
        all_queries.extend(unit_queries)
        
        # ë°°ì¹˜ ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨ - ë¶€ë¶„ ì‹¤íŒ¨ í—ˆìš© ì•ˆí•¨
        if failed_batch_count > 0:
            raise AnalysisError(f"{self.full_directory}: {failed_batch_count}ê°œ ë°°ì¹˜ ì‹¤íŒ¨ (ìƒì„¸: {all_failed_details})")
        
        log_process("ANALYZE", "PHASE2", f"âœ… {self.full_directory}: {len(all_queries)}ê°œ ì—…ë°ì´íŠ¸ ì¿¼ë¦¬")
        return all_queries, failed_batch_count, all_failed_details

    def _apply_summary_to_nodes(self, batch: AnalysisBatch, llm_result: Any) -> None:
        """LLM ê²°ê³¼ì—ì„œ summaryë¥¼ ì¶”ì¶œí•˜ì—¬ ë…¸ë“œì— ì ìš©í•©ë‹ˆë‹¤.
        
        ê¸°ë³¸ êµ¬í˜„: llm_resultê°€ dictì´ê³  'analysis' ë°°ì—´ì´ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
        ì „ëµë³„ë¡œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥
        
        Raises:
            AnalysisError: llm_resultê°€ Noneì´ê±°ë‚˜ ì˜ˆìƒì¹˜ ëª»í•œ íƒ€ì…ì¼ ë•Œ
        """
        if not llm_result:
            raise AnalysisError(f"ë°°ì¹˜#{batch.batch_id} LLM ê²°ê³¼ ì—†ìŒ")
        
        # ì¼ë°˜ì ì¸ ê²½ìš°: llm_resultê°€ dictì´ê³  analysis ë°°ì—´ì´ ìˆìŒ
        if isinstance(llm_result, dict):
            analysis_list = llm_result.get("analysis") or []
            for node, analysis in zip(batch.nodes, analysis_list):
                if analysis:
                    node.summary = analysis.get("summary") or ""
        # DBMSì˜ ê²½ìš°: (general_result, table_result) íŠœí”Œ
        elif isinstance(llm_result, tuple) and len(llm_result) >= 1:
            general_result = llm_result[0]
            if general_result:
                analysis_list = general_result.get("analysis") or []
                for node, analysis in zip(batch.nodes, analysis_list):
                    if analysis:
                        node.summary = analysis.get("summary") or ""
        else:
            raise AnalysisError(f"ë°°ì¹˜#{batch.batch_id} ì•Œ ìˆ˜ ì—†ëŠ” ê²°ê³¼ íƒ€ì…: {type(llm_result).__name__}")

    # =========================================================================
    # ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ
    # =========================================================================
    
    @staticmethod
    def validate_dict_result(
        result: Any,
        context: str,
        batch_id: Optional[int] = None,
        allow_none: bool = False,
    ) -> Dict[str, Any]:
        """LLM ê²°ê³¼ê°€ dictì¸ì§€ ê²€ì¦í•˜ê³ , ì•„ë‹ˆë©´ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.
        
        Args:
            result: ê²€ì¦í•  ê²°ê³¼
            context: ë¡œê·¸ì— í‘œì‹œí•  ì»¨í…ìŠ¤íŠ¸ (ì˜ˆ: "ì²­í¬ ë¶„ì„", "User Story")
            batch_id: ë°°ì¹˜ ID (ìˆìœ¼ë©´ ë¡œê·¸ì— í¬í•¨)
            allow_none: Trueë©´ Noneì¼ ë•Œ ë¹ˆ dict ë°˜í™˜, Falseë©´ ì˜ˆì™¸ ë°œìƒ
            
        Returns:
            resultê°€ dictì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
            
        Raises:
            AnalysisError: resultê°€ dictê°€ ì•„ë‹ ë•Œ
        """
        if result is None:
            if allow_none:
                return {}
            batch_info = f"ë°°ì¹˜#{batch_id} " if batch_id else ""
            raise AnalysisError(f"{batch_info}{context} ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤")
        
        if isinstance(result, dict):
            return result
        
        batch_info = f"ë°°ì¹˜#{batch_id} " if batch_id else ""
        raise AnalysisError(f"{batch_info}{context} ê²°ê³¼ê°€ dictê°€ ì•„ë‹˜: {type(result).__name__}")
    
    def _split_summaries_by_token(self, summaries: dict, max_token: int) -> List[dict]:
        """í† í° ê¸°ì¤€ìœ¼ë¡œ summariesë¥¼ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
        if not summaries:
            return []
        
        chunks = []
        current_chunk = {}
        current_tokens = 0
        
        for key, value in summaries.items():
            item_text = f"{key}: {value}"
            item_tokens = calculate_code_token(item_text)
            
            if current_tokens + item_tokens > max_token and current_chunk:
                chunks.append(current_chunk)
                current_chunk = {}
                current_tokens = 0
            
            current_chunk[key] = value
            current_tokens += item_tokens
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks

    # =========================================================================
    # ê³µí†µ ê´€ê³„ ì¿¼ë¦¬ ë¹Œë”
    # =========================================================================

    def _build_contains_query(self, parent: "StatementNode", child: "StatementNode") -> str:
        """CONTAINS ê´€ê³„ ì¿¼ë¦¬ (ê³µí†µ)"""
        return (
            f"MATCH (__cy_p__:{parent.node_type} {{startLine: {parent.start_line}, {self.node_base_props}}})\n"
            f"MATCH (__cy_c__:{child.node_type} {{startLine: {child.start_line}, {self.node_base_props}}})\n"
            f"MERGE (__cy_p__)-[__cy_r__:CONTAINS]->(__cy_c__)\n"
            f"RETURN __cy_r__"
        )

    def _build_parent_of_query(self, parent: "StatementNode", child: "StatementNode") -> str:
        """PARENT_OF ê´€ê³„ ì¿¼ë¦¬ (ê³µí†µ)"""
        return (
            f"MATCH (__cy_p__:{parent.node_type} {{startLine: {parent.start_line}, {self.node_base_props}}})\n"
            f"MATCH (__cy_c__:{child.node_type} {{startLine: {child.start_line}, {self.node_base_props}}})\n"
            f"MERGE (__cy_p__)-[__cy_r__:PARENT_OF]->(__cy_c__)\n"
            f"RETURN __cy_r__"
        )

