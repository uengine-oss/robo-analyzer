"""Framework ì½”ë“œ ë¶„ì„ê¸° - Java/Kotlin AST â†’ Neo4j ê·¸ë˜í”„

í´ë˜ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨ ìƒì„±ì— í•„ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

ë¶„ì„ íŒŒì´í”„ë¼ì¸:
1. AST ìˆ˜ì§‘ (StatementCollector)
2. ì •ì  ê·¸ë˜í”„ ìƒì„± (CLASS, METHOD, FIELD ë…¸ë“œ)
3. ìƒì†/êµ¬í˜„ ê´€ê³„ ì¶”ì¶œ (EXTENDS, IMPLEMENTS)
4. LLM ë°°ì¹˜ ë¶„ì„ (ìš”ì•½, ë©”ì„œë“œ ì½œ ì¶”ì¶œ)
5. í´ë˜ìŠ¤ ìš”ì•½ ë° User Story ìƒì„±
"""

from __future__ import annotations

import asyncio
import json
import logging
import re
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Tuple

from config.settings import settings
from util.rule_loader import RuleLoader
from util.exception import LLMCallError, CodeProcessError, AnalysisError
from util.utility_tool import calculate_code_token, escape_for_cypher, log_process


# ==================== ìƒìˆ˜ ì •ì˜ ====================
# ë…¸ë“œ íƒ€ì… ë¶„ë¥˜
NON_ANALYSIS_TYPES = frozenset(["FILE", "PACKAGE", "IMPORT"])
CLASS_TYPES = frozenset(["CLASS", "INTERFACE", "ENUM"])
INHERITANCE_TYPES = frozenset(["EXTENDS", "IMPLEMENTS"])
FIELD_TYPES = frozenset(["FIELD"])
METHOD_TYPES = frozenset(["METHOD", "CONSTRUCTOR"])
METHOD_SIGNATURE_TYPES = frozenset(["METHOD_SIGNATURE"])

# ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¤ëŠ” ìƒìˆ˜
MAX_BATCH_TOKEN = settings.batch.framework_max_batch_token
MAX_CONCURRENCY = settings.concurrency.framework_max_concurrency
INHERITANCE_CONCURRENCY = settings.concurrency.inheritance_concurrency
FIELD_CONCURRENCY = settings.concurrency.field_concurrency
METHOD_CONCURRENCY = settings.concurrency.method_concurrency
STATIC_QUERY_BATCH_SIZE = settings.batch.static_query_batch_size
MAX_SUMMARY_CHUNK_TOKEN = settings.batch.max_summary_chunk_token
MAX_CONTEXT_TOKEN = settings.batch.max_context_token
PARENT_EXPAND_THRESHOLD = settings.batch.parent_expand_threshold

# Java í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ê¸°ë³¸ íƒ€ì… - í´ë˜ìŠ¤ ìƒì„± ì œì™¸ ëŒ€ìƒ
JAVA_BUILTIN_TYPES = frozenset([
    # ê¸°ë³¸ íƒ€ì… ë° ë˜í¼
    "int", "long", "double", "float", "boolean", "char", "byte", "short", "void",
    "Integer", "Long", "Double", "Float", "Boolean", "Character", "Byte", "Short",
    # ê¸°ë³¸ í´ë˜ìŠ¤
    "String", "Object", "Class", "Enum", "System", "Math", "Runtime",
    # ì»¬ë ‰ì…˜
    "List", "ArrayList", "LinkedList", "Set", "HashSet", "TreeSet", "LinkedHashSet",
    "Map", "HashMap", "TreeMap", "LinkedHashMap", "ConcurrentHashMap",
    "Collection", "Collections", "Arrays", "Iterator", "Iterable",
    "Queue", "Deque", "Stack", "Vector", "PriorityQueue",
    # ìœ í‹¸ë¦¬í‹°
    "Optional", "Stream", "Collectors", "Comparator", "Comparable",
    "Date", "Calendar", "LocalDate", "LocalTime", "LocalDateTime", "Instant",
    "UUID", "Random", "Scanner", "Pattern", "Matcher",
    # ì˜ˆì™¸
    "Exception", "RuntimeException", "Throwable", "Error",
    "IOException", "SQLException", "NullPointerException", "IllegalArgumentException",
    # I/O
    "File", "Path", "Files", "InputStream", "OutputStream", "Reader", "Writer",
    "BufferedReader", "BufferedWriter", "PrintWriter", "FileReader", "FileWriter",
    # ê¸°íƒ€
    "StringBuilder", "StringBuffer", "BigDecimal", "BigInteger",
    "Logger", "Log", "LogFactory",
])

# ìœ í‹¸ë¦¬í‹°/í—¬í¼ í´ë˜ìŠ¤ íŒ¨í„´ - CALLS ê´€ê³„ ìƒì„± ì œì™¸ ëŒ€ìƒ
# (í”„ë¡œì íŠ¸ì— ì¡´ì¬í•˜ë”ë¼ë„ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ê´€ì ì—ì„œ ì¤‘ìš”í•˜ì§€ ì•Šì€ í´ë˜ìŠ¤)
UTILITY_CLASS_PATTERNS = frozenset([
    "Debug", "Logger", "Log", "LogFactory", "LogManager",
    "Utils", "Utility", "Utilities", "Helper", "Helpers",
    "Constants", "Config", "Configuration", "Settings",
    "Validator", "Validation", "Formatter", "Converter",
    "StringUtils", "DateUtils", "NumberUtils", "CollectionUtils",
    "Assert", "Assertions", "Preconditions", "Check",
])


# ==================== ë°ì´í„° í´ë˜ìŠ¤ ====================
@dataclass(slots=True)
class StatementNode:
    """í‰íƒ„í™”ëœ AST ë…¸ë“œë¥¼ í‘œí˜„í•©ë‹ˆë‹¤.
    
    - ìˆ˜ì§‘ ë‹¨ê³„ì—ì„œ ëª¨ë“  ë…¸ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    - ì´í›„ ë°°ì¹˜ê°€ ë§Œë“¤ì–´ì§ˆ ë•Œ ì´ ê°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    - LLM ìš”ì•½ì´ ëë‚˜ë©´ `summary`ì™€ `completion_event`ê°€ ì±„ì›Œì§‘ë‹ˆë‹¤.
    - `ok` í”Œë˜ê·¸ë¡œ ì„±ê³µ ì—¬ë¶€ë¥¼ ì¶”ì í•©ë‹ˆë‹¤ (ìì‹ ì‹¤íŒ¨ ì‹œ ë¶€ëª¨ë„ False).
    - `context`ëŠ” ë¶€ëª¨ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    """
    node_id: int
    start_line: int
    end_line: int
    node_type: str
    code: str
    token: int
    has_children: bool
    analyzable: bool
    class_key: Optional[str]
    class_name: Optional[str]
    class_kind: Optional[str]
    lines: List[Tuple[int, str]] = field(default_factory=list)
    parent: Optional["StatementNode"] = None
    children: List["StatementNode"] = field(default_factory=list)
    summary: Optional[str] = None
    context: Optional[str] = None  # ë¶€ëª¨ ì»¨í…ìŠ¤íŠ¸ (ìì‹ ë¶„ì„ ì‹œ ì „ë‹¬ë¨)
    ok: bool = True  # LLM ë¶„ì„ ì„±ê³µ ì—¬ë¶€ (ìì‹ ì‹¤íŒ¨ ì‹œ ë¶€ëª¨ë„ False)
    completion_event: asyncio.Event = field(init=False, repr=False)
    context_ready_event: asyncio.Event = field(init=False, repr=False)

    def __post_init__(self):
        object.__setattr__(self, "completion_event", asyncio.Event())
        object.__setattr__(self, "context_ready_event", asyncio.Event())

    def get_raw_code(self) -> str:
        """ë¼ì¸ ë²ˆí˜¸ë¥¼ í¬í•¨í•˜ì—¬ ë…¸ë“œì˜ ì›ë¬¸ ì½”ë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return "\n".join(f"{ln}: {text}" for ln, text in self.lines)

    def get_compact_code(self) -> str:
        """ìì‹ êµ¬ê°„ì€ ìì‹ ìš”ì•½(ì—†ìœ¼ë©´ placeholder)ìœ¼ë¡œ ì¹˜í™˜í•œ ì½”ë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        DBMS ë°©ì‹ì²˜ëŸ¼ ë‹¨ìˆœ ìˆœíšŒë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        """
        if not self.children:
            return self.get_raw_code()

        result_lines: List[str] = []
        line_index = 0
        total_lines = len(self.lines)
        sorted_children = sorted(self.children, key=lambda child: child.start_line)

        for child in sorted_children:
            # ìì‹ ì´ì „ì˜ ë¶€ëª¨ ê³ ìœ  ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬í•©ë‹ˆë‹¤.
            while line_index < total_lines and self.lines[line_index][0] < child.start_line:
                line_no, text = self.lines[line_index]
                result_lines.append(f"{line_no}: {text}")
                line_index += 1

            # ìì‹ êµ¬ê°„ì€ ìì‹ ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤ (ì—†ìœ¼ë©´ placeholder).
            if child.summary:
                child_summary = child.summary.strip()
                summary_line = f"{child.start_line}~{child.end_line}: {child_summary}"
            else:
                summary_line = f"{child.start_line}: ...code..."

            result_lines.append(summary_line)

            # ìì‹ êµ¬ê°„ ì›ë³¸ ì½”ë“œëŠ” ê±´ë„ˆëœë‹ˆë‹¤.
            while line_index < total_lines and self.lines[line_index][0] <= child.end_line:
                line_index += 1

        # ë§ˆì§€ë§‰ ìì‹ ì´í›„ ë¶€ëª¨ ì½”ë“œê°€ ë‚¨ì•„ ìˆë‹¤ë©´ ì¶”ê°€í•©ë‹ˆë‹¤.
        while line_index < total_lines:
            line_no, text = self.lines[line_index]
            result_lines.append(f"{line_no}: {text}")
            line_index += 1

        return "\n".join(result_lines)

    def get_placeholder_code(self, include_assigns: bool = False) -> str:
        """ìì‹ êµ¬ê°„ì„ placeholder(...code...)ë¡œ ì¹˜í™˜í•œ ì½”ë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        ê¸°ë³¸ ë™ì‘:
        - ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜, ìƒì†, êµ¬í˜„ ê´€ê³„ëŠ” ì›ë¬¸ ìœ ì§€
        - ë‚˜ë¨¸ì§€ ëª¨ë“  ìì‹ì€ ...code...ë¡œ ì¹˜í™˜
        
        Args:
            include_assigns: Trueì´ë©´ ASSIGNMENT/NEW_INSTANCE ë…¸ë“œë¥¼ ì¬ê·€ì ìœ¼ë¡œ ì°¾ì•„ì„œ ì›ë¬¸ ìœ ì§€
                            (ifë¬¸, forë¬¸ ë“±ì€ ì œê±°ë˜ê³  ASSIGN/NEW_INSTANCEë§Œ ë‚¨ìŒ)
        """
        if not self.children:
            return self.get_raw_code()
        
        # í•­ìƒ ì›ë¬¸ ìœ ì§€í•  ë…¸ë“œ íƒ€ì…: ìƒì†/êµ¬í˜„ ê´€ê³„, ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜
        PRESERVE_TYPES = INHERITANCE_TYPES | METHOD_TYPES | METHOD_SIGNATURE_TYPES
        
        # include_assigns=Trueì´ë©´ ASSIGNMENT/NEW_INSTANCEë¥¼ ì¬ê·€ì ìœ¼ë¡œ ìˆ˜ì§‘
        assign_node_set: set[Tuple[int, int]] = set()
        if include_assigns:
            ASSIGN_TYPES = {"ASSIGNMENT", "NEW_INSTANCE"}
            
            def find_assign_nodes_recursive(node: "StatementNode") -> List["StatementNode"]:
                """ì¬ê·€ì ìœ¼ë¡œ ASSIGNMENT, NEW_INSTANCE ë…¸ë“œë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
                results = []
                for child in node.children:
                    if child.node_type in ASSIGN_TYPES:
                        results.append(child)
                    # ìì‹ì˜ ìì‹ë„ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰
                    results.extend(find_assign_nodes_recursive(child))
                return results
            
            assign_nodes = find_assign_nodes_recursive(self)
            assign_node_set = {(n.start_line, n.end_line) for n in assign_nodes}
        
        result_lines: List[str] = []
        line_index = 0
        total_lines = len(self.lines)
        sorted_children = sorted(self.children, key=lambda child: child.start_line)
        
        for child in sorted_children:
            # ìì‹ ì´ì „ì˜ ë¶€ëª¨ ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥
            while line_index < total_lines and self.lines[line_index][0] < child.start_line:
                line_no, text = self.lines[line_index]
                result_lines.append(f"{line_no}: {text}")
                line_index += 1
            
            # ì›ë¬¸ ìœ ì§€í•  ë…¸ë“œ: ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜, ìƒì†/êµ¬í˜„, ë˜ëŠ” ASSIGNMENT/NEW_INSTANCE
            child_span = (child.start_line, child.end_line)
            should_preserve = (
                child.node_type in PRESERVE_TYPES or 
                (include_assigns and child_span in assign_node_set)
            )
            
            if should_preserve:
                # ì›ë¬¸ ê·¸ëŒ€ë¡œ ì¶œë ¥
                while line_index < total_lines and self.lines[line_index][0] <= child.end_line:
                    line_no, text = self.lines[line_index]
                    result_lines.append(f"{line_no}: {text}")
                    line_index += 1
            else:
                # ë‚˜ë¨¸ì§€ ìì‹ì€ ...code...ë¡œ ì¹˜í™˜
                result_lines.append(f"{child.start_line}: ...code...")
                while line_index < total_lines and self.lines[line_index][0] <= child.end_line:
                    line_index += 1
        
        # ë§ˆì§€ë§‰ ìì‹ ì´í›„ ë¶€ëª¨ ì½”ë“œê°€ ë‚¨ì•„ ìˆë‹¤ë©´ ì¶”ê°€
        while line_index < total_lines:
            line_no, text = self.lines[line_index]
            result_lines.append(f"{line_no}: {text}")
            line_index += 1
        
        return "\n".join(result_lines)

    def get_code_with_assigns_only(self) -> str:
        """ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ + ASSIGNMENT/NEW_INSTANCE ìì‹ë§Œ í¬í•¨ëœ ì½”ë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        get_placeholder_code(include_assigns=True)ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
        """
        return self.get_placeholder_code(include_assigns=True)

    def get_skeleton_code(self) -> str:
        """ìì‹ êµ¬ê°„ì„ .... ë¡œ ì••ì¶•í•œ ìŠ¤ì¼ˆë ˆí†¤ ì½”ë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        ì—°ì†ëœ ìì‹ êµ¬ê°„ì€ í•˜ë‚˜ì˜ .... ë¡œ ì••ì¶•ë©ë‹ˆë‹¤.
        ë¶€ëª¨ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œìš©ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
        """
        if not self.children:
            return self.get_raw_code()

        result_lines: List[str] = []
        sorted_children = sorted(self.children, key=lambda child: child.start_line)
        in_child_block = False

        for line_no, text in self.lines:
            is_child_line = any(
                child.start_line <= line_no <= child.end_line
                for child in sorted_children
            )

            if is_child_line:
                if not in_child_block:
                    result_lines.append("    ....")
                    in_child_block = True
                # ì—°ì†ëœ ìì‹ ë¼ì¸ì€ ìŠ¤í‚µ
            else:
                in_child_block = False
                result_lines.append(f"{line_no}: {text}")

        return "\n".join(result_lines)

    def get_ancestor_context(self, max_tokens: int = MAX_CONTEXT_TOKEN) -> str:
        """ì¡°ìƒ ë…¸ë“œë“¤ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê²°í•©í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        ê°€ì¥ ê°€ê¹Œìš´ ì¡°ìƒë¶€í„° í† í° ìƒí•œê¹Œì§€ ëˆ„ì í•©ë‹ˆë‹¤.
        """
        if not self.parent:
            return ""

        context_parts: List[str] = []
        remaining = max_tokens
        current = self.parent

        while current and remaining > 0:
            # ë¶€ëª¨ì˜ contextê°€ ìˆìœ¼ë©´ ì‚¬ìš© (LLMì´ ìƒì„±í•œ í•µì‹¬ ì»¨í…ìŠ¤íŠ¸)
            if current.context:
                ctx_tokens = calculate_code_token(current.context)
                if ctx_tokens <= remaining:
                    context_parts.insert(0, current.context)
                    remaining -= ctx_tokens
                else:
                    # í† í° ì´ˆê³¼ ì‹œ ì¤‘ë‹¨
                    break
            current = current.parent

        if not context_parts:
            return ""

        return "[CONTEXT]\n" + "\n---\n".join(context_parts) + "\n[/CONTEXT]\n"

    def needs_context_generation(self) -> bool:
        """ì´ ë…¸ë“œê°€ ì»¨í…ìŠ¤íŠ¸ ìƒì„±ì´ í•„ìš”í•œ ë¶€ëª¨ ë…¸ë“œì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        
        ì¡°ê±´:
        - has_children = True (ìì‹ì´ ìˆìŒ)
        - analyzable = True (ë¶„ì„ ëŒ€ìƒ)
        - node_typeì´ CLASS_TYPESê°€ ì•„ë‹˜ (í´ë˜ìŠ¤ëŠ” ì œì™¸)
        """
        return (
            self.has_children
            and self.analyzable
            and self.node_type not in CLASS_TYPES
        )


@dataclass(slots=True)
class ClassInfo:
    """í´ë˜ìŠ¤/ì¸í„°í˜ì´ìŠ¤ ì •ë³´ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
    key: str
    name: str
    kind: str
    node_start: int
    node_end: int
    pending_nodes: int = 0
    finalized: bool = False


@dataclass(slots=True)
class AnalysisBatch:
    """ë¶„ì„ ë°°ì¹˜ ì •ë³´."""
    batch_id: int
    nodes: List[StatementNode]
    ranges: List[Dict[str, int]]
    progress_line: int

    def build_payload(self) -> Tuple[str, str]:
        """LLM í˜¸ì¶œìš© ì½”ë“œì™€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¦¬í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            (code, context) íŠœí”Œ - ì½”ë“œì™€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¦¬
        """
        code_parts: List[str] = []
        context_parts: List[str] = []
        
        for node in self.nodes:
            code = node.get_compact_code() if node.has_children else node.get_raw_code()
            code_parts.append(code)
            
            context = node.get_ancestor_context()
            if context:
                context_parts.append(context)
            else:
                context_parts.append("")
        
        return "\n\n".join(code_parts), "\n\n".join(context_parts)


@dataclass(slots=True)
class BatchResult:
    """ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ (calls ë°°ì—´ì€ general_resultì— í†µí•©ë¨)."""
    batch: AnalysisBatch
    general_result: Optional[Dict[str, Any]]


# ==================== í—¬í¼ í•¨ìˆ˜ ====================
def _is_valid_class_name_for_calls(name: str) -> bool:
    """calls ê´€ê³„ ìƒì„±ì— ìœ íš¨í•œ í´ë˜ìŠ¤ëª…ì¸ì§€ ê²€ì¦.
    
    ê°€ì§œ í´ë˜ìŠ¤ ìƒì„±ì„ ë°©ì§€í•˜ê¸° ìœ„í•´:
    - Java í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì œì™¸
    - ìœ í‹¸ë¦¬í‹°/í—¬í¼ í´ë˜ìŠ¤ ì œì™¸ (Debug, Logger, Utils ë“±)
    - ì†Œë¬¸ìë§Œìœ¼ë¡œ ëœ ì§§ì€ ì´ë¦„(ë³€ìˆ˜ëª…ìœ¼ë¡œ ë³´ì´ëŠ” ê²ƒ) ì œì™¸
    - í•œ ê¸€ì ì´ë¦„ ì œì™¸
    """
    if not name:
        return False
    
    # Java í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì œì™¸
    if name in JAVA_BUILTIN_TYPES:
        return False
    
    # ìœ í‹¸ë¦¬í‹°/í—¬í¼ í´ë˜ìŠ¤ ì œì™¸ (ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ê´€ì ì—ì„œ ì¤‘ìš”í•˜ì§€ ì•ŠìŒ)
    if name in UTILITY_CLASS_PATTERNS:
        return False
    
    # í•œ ê¸€ì ì´ë¦„ ì œì™¸ (i, j, k, o, e ë“± ë°˜ë³µ ë³€ìˆ˜)
    if len(name) == 1:
        return False
    
    # ì†Œë¬¸ìë¡œë§Œ ì‹œì‘í•˜ê³  3ê¸€ì ì´í•˜ì¸ ê²ƒ ì œì™¸ (ë³€ìˆ˜ëª…ìœ¼ë¡œ ë³´ì„)
    if name[0].islower() and len(name) <= 3:
        return False
    
    # ëª¨ë‘ ì†Œë¬¸ìì¸ ì§§ì€ ì´ë¦„ ì œì™¸ (item, items, list, map ë“±)
    if name.islower() and len(name) <= 6:
        return False
    
    return True


# ==================== RuleLoader í—¬í¼ ====================
def _rule_loader() -> RuleLoader:
    return RuleLoader(target_lang="framework")


def analyze_code(code: str, context: str, ranges: list, count: int, api_key: str, locale: str) -> Dict[str, Any]:
    """ì½”ë“œ ë²”ìœ„ë³„ ë¶„ì„ - summary, calls, variables ì¶”ì¶œ (ì»¨í…ìŠ¤íŠ¸ì™€ ì½”ë“œ ë¶„ë¦¬ ì „ë‹¬)."""
    inputs = {"code": code, "ranges": ranges, "count": count, "locale": locale}
    if context.strip():
        inputs["context"] = context
    return _rule_loader().execute(
        "analysis",
        inputs,
        api_key,
    )


def analyze_class_summary_only(summaries: dict, api_key: str, locale: str, previous_summary: str = "") -> Dict[str, Any]:
    """í´ë˜ìŠ¤ ì „ì²´ ìš”ì•½ ìƒì„± (Summaryë§Œ).
    
    Args:
        summaries: ë©¤ë²„ ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        api_key: LLM API í‚¤
        locale: ì¶œë ¥ ì–¸ì–´
        previous_summary: ì´ì „ ì²­í¬ì˜ ìš”ì•½ ê²°ê³¼ (ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ ì‹œ)
    """
    return _rule_loader().execute(
        "class_summary_only",
        {"summaries": summaries, "locale": locale, "previous_summary": previous_summary},
        api_key,
    )


def analyze_class_user_story(summary: str, api_key: str, locale: str) -> Dict[str, Any]:
    """í´ë˜ìŠ¤ User Story + AC ìƒì„±.
    
    Args:
        summary: í´ë˜ìŠ¤ì˜ ìƒì„¸ ìš”ì•½
        api_key: LLM API í‚¤
        locale: ì¶œë ¥ ì–¸ì–´
    """
    return _rule_loader().execute(
        "class_user_story",
        {"summary": summary, "locale": locale},
        api_key,
    )


def analyze_inheritance(declaration_code: str, api_key: str, locale: str) -> Dict[str, Any]:
    """ìƒì†/êµ¬í˜„ ê´€ê³„ ì¶”ì¶œ."""
    return _rule_loader().execute(
        "inheritance",
        {"declaration_code": declaration_code, "locale": locale},
        api_key,
    )


def analyze_field(declaration_code: str, api_key: str, locale: str) -> Dict[str, Any]:
    """í•„ë“œ ì •ë³´ ì¶”ì¶œ."""
    return _rule_loader().execute(
        "field",
        {"declaration_code": declaration_code, "locale": locale},
        api_key,
    )


def analyze_method(declaration_code: str, api_key: str, locale: str) -> Dict[str, Any]:
    """ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ë¶„ì„ - íŒŒë¼ë¯¸í„°/ë°˜í™˜ íƒ€ì… ì¶”ì¶œ."""
    return _rule_loader().execute(
        "method",
        {"declaration_code": declaration_code, "locale": locale},
        api_key,
    )


def analyze_parent_context(skeleton_code: str, ancestor_context: str, api_key: str, locale: str) -> Dict[str, Any]:
    """ë¶€ëª¨ ë…¸ë“œì˜ ìŠ¤ì¼ˆë ˆí†¤ ì½”ë“œì—ì„œ í•µì‹¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    return _rule_loader().execute(
        "parent_context",
        {"skeleton_code": skeleton_code, "ancestor_context": ancestor_context, "locale": locale},
        api_key,
    )
# ==================== ë…¸ë“œ ìˆ˜ì§‘ê¸° ====================
class StatementCollector:
    """ASTë¥¼ í›„ìœ„ìˆœíšŒí•˜ì—¬ StatementNodeì™€ í´ë˜ìŠ¤ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""

    def __init__(self, antlr_data: Dict[str, Any], file_content: str, directory: str, file_name: str):
        self.antlr_data = antlr_data
        self.file_content = file_content
        self.directory = directory
        self.file_name = file_name
        self.nodes: List[StatementNode] = []
        self.classes: Dict[str, ClassInfo] = {}
        self._node_id = 0
        self._file_lines = file_content.split("\n")

    def collect(self) -> Tuple[List[StatementNode], Dict[str, ClassInfo]]:
        """AST ì „ì—­ì„ í›„ìœ„ ìˆœíšŒí•˜ì—¬ ë…¸ë“œ ëª©ë¡ê³¼ í´ë˜ìŠ¤ ì •ë³´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        self._visit(self.antlr_data, None, None, None, None)
        return self.nodes, self.classes

    def _make_class_key(self, class_name: Optional[str], start_line: int) -> str:
        """í´ë˜ìŠ¤ ê³ ìœ í‚¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        base = class_name or f"anonymous_{start_line}"
        return f"{self.directory}:{self.file_name}:{base}:{start_line}"

    def _extract_class_name(self, code: str, node_type: str) -> Optional[str]:
        """ì½”ë“œì—ì„œ í´ë˜ìŠ¤/ì¸í„°í˜ì´ìŠ¤ ì´ë¦„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        patterns = {
            "CLASS": r"\bclass\s+(\w+)",
            "INTERFACE": r"\binterface\s+(\w+)",
            "ENUM": r"\benum\s+(\w+)",
            "RECORD": r"\brecord\s+(\w+)",
            "ANNOTATION_TYPE": r"@interface\s+(\w+)",
        }
        pattern = patterns.get(node_type)
        if pattern:
            match = re.search(pattern, code, re.IGNORECASE)
            if match:
                return match.group(1)
        return None

    def _visit(
        self,
        node: Dict[str, Any],
        parent: Optional[StatementNode],
        current_class: Optional[str],
        current_class_name: Optional[str],
        current_class_kind: Optional[str],
    ) -> Optional[StatementNode]:
        """ì¬ê·€ì ìœ¼ë¡œ ASTë¥¼ ë‚´ë ¤ê°€ë©° StatementNodeë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        node_type = node["type"]
        start_line = node["startLine"]
        end_line = node["endLine"]
        children = node.get("children", []) or []

        # ì½”ë“œ ì¶”ì¶œ
        line_entries = [
            (ln, self._file_lines[ln - 1] if 0 < ln <= len(self._file_lines) else "")
            for ln in range(start_line, end_line + 1)
        ]
        code = "\n".join(f"{ln}: {txt}" for ln, txt in line_entries)

        class_key = current_class
        class_name = current_class_name
        class_kind = current_class_kind

        # í´ë˜ìŠ¤/ì¸í„°í˜ì´ìŠ¤ ë…¸ë“œ ì²˜ë¦¬
        if node_type in CLASS_TYPES:
            extracted_name = self._extract_class_name(code, node_type)
            class_key = self._make_class_key(extracted_name, start_line)
            class_name = extracted_name
            class_kind = node_type
            if class_key not in self.classes:
                self.classes[class_key] = ClassInfo(
                    key=class_key,
                    name=extracted_name or class_key,
                    kind=node_type,
                    node_start=start_line,
                    node_end=end_line,
                )
                log_process("ANALYZE", "COLLECT", f"ğŸ“‹ í´ë˜ìŠ¤ ë°œê²¬: {extracted_name} ({node_type}, ë¼ì¸ {start_line}~{end_line})")

        # ìì‹ ë…¸ë“œ ìˆ˜ì§‘
        child_nodes: List[StatementNode] = []
        for ch in children:
            cn = self._visit(ch, None, class_key, class_name, class_kind)
            if cn:
                child_nodes.append(cn)

        # ë¶„ì„ ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨ (FIELDëŠ” ì„ í–‰ ì²˜ë¦¬ì—ì„œ ASSOCIATIONìœ¼ë¡œ ì²˜ë¦¬ë¨)
        analyzable = node_type not in NON_ANALYSIS_TYPES and node_type not in CLASS_TYPES and node_type not in FIELD_TYPES
        token = calculate_code_token(code)

        self._node_id += 1
        st = StatementNode(
            node_id=self._node_id,
            start_line=start_line,
            end_line=end_line,
            node_type=node_type,
            code=code,
            token=token,
            has_children=bool(child_nodes),
            analyzable=analyzable,
            class_key=class_key,
            class_name=class_name,
            class_kind=class_kind,
            lines=line_entries,
        )
        for c in child_nodes:
            c.parent = st
        st.children.extend(child_nodes)

        # ë¶„ì„ ëŒ€ìƒ ë…¸ë“œ ì¹´ìš´íŠ¸
        if analyzable and class_key and class_key in self.classes:
            self.classes[class_key].pending_nodes += 1

        if not analyzable and node_type not in CLASS_TYPES:
            st.completion_event.set()

        self.nodes.append(st)
        log_process(
            "ANALYZE",
            "COLLECT",
            f"âœ… {node_type} ë…¸ë“œ ìˆ˜ì§‘ ì™„ë£Œ: ë¼ì¸ {start_line}~{end_line}, í† í° {token}, ìì‹ {len(child_nodes)}ê°œ",
        )
        return st


# ==================== ë°°ì¹˜ í”Œë˜ë„ˆ ====================
class BatchPlanner:
    """ìˆ˜ì§‘ëœ ë…¸ë“œë¥¼ í† í° í•œë„ ë‚´ì—ì„œ ë°°ì¹˜ë¡œ ë¬¶ìŠµë‹ˆë‹¤."""

    def __init__(self, token_limit: int = MAX_BATCH_TOKEN):
        self.token_limit = token_limit

    def plan(self, nodes: List[StatementNode]) -> List[AnalysisBatch]:
        """í† í° í•œë„ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ë…¸ë“œë¥¼ ë¶„í• í•˜ì—¬ ë¶„ì„ ë°°ì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        batches: List[AnalysisBatch] = []
        current_nodes: List[StatementNode] = []
        current_tokens = 0
        batch_id = 1

        for node in nodes:
            if not node.analyzable:
                continue
            if node.has_children:
                if current_nodes:
                    batches.append(self._create_batch(batch_id, current_nodes))
                    log_process(
                        "ANALYZE",
                        "BATCH",
                        f"ğŸ“¦ ë°°ì¹˜ #{batch_id} í™•ì •: ë¦¬í”„ ë…¸ë“œ {len(current_nodes)}ê°œ (í† í° {current_tokens}/{self.token_limit})",
                    )
                    batch_id += 1
                    current_nodes = []
                    current_tokens = 0
                batches.append(self._create_batch(batch_id, [node]))
                log_process(
                    "ANALYZE",
                    "BATCH",
                    f"ğŸ“¦ ë°°ì¹˜ #{batch_id} í™•ì •: ë¶€ëª¨ ë…¸ë“œ ë‹¨ë… (ë¼ì¸ {node.start_line}~{node.end_line}, í† í° {node.token})",
                )
                batch_id += 1
                continue
            if current_nodes and current_tokens + node.token > self.token_limit:
                batches.append(self._create_batch(batch_id, current_nodes))
                log_process(
                    "ANALYZE",
                    "BATCH",
                    f"ğŸ“¦ ë°°ì¹˜ #{batch_id} í™•ì •: í† í° í•œë„ ë„ë‹¬ (ëˆ„ì  {current_tokens}/{self.token_limit})",
                )
                batch_id += 1
                current_nodes = []
                current_tokens = 0
            current_nodes.append(node)
            current_tokens += node.token

        if current_nodes:
            batches.append(self._create_batch(batch_id, current_nodes))
            log_process(
                "ANALYZE",
                "BATCH",
                f"ğŸ“¦ ë°°ì¹˜ #{batch_id} í™•ì •: ë§ˆì§€ë§‰ ë¦¬í”„ ë…¸ë“œ {len(current_nodes)}ê°œ (í† í° {current_tokens}/{self.token_limit})",
            )
        return batches

    def _create_batch(self, batch_id: int, nodes: List[StatementNode]) -> AnalysisBatch:
        """ë°°ì¹˜ IDì™€ ë…¸ë“œ ë¦¬ìŠ¤íŠ¸ë¡œ AnalysisBatch ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (DBMS ìŠ¤íƒ€ì¼ê³¼ ë™ì¼)."""
        ranges = [{"startLine": node.start_line, "endLine": node.end_line} for node in nodes]
        progress = max(node.end_line for node in nodes)
        return AnalysisBatch(
            batch_id=batch_id, 
            nodes=nodes, 
            ranges=ranges, 
            progress_line=progress
        )


# ==================== LLM í˜¸ì¶œ ====================
class LLMInvoker:
    """ë°°ì¹˜ë¥¼ LLMì— ì „ë‹¬í•˜ì—¬ ë¶„ì„ ê²°ê³¼ë¥¼ ì–»ìŠµë‹ˆë‹¤.
    
    calls ë°°ì—´ì€ analysis.yaml í”„ë¡¬í”„íŠ¸ì— í†µí•©ë˜ì–´ 
    ë¶„ì„ ê²°ê³¼ì˜ analysis[].calls í•„ë“œë¡œ ë°˜í™˜ë©ë‹ˆë‹¤.
    """

    def __init__(self, api_key: str, locale: str):
        self.api_key = api_key
        self.locale = locale

    async def invoke(self, batch: AnalysisBatch) -> Optional[Dict[str, Any]]:
        """ë°°ì¹˜ ì½”ë“œë¥¼ LLMì— ì „ë‹¬í•˜ì—¬ ë¶„ì„ ê²°ê³¼ë¥¼ ì–»ìŠµë‹ˆë‹¤.
        
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (analysis ë°°ì—´ í¬í•¨, ê° ìš”ì†Œì— calls ë°°ì—´ í¬í•¨)
        """
        if not batch.ranges:
            raise AnalysisError(f"ë°°ì¹˜ #{batch.batch_id}ì— ë¶„ì„í•  ë²”ìœ„ê°€ ì—†ìŠµë‹ˆë‹¤")

        code, context = batch.build_payload()
        result = await asyncio.to_thread(
            analyze_code,
            code,
            context,
            batch.ranges,
            len(batch.ranges),
            self.api_key,
            self.locale,
        )
        return result


# ==================== AST í”„ë¡œì„¸ì„œ ë³¸ì²´ ====================
class FrameworkAstProcessor:
    """Framework AST ì²˜ë¦¬ ë° LLM ë¶„ì„ íŒŒì´í”„ë¼ì¸
    
    2ë‹¨ê³„ ë¶„ì„ ì§€ì›:
    - Phase 1: build_static_graph_queries() - ì •ì  ê·¸ë˜í”„ ì¿¼ë¦¬ ìƒì„±
    - Phase 2: run_llm_analysis() - LLM ë¶„ì„ í›„ ì—…ë°ì´íŠ¸ ì¿¼ë¦¬ ìƒì„±
    """

    def __init__(
        self,
        antlr_data: dict,
        file_content: str,
        directory: str,
        file_name: str,
        user_id: str,
        api_key: str,
        locale: str,
        project_name: str,
        last_line: int,
    ):
        self.antlr_data = antlr_data
        self.file_content = file_content
        self.last_line = last_line
        self.directory = directory
        self.file_name = file_name
        self.user_id = user_id
        self.api_key = api_key
        self.locale = locale
        self.project_name = project_name
        self.max_workers = MAX_CONCURRENCY
        
        # full_directory: ë””ë ‰í† ë¦¬ + íŒŒì¼ëª… (Neo4j directory ì†ì„±ìœ¼ë¡œ ì‚¬ìš©)
        normalized_dir = directory.replace('\\', '/') if directory else ''
        self.full_directory = f"{normalized_dir}/{file_name}" if normalized_dir else file_name

        self.node_base_props = (
            f"directory: '{escape_for_cypher(self.full_directory)}', file_name: '{file_name}', "
            f"user_id: '{user_id}', project_name: '{project_name}'"
        )
        
        # AST ìˆ˜ì§‘ ê²°ê³¼ ìºì‹œ (Phase 1ì—ì„œ ìˆ˜ì§‘, Phase 2ì—ì„œ ì‚¬ìš©)
        self._nodes: Optional[List[StatementNode]] = None
        self._classes: Optional[Dict[str, ClassInfo]] = None
        self._field_type_cache: Optional[Dict[str, Dict[str, str]]] = None

    def build_static_graph_queries(self) -> List[str]:
        """[Phase 1] ASTë¥¼ ìˆ˜ì§‘í•˜ê³  ì •ì  ê·¸ë˜í”„ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Returns:
            ì •ì  ë…¸ë“œ ë° ê´€ê³„ ìƒì„± ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        log_process("ANALYZE", "PHASE1", f"ğŸ—ï¸ {self.full_directory} ì •ì  ê·¸ë˜í”„ ìƒì„±")
        
        # AST ìˆ˜ì§‘
        collector = StatementCollector(
            self.antlr_data, self.file_content, self.directory, self.file_name
        )
        self._nodes, self._classes = collector.collect()
        
        if not self._nodes:
            raise AnalysisError(f"ë¶„ì„ ëŒ€ìƒ ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤: {self.full_directory}")
        
        # í•„ë“œ íƒ€ì… ìºì‹œ ì´ˆê¸°í™”
        self._field_type_cache = {key: {} for key in self._classes} if self._classes else {}
        
        # ì •ì  ë…¸ë“œ ì¿¼ë¦¬ ìƒì„±
        queries: List[str] = []
        file_node = None
        for node in self._nodes:
            queries.extend(self._build_static_node_queries(node))
            if node.node_type == "FILE":
                file_node = node
        
        # Project â†’ File (CONTAINS) ê´€ê³„ ìƒì„±
        if file_node:
            queries.extend(self._build_project_file_relationship())
        
        # ê´€ê³„ ì¿¼ë¦¬ ìƒì„± (HAS_METHOD, HAS_FIELD, CONTAINS, PARENT_OF)
        queries.extend(self._build_relationship_queries())
        
        log_process("ANALYZE", "PHASE1", f"âœ… {self.full_directory}: {len(queries)}ê°œ ì¿¼ë¦¬ ìƒì„±")
        return queries

    async def _generate_parent_contexts(self) -> None:
        """ë¶€ëª¨ ë…¸ë“œë“¤ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ top-down ë°©ì‹ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
        
        ì»¨í…ìŠ¤íŠ¸ ìƒì„±ì´ í•„ìš”í•œ ë¶€ëª¨ ë…¸ë“œë“¤ì— ëŒ€í•´:
        1. ë¶€ëª¨ì˜ context_ready_eventë¥¼ ê¸°ë‹¤ë¦¼ (ì¡°ìƒ ì»¨í…ìŠ¤íŠ¸ í•„ìš”)
        2. ìŠ¤ì¼ˆë ˆí†¤ ì½”ë“œ + ì¡°ìƒ ì»¨í…ìŠ¤íŠ¸ë¡œ LLM í˜¸ì¶œ
        3. ê²°ê³¼ë¥¼ node.contextì— ì €ì¥
        4. context_ready_event ì„¤ì •
        """
        if not self._nodes:
            return

        # ì»¨í…ìŠ¤íŠ¸ ìƒì„±ì´ í•„ìš”í•œ ë…¸ë“œ í•„í„°ë§
        context_nodes = [n for n in self._nodes if n.needs_context_generation()]
        
        if not context_nodes:
            # ì»¨í…ìŠ¤íŠ¸ ìƒì„± í•„ìš” ì—†ìœ¼ë©´ ëª¨ë“  ë…¸ë“œì˜ context_ready_event ì„¤ì •
            for node in self._nodes:
                node.context_ready_event.set()
            return

        log_process("ANALYZE", "CONTEXT", f"ğŸ“ ë¶€ëª¨ ì»¨í…ìŠ¤íŠ¸ ìƒì„±: {len(context_nodes)}ê°œ ë…¸ë“œ")

        # ê¹Šì´ ìˆœìœ¼ë¡œ ì •ë ¬ (ì–•ì€ ë…¸ë“œ ë¨¼ì €)
        def get_depth(node: StatementNode) -> int:
            depth = 0
            current = node.parent
            while current:
                depth += 1
                current = current.parent
            return depth

        context_nodes.sort(key=get_depth)

        semaphore = asyncio.Semaphore(self.max_workers)

        async def generate_context(node: StatementNode) -> None:
            async with semaphore:
                try:
                    # ë¶€ëª¨ì˜ ì»¨í…ìŠ¤íŠ¸ê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°
                    if node.parent:
                        await node.parent.context_ready_event.wait()

                    # ìŠ¤ì¼ˆë ˆí†¤ ì½”ë“œ ìƒì„±
                    skeleton = node.get_skeleton_code()
                    
                    # ì¡°ìƒ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
                    ancestor_ctx = node.get_ancestor_context()

                    # LLM í˜¸ì¶œ (skeleton_codeì™€ ancestor_context ë¶„ë¦¬ ì „ë‹¬)
                    result = await asyncio.to_thread(
                        analyze_parent_context, skeleton, ancestor_ctx, self.api_key, self.locale
                    )

                    # ì»¨í…ìŠ¤íŠ¸ ì €ì¥
                    if isinstance(result, dict):
                        node.context = result.get("context_summary", "")
                    else:
                        # dictê°€ ì•„ë‹Œ ê²½ìš° ì˜ˆì™¸ ë°œìƒ (í˜¸ì¶œë¶€ì—ì„œ ë¡œê·¸ ë‚¨ê¹€)
                        raise ValueError(f"parent_context ê·œì¹™ì´ dictê°€ ì•„ë‹Œ ê°’ì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤: {type(result)}")

                except Exception as e:
                    log_process("ANALYZE", "CONTEXT", f"âŒ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨ (ì¹˜ëª…ì ): {node.node_type}[{node.start_line}]: {e}", logging.ERROR)
                    # ì»¨í…ìŠ¤íŠ¸ ì—†ì´ ë¶„ì„í•˜ë©´ ë³€ìˆ˜/ê°ì²´ í•´ì„ ì˜¤ë¥˜ ë“±ìœ¼ë¡œ ê²°ê³¼ê°€ ì—‰ë§ì´ ë¨
                    # ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œì„œ ì‹¤íŒ¨ë¥¼ ëª…í™•íˆ í‘œì‹œ
                    raise
                finally:
                    # í•­ìƒ context_ready_event ì„¤ì • (ìì‹ì´ ëŒ€ê¸°í•˜ì§€ ì•Šë„ë¡)
                    node.context_ready_event.set()

        # ì»¨í…ìŠ¤íŠ¸ ìƒì„±ì´ í•„ìš” ì—†ëŠ” ë…¸ë“œëŠ” ë°”ë¡œ event ì„¤ì •
        context_node_set = set(n.node_id for n in context_nodes)
        for node in self._nodes:
            if node.node_id not in context_node_set:
                node.context_ready_event.set()

        # ë³‘ë ¬ë¡œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        await asyncio.gather(*[generate_context(n) for n in context_nodes])

        log_process("ANALYZE", "CONTEXT", f"âœ… ë¶€ëª¨ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ")

    async def run_llm_analysis(self) -> Tuple[List[str], int, List[Dict[str, Any]]]:
        """[Phase 2] LLM ë¶„ì„ì„ ì‹¤í–‰í•˜ê³  ì—…ë°ì´íŠ¸ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        ì¤‘ìš”: ìì‹â†’ë¶€ëª¨ ìš”ì•½ ì˜ì¡´ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•´ completion_event ê¸°ë°˜ ëŒ€ê¸°
        - ë¶€ëª¨ ë…¸ë“œëŠ” ìì‹ ë…¸ë“œì˜ completion_eventë¥¼ ê¸°ë‹¤ë¦° í›„ ì‹¤í–‰
        - leaf ë…¸ë“œëŠ” ë°”ë¡œ ì‹¤í–‰, parent ë…¸ë“œëŠ” ìì‹ ì™„ë£Œ í›„ ì‹¤í–‰
        
        ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬:
        - Phase 1.5: ë¶€ëª¨ ë…¸ë“œì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¨¼ì € ìƒì„±
        - Phase 2: ìì‹ ë¶„ì„ ì‹œ ë¶€ëª¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì „ë‹¬
        
        Returns:
            (ë¶„ì„ ê²°ê³¼ ì—…ë°ì´íŠ¸ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸, ì‹¤íŒ¨í•œ ë°°ì¹˜ ìˆ˜, ì‹¤íŒ¨ ìƒì„¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸)
        """
        if self._nodes is None:
            raise AnalysisError(f"Phase 1ì´ ë¨¼ì € ì‹¤í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤: {self.file_name}")
        
        log_process("ANALYZE", "PHASE2", f"ğŸ¤– {self.full_directory} LLM ë¶„ì„ ì‹œì‘")
        
        all_queries: List[str] = []
        failed_batch_count = 0
        all_failed_details: List[Dict[str, Any]] = []
        
        # ì„ í–‰ ì²˜ë¦¬: ìƒì†/êµ¬í˜„ + í•„ë“œ + ë©”ì„œë“œ
        preprocessing_queries = await self._run_preprocessing()
        all_queries.extend(preprocessing_queries)
        
        # Phase 1.5: ë¶€ëª¨ ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ìì‹ ë¶„ì„ ì „ì— ë¨¼ì € ì‹¤í–‰)
        await self._generate_parent_contexts()
        
        # ë°°ì¹˜ ë¶„ì„
        planner = BatchPlanner()
        batches = planner.plan(self._nodes)
        
        if not batches:
            log_process("ANALYZE", "PHASE2", f"âš ï¸ {self.full_directory}: ë¶„ì„ ëŒ€ìƒ ë°°ì¹˜ ì—†ìŒ")
            return all_queries, 0, []
        
        log_process("ANALYZE", "PHASE2", f"ğŸ“Š ë°°ì¹˜ {len(batches)}ê°œ (completion_event ê¸°ë°˜ ì˜ì¡´ì„± ë³´ì¥)")
        
        # í´ë˜ìŠ¤ë³„ summary ìˆ˜ì§‘ìš© ì €ì¥ì†Œ
        class_summary_store: Dict[str, Dict[str, str]] = {key: {} for key in (self._classes or {})}
        
        # LLM í˜¸ì¶œ ë° ê²°ê³¼ ì²˜ë¦¬
        invoker = LLMInvoker(self.api_key, self.locale)
        
        async def process_batch(batch: AnalysisBatch, semaphore: asyncio.Semaphore) -> Tuple[List[str], Dict[str, Any]]:
            """ë°°ì¹˜ ì²˜ë¦¬ í›„ ì¿¼ë¦¬ì™€ ë¶„ì„ ê²°ê³¼ ë°˜í™˜. ë…¸ë“œì— summaryë„ ì„¤ì •.
            
            í•µì‹¬: ë¶€ëª¨ ë…¸ë“œëŠ” ìì‹ completion_eventë¥¼ ê¸°ë‹¤ë¦° í›„ ì‹¤í–‰ë¨
            â†’ ê¹Šì´ ê³„ì‚° ì—†ì´ ìì—°ìŠ¤ëŸ½ê²Œ leaf â†’ parent ìˆœì„œ ë³´ì¥
            
            ì¤‘ìš”: 
            - try/finallyë¡œ completion_event.set()ì„ ë³´ì¥í•˜ì—¬ ë°ë“œë½ ë°©ì§€
            - ìì‹ ì¤‘ ok=Falseê°€ ìˆìœ¼ë©´ ë¶€ëª¨ë„ ok=False (ë¶ˆì™„ì „ ìš”ì•½ ì „íŒŒ)
            - ë¶€ëª¨ ì»¨í…ìŠ¤íŠ¸ê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°
            """
            batch_failed = False
            async with semaphore:
                try:
                    # 0. ê° ë…¸ë“œì˜ ë¶€ëª¨ ì»¨í…ìŠ¤íŠ¸ê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°
                    for node in batch.nodes:
                        await node.context_ready_event.wait()
                    
                    # 1. ë°°ì¹˜ ë‚´ ëª¨ë“  ë…¸ë“œì˜ ìì‹ ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¼
                    for node in batch.nodes:
                        if node.has_children:
                            for child in node.children:
                                await child.completion_event.wait()
                                # ìì‹ ì¤‘ í•˜ë‚˜ë¼ë„ ì‹¤íŒ¨í•˜ë©´ ë¶€ëª¨ë„ ë¶ˆì™„ì „
                                if not child.ok:
                                    node.ok = False
                    
                    log_process("ANALYZE", "LLM", f"ë°°ì¹˜ #{batch.batch_id} ì²˜ë¦¬ ì¤‘ ({len(batch.nodes)}ê°œ ë…¸ë“œ)")
                    result = await invoker.invoke(batch)
                    
                    # 2. ë…¸ë“œì— summary ì„¤ì •
                    if result:
                        analysis_list = result.get("analysis") or []
                        for node, analysis in zip(batch.nodes, analysis_list):
                            if analysis:
                                node.summary = analysis.get("summary") or ""
                    
                    queries = self._build_analysis_queries(batch, result)
                    return queries, {"batch": batch, "result": result}
                except Exception:
                    # ë°°ì¹˜ ì‹¤íŒ¨ ì‹œ ëª¨ë“  ë…¸ë“œë¥¼ ok=Falseë¡œ ë§ˆí‚¹
                    batch_failed = True
                    for node in batch.nodes:
                        node.ok = False
                    raise
                finally:
                    # 3. ë¬´ì¡°ê±´ completion_event ì„¤ì • (ì‹¤íŒ¨í•´ë„ ë¶€ëª¨ê°€ ëŒ€ê¸°í•˜ì§€ ì•Šë„ë¡)
                    for node in batch.nodes:
                        node.completion_event.set()
        
        def collect_results(batch_results: list, batches_list: List[AnalysisBatch], level_name: str) -> Tuple[int, List[Dict[str, Any]]]:
            """ë°°ì¹˜ ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•˜ê³  (ì‹¤íŒ¨ ìˆ˜, ì‹¤íŒ¨ ìƒì„¸ ì •ë³´) ë°˜í™˜."""
            nonlocal all_queries
            fail_count = 0
            failed_details: List[Dict[str, Any]] = []
            
            for i, result in enumerate(batch_results):
                if isinstance(result, Exception):
                    batch = batches_list[i] if i < len(batches_list) else None
                    batch_id = batch.batch_id if batch else i
                    node_ranges = ", ".join(f"L{n.start_line}-{n.end_line}" for n in batch.nodes) if batch else "unknown"
                    error_msg = str(result)[:100]  # ìµœëŒ€ 100ì
                    
                    log_process("ANALYZE", "ERROR", f"[{level_name}] ë°°ì¹˜ #{batch_id} ì‹¤íŒ¨ ({node_ranges}): {error_msg}", logging.ERROR)
                    fail_count += 1
                    failed_details.append({
                        "batch_id": batch_id,
                        "node_ranges": node_ranges,
                        "error": error_msg
                    })
                else:
                    queries, batch_data = result
                    all_queries.extend(queries)
                    
                    # í´ë˜ìŠ¤ë³„ summary ìˆ˜ì§‘
                    batch_obj = batch_data["batch"]
                    llm_result = batch_data["result"]
                    if llm_result:
                        analysis_list = llm_result.get("analysis") or []
                        for node, analysis in zip(batch_obj.nodes, analysis_list):
                            if not analysis:
                                continue
                            summary = analysis.get("summary") or ""
                            if summary and node.class_key and node.class_key in class_summary_store:
                                key = f"{node.node_type}_{node.start_line}_{node.end_line}"
                                class_summary_store[node.class_key][key] = summary
            return fail_count, failed_details
        
        # ëª¨ë“  ë°°ì¹˜ ë³‘ë ¬ ì‹¤í–‰ (completion_eventê°€ ìˆœì„œ ë³´ì¥)
        # - leaf ë°°ì¹˜: ìì‹ì´ ì—†ìœ¼ë¯€ë¡œ ë°”ë¡œ ì‹¤í–‰
        # - parent ë°°ì¹˜: ìì‹ completion_event.wait() í›„ ì‹¤í–‰ â†’ ìì—°ìŠ¤ëŸ½ê²Œ ìˆœì„œ ë³´ì¥
        semaphore = asyncio.Semaphore(min(self.max_workers, len(batches)))
        batch_results = await asyncio.gather(
            *[process_batch(b, semaphore) for b in batches],
            return_exceptions=True
        )
        fail_count, failed_details = collect_results(batch_results, batches, "LLM")
        failed_batch_count += fail_count
        all_failed_details.extend(failed_details)
        
        # í´ë˜ìŠ¤ë³„ summary ì²˜ë¦¬ (ì²­í¬ ê¸°ë°˜ + User Story)
        if self._classes:
            class_queries = await self._process_class_summaries(class_summary_store)
            all_queries.extend(class_queries)
        
        # ì‹¤íŒ¨ í†µê³„ ë¡œê¹…
        if failed_batch_count > 0:
            log_process("ANALYZE", "PHASE2", f"âš ï¸ {self.full_directory}: {failed_batch_count}ê°œ ë°°ì¹˜ ì‹¤íŒ¨", logging.WARNING)
        
        log_process("ANALYZE", "PHASE2", f"âœ… {self.full_directory}: {len(all_queries)}ê°œ ì—…ë°ì´íŠ¸ ì¿¼ë¦¬")
        return all_queries, failed_batch_count, all_failed_details
    
    async def _process_class_summaries(self, class_summary_store: Dict[str, Dict[str, str]]) -> List[str]:
        """í´ë˜ìŠ¤ë³„ summaryë¥¼ ì²­í¬ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ìµœì¢… summary + User Story ìƒì„±.
        
        Args:
            class_summary_store: í´ë˜ìŠ¤ë³„ ë…¸ë“œ summary ì €ì¥ì†Œ
            
        Returns:
            ìƒì„±ëœ Neo4j ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        queries: List[str] = []
        
        if not self._classes:
            return queries
        
        for class_key, info in self._classes.items():
            summaries = class_summary_store.get(class_key, {})
            if not summaries:
                continue
            
            # í´ë˜ìŠ¤ ë…¸ë“œ ì°¾ê¸°
            class_node = next(
                (n for n in self._nodes if n.start_line == info.node_start and n.node_type == info.kind),
                None,
            )
            if not class_node:
                continue
            
            # í•˜ìœ„ ë…¸ë“œ ì¤‘ ì‹¤íŒ¨ê°€ ìˆìœ¼ë©´ ìµœì¢… summary/UserStory ìŠ¤í‚µ
            if not class_node.ok:
                log_process("ANALYZE", "SUMMARY", f"âš ï¸ {info.name}: í•˜ìœ„ ë¶„ì„ ì‹¤íŒ¨ë¡œ ìµœì¢… summary ìƒì„± ìŠ¤í‚µ")
                continue
            
            all_user_stories: List[Dict[str, Any]] = []
            final_summary = ""
            
            try:
                # 1ë‹¨ê³„: í† í° ê¸°ì¤€ìœ¼ë¡œ ì²­í¬ ë¶„í• 
                chunks = self._split_summaries_by_token(summaries, MAX_SUMMARY_CHUNK_TOKEN)
                
                if not chunks:
                    continue
                
                log_process("ANALYZE", "SUMMARY", f"ğŸ“¦ {info.name}: summary ì²­í¬ ë¶„í•  ì™„ë£Œ ({len(chunks)}ê°œ ì²­í¬)")
                
                # 2ë‹¨ê³„: ê° ì²­í¬ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ì—¬ summaryë§Œ ìƒì„± (User StoryëŠ” ìµœì¢… summaryì—ì„œë§Œ ìƒì„±)
                async def process_chunk(chunk_idx: int, chunk: dict) -> str:
                    chunk_tokens = calculate_code_token(json.dumps(chunk, ensure_ascii=False))
                    log_process("ANALYZE", "SUMMARY", f"  â†’ ì²­í¬ {chunk_idx + 1}/{len(chunks)} ì²˜ë¦¬ ì‹œì‘ (í† í°: {chunk_tokens})")
                    
                    # Summary ìƒì„±
                    summary_result = await asyncio.to_thread(
                        analyze_class_summary_only,
                        chunk,
                        self.api_key,
                        self.locale,
                        ""
                    )
                    
                    chunk_summary = ""
                    if isinstance(summary_result, dict):
                        chunk_summary = summary_result.get('summary', '')
                    
                    return chunk_summary
                
                # ëª¨ë“  ì²­í¬ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
                chunk_results_raw = await asyncio.gather(
                    *[process_chunk(idx, chunk) for idx, chunk in enumerate(chunks)]
                )
                
                # ê²°ê³¼ ì¶”ì¶œ
                chunk_results = []
                for chunk_summary in chunk_results_raw:
                    if chunk_summary:
                        chunk_results.append(chunk_summary)
                
                if not chunk_results:
                    continue
                
                # 3ë‹¨ê³„: ëª¨ë“  ì²­í¬ì˜ summaryë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
                if len(chunk_results) == 1:
                    final_summary = chunk_results[0]
                else:
                    combined_summaries = {f"CHUNK_{idx + 1}": s for idx, s in enumerate(chunk_results)}
                    final_summary_result = await asyncio.to_thread(
                        analyze_class_summary_only,
                        combined_summaries,
                        self.api_key,
                        self.locale,
                        ""
                    )
                    if isinstance(final_summary_result, dict):
                        final_summary = final_summary_result.get('summary', "\n\n".join(chunk_results))
                    else:
                        final_summary = "\n\n".join(chunk_results)
                
                log_process("ANALYZE", "SUMMARY", f"âœ… {info.name}: summary í†µí•© ì™„ë£Œ")
                
                # 4ë‹¨ê³„: ìµœì¢… summaryë¡œ User Story ìƒì„± (ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ìµœì¢… summaryì—ì„œë§Œ ìƒì„±)
                if final_summary:
                    user_story_result = await asyncio.to_thread(
                        analyze_class_user_story,
                        final_summary,
                        self.api_key,
                        self.locale
                    )
                    if isinstance(user_story_result, dict):
                        all_user_stories = user_story_result.get('user_stories', []) or []
                
                if all_user_stories:
                    log_process("ANALYZE", "SUMMARY", f"âœ… {info.name}: User Story {len(all_user_stories)}ê°œ")
                else:
                    log_process("ANALYZE", "SUMMARY", f"âœ… {info.name}: User Story ì—†ìŒ")
                
            except Exception as exc:
                log_process("ANALYZE", "SUMMARY", f"âŒ í´ë˜ìŠ¤ ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {info.name}", logging.ERROR, exc)
                continue
            
            if not final_summary:
                continue
            
            # Neo4j ì¿¼ë¦¬ ìƒì„±
            escaped_summary = escape_for_cypher(str(final_summary))
            
            # Summary ì €ì¥
            queries.append(
                f"MATCH (n:{info.kind} {{startLine: {info.node_start}, {self.node_base_props}}})\n"
                f"SET n.summary = '{escaped_summary}'\n"
                f"RETURN n"
            )
            
            # User Story + AC ì €ì¥
            if all_user_stories:
                class_name_escaped = escape_for_cypher(info.name)
                for us_idx, us in enumerate(all_user_stories, 1):
                    us_id = us.get('id', f"US-{us_idx}")
                    role = escape_for_cypher(us.get('role', ''))
                    goal = escape_for_cypher(us.get('goal', ''))
                    benefit = escape_for_cypher(us.get('benefit', ''))
                    
                    # UserStory ë…¸ë“œ ìƒì„± ë° ê´€ê³„
                    queries.append(
                        f"MATCH (c:{info.kind} {{startLine: {info.node_start}, {self.node_base_props}}})\n"
                        f"MERGE (us:UserStory {{id: '{escape_for_cypher(us_id)}', class_name: '{class_name_escaped}', {self.node_base_props}}})\n"
                        f"SET us.role = '{role}', us.goal = '{goal}', us.benefit = '{benefit}'\n"
                        f"MERGE (c)-[:HAS_USER_STORY]->(us)\n"
                        f"RETURN us"
                    )
                    
                    # AcceptanceCriteria ë…¸ë“œ ìƒì„± ë° ê´€ê³„
                    for ac_idx, ac in enumerate(us.get('acceptance_criteria', []) or [], 1):
                        ac_id = ac.get('id', f"AC-{us_idx}-{ac_idx}")
                        ac_title = escape_for_cypher(ac.get('title', ''))
                        ac_given = escape_for_cypher(ac.get('given', ''))
                        ac_when = escape_for_cypher(ac.get('when', ''))
                        ac_then = escape_for_cypher(ac.get('then', ''))
                        
                        queries.append(
                            f"MATCH (us:UserStory {{id: '{escape_for_cypher(us_id)}', class_name: '{class_name_escaped}', {self.node_base_props}}})\n"
                            f"MERGE (ac:AcceptanceCriteria {{id: '{escape_for_cypher(ac_id)}', user_story_id: '{escape_for_cypher(us_id)}', {self.node_base_props}}})\n"
                            f"SET ac.title = '{ac_title}', ac.given = '{ac_given}', ac.when = '{ac_when}', ac.then = '{ac_then}'\n"
                            f"MERGE (us)-[:HAS_AC]->(ac)\n"
                            f"RETURN ac"
                        )
        
        return queries
    
    def _split_summaries_by_token(self, summaries: dict, max_token: int) -> List[dict]:
        """í† í° ê¸°ì¤€ìœ¼ë¡œ summariesë¥¼ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
        if not summaries:
            return []
        
        chunks = []
        current_chunk = {}
        current_tokens = 0
        
        for key, value in summaries.items():
            item_text = f"{key}: {value}"
            item_tokens = calculate_code_token(item_text)
            
            if current_tokens + item_tokens > max_token and current_chunk:
                chunks.append(current_chunk)
                current_chunk = {}
                current_tokens = 0
            
            current_chunk[key] = value
            current_tokens += item_tokens
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks

    def _build_analysis_queries(
        self, 
        batch: AnalysisBatch, 
        result: Optional[Dict[str, Any]]
    ) -> List[str]:
        """LLM ë¶„ì„ ê²°ê³¼ë¥¼ MATCH ê¸°ë°˜ ì—…ë°ì´íŠ¸ ì¿¼ë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        queries: List[str] = []
        
        if not result:
            return queries
        
        analysis_list = result.get("analysis") or []
        
        for node, analysis in zip(batch.nodes, analysis_list):
            if not analysis:
                continue
            
            # ìš”ì•½ ì—…ë°ì´íŠ¸ (MATCH + SET)
            summary = analysis.get("summary") or ""
            if summary:
                escaped_summary = escape_for_cypher(str(summary))
                queries.append(
                    f"MATCH (n:{node.node_type} {{startLine: {node.start_line}, {self.node_base_props}}})\n"
                    f"SET n.summary = '{escaped_summary}'\n"
                    f"RETURN n"
                )
            
            # DEPENDENCY ê´€ê³„ (localDependencies)
            for dep in analysis.get("localDependencies", []) or []:
                if not dep:
                    continue
                dep_type = dep.get("type", "") if isinstance(dep, dict) else str(dep)
                if not dep_type or not _is_valid_class_name_for_calls(dep_type):
                    log_process("ANALYZE", "DEPENDENCY", f"âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ì˜ì¡´ íƒ€ì… ì œì™¸: {dep_type} (node={node.start_line})", logging.DEBUG)
                    continue
                source_member = dep.get("sourceMember", "unknown") if isinstance(dep, dict) else "unknown"
                
                # class_kindì™€ parent í™•ì¸
                if not node.class_kind:
                    log_process("ANALYZE", "DEPENDENCY", f"âš ï¸ class_kindê°€ None: {dep_type} (node={node.start_line}, type={node.node_type})", logging.DEBUG)
                    continue
                if not node.parent:
                    log_process("ANALYZE", "DEPENDENCY", f"âš ï¸ parentê°€ None: {dep_type} (node={node.start_line}, type={node.node_type})", logging.DEBUG)
                    continue
                
                # í´ë˜ìŠ¤ ë…¸ë“œ ì°¾ê¸° (class_kindì™€ parent.start_line ì‚¬ìš©)
                queries.append(
                    f"MATCH (src:{node.class_kind} {{startLine: {node.parent.start_line}, {self.node_base_props}}})\n"
                    f"MATCH (dst) WHERE (dst:CLASS OR dst:INTERFACE OR dst:ENUM)\n"
                    f"  AND toLower(dst.class_name) = toLower('{escape_for_cypher(dep_type)}')\n"
                    f"  AND dst.user_id = '{self.user_id}' AND dst.project_name = '{self.project_name}'\n"
                    f"  AND src <> dst AND NOT (src)-[:ASSOCIATION|COMPOSITION]->(dst)\n"
                    f"MERGE (src)-[r:DEPENDENCY {{usage: 'local', source_member: '{escape_for_cypher(source_member)}'}}]->(dst)\n"
                    f"RETURN r"
                )
                log_process("ANALYZE", "DEPENDENCY", f"âœ… DEPENDENCY ê´€ê³„ ìƒì„±: {node.class_kind} -> {dep_type} (sourceMember={source_member})", logging.DEBUG)
            
            # CALLS ê´€ê³„ (calls ë°°ì—´ - í”„ë¡¬í”„íŠ¸ í†µí•©)
            for call_str in analysis.get("calls", []) or []:
                if not call_str or not isinstance(call_str, str):
                    continue
                parts = call_str.split(".", 1)
                if len(parts) != 2:
                    continue
                target_class, method_name = parts
                
                if not _is_valid_class_name_for_calls(target_class):
                    continue
                
                if node.class_kind and node.parent:
                    queries.append(
                        f"MATCH (src:{node.class_kind} {{startLine: {node.parent.start_line}, {self.node_base_props}}})\n"
                        f"MATCH (dst) WHERE (dst:CLASS OR dst:INTERFACE OR dst:ENUM)\n"
                        f"  AND toLower(dst.class_name) = toLower('{escape_for_cypher(target_class)}')\n"
                        f"  AND dst.user_id = '{self.user_id}' AND dst.project_name = '{self.project_name}'\n"
                        f"MERGE (src)-[r:CALLS {{method: '{escape_for_cypher(method_name)}'}}]->(dst)\n"
                        f"RETURN r"
                    )
        
        return queries

    def _build_project_file_relationship(self) -> List[str]:
        """Project â†’ File (CONTAINS) ê´€ê³„ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        escaped_file_name = escape_for_cypher(self.file_name)
        escaped_dir = escape_for_cypher(self.full_directory)
        return [
            f"MATCH (p:Project {{user_id: '{self.user_id}', name: '{escape_for_cypher(self.project_name)}'}})\n"
            f"MATCH (f:FILE {{startLine: 1, directory: '{escaped_dir}', file_name: '{escaped_file_name}', user_id: '{self.user_id}', project_name: '{self.project_name}'}})\n"
            f"MERGE (p)-[r:CONTAINS]->(f)\n"
            f"RETURN r"
        ]

    def _build_relationship_queries(self) -> List[str]:
        """ì •ì  ê´€ê³„ ì¿¼ë¦¬ (HAS_METHOD, HAS_FIELD, CONTAINS, PARENT_OF)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        ê·œì¹™:
        - File â†’ CLASS/INTERFACE/ENUM (ìµœìƒìœ„ íƒ€ì…ë§Œ): CONTAINS
        - Class â†’ Method: HAS_METHOD
        - Class â†’ Field: HAS_FIELD
        - ê·¸ ì™¸ ë¶€ëª¨-ìì‹: PARENT_OF
        """
        queries: List[str] = []
        
        for node in self._nodes or []:
            if not node.parent:
                continue
            
            # ë¶€ëª¨-ìì‹ ê´€ê³„ ìƒì„±
            parent = node.parent
            
            # File â†’ ìµœìƒìœ„ íƒ€ì…(CLASS/INTERFACE/ENUM)ë§Œ CONTAINS
            if parent.node_type == "FILE" and node.node_type in CLASS_TYPES:
                queries.append(self._build_contains_query(parent, node))
            # Class â†’ Method: HAS_METHOD
            elif node.node_type in METHOD_TYPES:
                queries.append(self._build_has_method_query(parent, node))
            # Class â†’ Field: HAS_FIELD
            elif node.node_type in FIELD_TYPES:
                queries.append(self._build_has_field_query(parent, node))
            # ê·¸ ì™¸: PARENT_OF
            else:
                queries.append(self._build_parent_of_query(parent, node))
        
        return queries
    
    def _build_contains_query(self, parent: StatementNode, child: StatementNode) -> str:
        """CONTAINS ê´€ê³„ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (File â†’ ì§ì ‘ ìì‹ë§Œ)."""
        return (
                f"MATCH (p:{parent.node_type} {{startLine: {parent.start_line}, {self.node_base_props}}})\n"
            f"MATCH (c:{child.node_type} {{startLine: {child.start_line}, {self.node_base_props}}})\n"
            f"MERGE (p)-[r:CONTAINS]->(c)\n"
                f"RETURN r"
            )
        
    def _build_has_method_query(self, parent: StatementNode, child: StatementNode) -> str:
        """HAS_METHOD ê´€ê³„ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        return (
            f"MATCH (p:{parent.node_type} {{startLine: {parent.start_line}, {self.node_base_props}}})\n"
            f"MATCH (c:{child.node_type} {{startLine: {child.start_line}, {self.node_base_props}}})\n"
            f"MERGE (p)-[r:HAS_METHOD]->(c)\n"
            f"RETURN r"
        )
    
    def _build_has_field_query(self, parent: StatementNode, child: StatementNode) -> str:
        """HAS_FIELD ê´€ê³„ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        return (
            f"MATCH (p:{parent.node_type} {{startLine: {parent.start_line}, {self.node_base_props}}})\n"
            f"MATCH (c:{child.node_type} {{startLine: {child.start_line}, {self.node_base_props}}})\n"
            f"MERGE (p)-[r:HAS_FIELD]->(c)\n"
            f"RETURN r"
        )
    
    def _build_parent_of_query(self, parent: StatementNode, child: StatementNode) -> str:
        """PARENT_OF ê´€ê³„ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        return (
            f"MATCH (p:{parent.node_type} {{startLine: {parent.start_line}, {self.node_base_props}}})\n"
            f"MATCH (c:{child.node_type} {{startLine: {child.start_line}, {self.node_base_props}}})\n"
            f"MERGE (p)-[r:PARENT_OF]->(c)\n"
            f"RETURN r"
        )

    async def _run_preprocessing(self) -> List[str]:
        """ì„ í–‰ ì²˜ë¦¬: ìƒì†/êµ¬í˜„, í•„ë“œ, ë©”ì„œë“œ ë¶„ì„ í›„ ì¿¼ë¦¬ ìƒì„±."""
        queries: List[str] = []
        
        # ìƒì†/êµ¬í˜„, í•„ë“œ, ë©”ì„œë“œ ë…¸ë“œ ë¶„ë¥˜
        inheritance_nodes = []
        field_nodes = []
        method_nodes = []
        
        for node in self._nodes or []:
            if node.node_type in INHERITANCE_TYPES:
                inheritance_nodes.append(node)
            elif node.node_type in FIELD_TYPES:
                field_nodes.append(node)
            elif node.node_type in METHOD_SIGNATURE_TYPES:
                method_nodes.append(node)
        
        # ë³‘ë ¬ ì²˜ë¦¬
        tasks = []
        
        if inheritance_nodes:
            tasks.append(self._analyze_inheritance_nodes(inheritance_nodes))
        if field_nodes:
            tasks.append(self._analyze_field_nodes(field_nodes))
        if method_nodes:
            tasks.append(self._analyze_method_nodes(method_nodes))
        
        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            for result in results:
                if isinstance(result, list):
                    queries.extend(result)
                elif isinstance(result, Exception):
                    log_process("ANALYZE", "PREPROCESS", f"ì„ í–‰ ì²˜ë¦¬ ì˜¤ë¥˜: {result}", logging.WARNING)
        
        return queries

    async def _analyze_inheritance_nodes(self, nodes: List[StatementNode]) -> List[str]:
        """ìƒì†/êµ¬í˜„ ë…¸ë“œ ë¶„ì„."""
        queries: List[str] = []
        semaphore = asyncio.Semaphore(INHERITANCE_CONCURRENCY)
        
        async def analyze_one(node: StatementNode) -> List[str]:
            async with semaphore:
                try:
                    result = await asyncio.to_thread(
                        analyze_inheritance, node.code, self.api_key, self.locale
                    )
                    return self._build_inheritance_queries(node, result)
                except Exception as e:
                    log_process("ANALYZE", "INHERITANCE", f"âŒ ìƒì† ë¶„ì„ ì‹¤íŒ¨ (node={node.start_line}): {e}", logging.ERROR, e)
                    raise
        
        results = await asyncio.gather(*[analyze_one(n) for n in nodes])
        for r in results:
            queries.extend(r)
        
        return queries

    async def _analyze_field_nodes(self, nodes: List[StatementNode]) -> List[str]:
        """í•„ë“œ ë…¸ë“œ ë¶„ì„."""
        queries: List[str] = []
        semaphore = asyncio.Semaphore(FIELD_CONCURRENCY)
        
        async def analyze_one(node: StatementNode) -> List[str]:
            async with semaphore:
                try:
                    result = await asyncio.to_thread(
                        analyze_field, node.code, self.api_key, self.locale
                    )
                    return self._build_field_queries(node, result)
                except Exception as e:
                    log_process("ANALYZE", "FIELD", f"âŒ í•„ë“œ ë¶„ì„ ì‹¤íŒ¨ (node={node.start_line}): {e}", logging.ERROR, e)
                    raise
        
        results = await asyncio.gather(*[analyze_one(n) for n in nodes])
        for r in results:
            queries.extend(r)
        
        return queries

    async def _analyze_method_nodes(self, nodes: List[StatementNode]) -> List[str]:
        """ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ë¶„ì„."""
        queries: List[str] = []
        semaphore = asyncio.Semaphore(METHOD_CONCURRENCY)
        
        async def analyze_one(node: StatementNode) -> List[str]:
            async with semaphore:
                try:
                    result = await asyncio.to_thread(
                        analyze_method, node.code, self.api_key, self.locale
                    )
                    return self._build_method_queries(node, result)
                except Exception as e:
                    log_process("ANALYZE", "METHOD", f"âŒ ë©”ì„œë“œ ë¶„ì„ ì‹¤íŒ¨ (node={node.start_line}): {e}", logging.ERROR, e)
                    raise
        
        results = await asyncio.gather(*[analyze_one(n) for n in nodes])
        for r in results:
            queries.extend(r)
        
        return queries

    # ===== ì¿¼ë¦¬ ë¹Œë” ë©”ì„œë“œ =====
    def _build_static_node_queries(self, node: StatementNode) -> List[str]:
        """ì •ì  ë…¸ë“œ ìƒì„± ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        queries: List[str] = []
        label = node.node_type
        
        # name ì†ì„± ê²°ì •: CLASS/INTERFACE/METHODëŠ” ì‹¤ì œ ì´ë¦„, ê·¸ ì™¸ëŠ” íƒ€ì…[ë¼ì¸ë²ˆí˜¸]
        if label == "FILE":
            node_name = self.file_name
        elif label in CLASS_TYPES and node.class_name:
            node_name = node.class_name
        else:
            node_name = f"{label}[{node.start_line}]"
        
        escaped_name = escape_for_cypher(node_name)
        has_children = "true" if node.has_children else "false"
        escaped_code = escape_for_cypher(node.code)

        base_set = [
            f"n.endLine = {node.end_line}",
            f"n.name = '{escaped_name}'",
            f"n.node_code = '{escaped_code}'",
            f"n.token = {node.token}",
            f"n.has_children = {has_children}",
        ]

        # CLASS/INTERFACE ë“±: class_nameê³¼ type ì†ì„± ì¶”ê°€
        if label in CLASS_TYPES and node.class_name:
            base_set.append(f"n.class_name = '{escape_for_cypher(node.class_name)}'")
            base_set.append(f"n.type = '{label}'")
        # ê·¸ ì™¸ ë…¸ë“œ: ì†Œì† í´ë˜ìŠ¤ëª… ì €ì¥
        elif node.class_name:
            base_set.append(f"n.class_name = '{escape_for_cypher(node.class_name)}'")

        if node.has_children:
            escaped_placeholder = escape_for_cypher(node.get_placeholder_code())
            base_set.append(f"n.summarized_code = '{escaped_placeholder}'")

        base_set_str = ", ".join(base_set)
        
        # CLASS/INTERFACE/ENUM ë…¸ë“œ: MERGEë¡œ ìƒì„± (ì¤‘ë³µ ë°©ì§€)
        # ìƒˆ ì•„í‚¤í…ì²˜ì—ì„œëŠ” Phase 1ì—ì„œ ëª¨ë“  í´ë˜ìŠ¤ê°€ ë¨¼ì € ìƒì„±ë˜ë¯€ë¡œ TEMP ë…¸ë“œ íŒ¨í„´ ì œê±°
        if label in ("CLASS", "INTERFACE", "ENUM") and node.class_name:
            escaped_class_name = escape_for_cypher(node.class_name)
            queries.append(
                f"MERGE (n:{label} {{class_name: '{escaped_class_name}', user_id: '{self.user_id}', project_name: '{self.project_name}'}})\n"
                f"SET n.startLine = {node.start_line}, n.directory = '{escape_for_cypher(self.full_directory)}', n.file_name = '{self.file_name}', {base_set_str}\n"
                f"RETURN n"
            )
        else:
            queries.append(
                f"MERGE (n:{label} {{startLine: {node.start_line}, {self.node_base_props}}})\n"
                f"SET {base_set_str}\n"
                f"RETURN n"
            )
        return queries

    def _build_inheritance_queries(self, node: StatementNode, analysis: Dict[str, Any]) -> List[str]:
        """ìƒì†/êµ¬í˜„ ë¶„ì„ ê²°ê³¼ë¥¼ Neo4j ì¿¼ë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        if not isinstance(analysis, dict):
            raise AnalysisError(f"ìƒì† ë¶„ì„ ê²°ê³¼ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (node={node.start_line}): {type(analysis)}")

        queries: List[str] = []
        relations = analysis.get("relations") or []

        for rel in relations:
            to_type = escape_for_cypher(rel.get("toType") or "")
            rel_type = rel.get("relationType") or "EXTENDS"
            to_type_kind = escape_for_cypher(rel.get("toTypeKind") or ("INTERFACE" if rel_type == "IMPLEMENTS" else "CLASS"))

            if not to_type:
                continue

            # ì†ŒìŠ¤ í´ë˜ìŠ¤ ë…¸ë“œ ë§¤ì¹­
            src_match = f"MATCH (src:{node.class_kind or 'CLASS'} {{startLine: {node.parent.start_line if node.parent else node.start_line}, {self.node_base_props}}})"

            # Phase 1ì—ì„œ ëª¨ë“  í´ë˜ìŠ¤ê°€ ìƒì„±ë˜ë¯€ë¡œ TEMP ë…¸ë“œ ìƒì„± ì—†ì´ MATCHë§Œ ì‚¬ìš©
            # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í´ë˜ìŠ¤(ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë“±)ì— ëŒ€í•œ ê´€ê³„ëŠ” ìƒì„±ë˜ì§€ ì•ŠìŒ
            queries.append(
                f"{src_match}\n"
                f"MATCH (dst) WHERE (dst:CLASS OR dst:INTERFACE OR dst:ENUM)\n"
                f"  AND toLower(dst.class_name) = toLower('{to_type}')\n"
                f"  AND dst.user_id = '{self.user_id}'\n"
                f"  AND dst.project_name = '{self.project_name}'\n"
                f"MERGE (src)-[r:{rel_type}]->(dst)\n"
                f"RETURN src, dst, r"
            )

        return queries

    def _build_field_queries(self, node: StatementNode, analysis: Dict[str, Any]) -> List[str]:
        """í•„ë“œ ë¶„ì„ ê²°ê³¼ë¥¼ Neo4j ì¿¼ë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        if not isinstance(analysis, dict):
            raise AnalysisError(f"í•„ë“œ ë¶„ì„ ê²°ê³¼ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (node={node.start_line}): {type(analysis)}")

        queries: List[str] = []
        fields = analysis.get("fields") or []

        for field_info in fields:
            field_name = escape_for_cypher(field_info.get("field_name") or "")
            field_type_raw = field_info.get("field_type") or ""
            field_type = escape_for_cypher(field_type_raw)
            target_class_raw = field_info.get("target_class")
            target_class = escape_for_cypher(target_class_raw) if target_class_raw else None
            visibility = escape_for_cypher(field_info.get("visibility") or "private")
            is_static = "true" if field_info.get("is_static") else "false"
            is_final = "true" if field_info.get("is_final") else "false"
            multiplicity = escape_for_cypher(field_info.get("multiplicity") or "1")
            association_type = field_info.get("association_type") or "ASSOCIATION"

            if not field_name:
                continue

            # í•„ë“œ íƒ€ì… ìºì‹œ ì—…ë°ì´íŠ¸ (Collection/Map í•„í„°ë§ìš©)
            if node.class_key and node.class_key in self._field_type_cache:
                # escape ì „ ì›ë³¸ í•„ë“œëª…ê³¼ íƒ€ì… ì €ì¥
                original_field_name = field_info.get("field_name") or ""
                self._field_type_cache[node.class_key][original_field_name] = field_type_raw

            # FIELD ë…¸ë“œ ì†ì„± ì—…ë°ì´íŠ¸
            # target_classê°€ ìˆìœ¼ë©´ í´ë˜ìŠ¤ íƒ€ì… í•„ë“œ (ì—°ê´€ ê´€ê³„ ëŒ€ìƒ)
            target_class_set = f", f.target_class = '{target_class}'" if target_class else ""
            queries.append(
                f"MATCH (f:FIELD {{startLine: {node.start_line}, {self.node_base_props}}})\n"
                f"SET f.name = '{field_name}', f.field_type = '{field_type}', "
                f"f.visibility = '{visibility}', f.is_static = {is_static}, f.is_final = {is_final}{target_class_set}\n"
                f"RETURN f"
            )

            # ì—°ê´€ ê´€ê³„ ìƒì„± (ASSOCIATION, COMPOSITION)
            # Phase 1ì—ì„œ ëª¨ë“  í´ë˜ìŠ¤ê°€ ìƒì„±ë˜ë¯€ë¡œ TEMP ë…¸ë“œ ìƒì„± ì—†ì´ MATCHë§Œ ì‚¬ìš©
            if target_class:
                src_match = f"MATCH (src:{node.class_kind or 'CLASS'} {{startLine: {node.parent.start_line if node.parent else node.start_line}, {self.node_base_props}}})"
                queries.append(
                    f"{src_match}\n"
                    f"MATCH (dst) WHERE (dst:CLASS OR dst:INTERFACE OR dst:ENUM)\n"
                    f"  AND toLower(dst.class_name) = toLower('{target_class}')\n"
                    f"  AND dst.user_id = '{self.user_id}'\n"
                    f"  AND dst.project_name = '{self.project_name}'\n"
                    f"MERGE (src)-[r:{association_type} {{source_member: '{field_name}', multiplicity: '{multiplicity}'}}]->(dst)\n"
                    f"RETURN src, dst, r"
                )

        return queries

    def _build_method_queries(self, node: StatementNode, analysis: Dict[str, Any]) -> List[str]:
        """ë©”ì„œë“œ ë¶„ì„ ê²°ê³¼ë¥¼ Neo4j ì¿¼ë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        if not isinstance(analysis, dict):
            raise AnalysisError(f"ë©”ì„œë“œ ë¶„ì„ ê²°ê³¼ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (node={node.start_line}): {type(analysis)}")

        queries: List[str] = []
        
        method_name = escape_for_cypher(analysis.get("method_name") or "")
        return_type = escape_for_cypher(analysis.get("return_type") or "void")
        visibility = escape_for_cypher(analysis.get("visibility") or "public")
        is_static = "true" if analysis.get("is_static") else "false"
        method_kind = escape_for_cypher(analysis.get("method_type") or "normal")
        parameters = analysis.get("parameters") or []
        dependencies = analysis.get("dependencies") or []

        # METHOD ë…¸ë“œì— ì‹œê·¸ë‹ˆì²˜ ì •ë³´ ì €ì¥ (nameë„ methodNameìœ¼ë¡œ ì„¤ì •)
        queries.append(
            f"MATCH (m:{node.node_type} {{startLine: {node.start_line}, {self.node_base_props}}})\n"
            f"SET m.name = '{method_name}', m.return_type = '{return_type}', "
            f"m.visibility = '{visibility}', m.is_static = {is_static}, "
            f"m.method_type = '{method_kind}'\n"
            f"RETURN m"
        )

        # ê° íŒŒë¼ë¯¸í„°ë¥¼ ê°œë³„ Parameter ë…¸ë“œë¡œ ì €ì¥
        for idx, param in enumerate(parameters):
            param_name = escape_for_cypher(param.get("name") or "")
            param_type = escape_for_cypher(param.get("type") or "")
            if not param_name:
                continue
            queries.append(
                f"MATCH (m:{node.node_type} {{startLine: {node.start_line}, {self.node_base_props}}})\n"
                # Parameter ë…¸ë“œ ì†ì„±ëª…ì€ snake_caseë¡œ í†µì¼
                f"MERGE (p:Parameter {{name: '{param_name}', method_start_line: {node.start_line}, {self.node_base_props}}})\n"
                f"SET p.type = '{param_type}', p.index = {idx}\n"
                f"MERGE (m)-[r:HAS_PARAMETER]->(p)\n"
                f"RETURN m, p, r"
            )

        # ì˜ì¡´ ê´€ê³„ ìƒì„± (DEPENDENCY) - ì—°ê´€ ê´€ê³„ê°€ ì—†ì„ ë•Œë§Œ
        # Phase 1ì—ì„œ ëª¨ë“  í´ë˜ìŠ¤ê°€ ìƒì„±ë˜ë¯€ë¡œ TEMP ë…¸ë“œ ìƒì„± ì—†ì´ MATCHë§Œ ì‚¬ìš©
        for dep in dependencies:
            target_type = escape_for_cypher(dep.get("target_class") or "")
            usage = escape_for_cypher(dep.get("usage") or "parameter")
            is_value_object_cypher = "true" if dep.get("is_value_object") else "false"

            if not target_type:
                continue

            src_match = f"MATCH (src:{node.class_kind or 'CLASS'} {{startLine: {node.parent.start_line if node.parent else node.start_line}, {self.node_base_props}}})"
            queries.append(
                f"{src_match}\n"
                f"MATCH (dst) WHERE (dst:CLASS OR dst:INTERFACE OR dst:ENUM)\n"
                f"  AND toLower(dst.class_name) = toLower('{target_type}')\n"
                f"  AND dst.user_id = '{self.user_id}'\n"
                f"  AND dst.project_name = '{self.project_name}'\n"
                f"  AND src <> dst\n"
                f"  AND NOT (src)-[:ASSOCIATION|COMPOSITION]->(dst)\n"
                f"MERGE (src)-[r:DEPENDENCY {{usage: '{usage}', source_member: '{method_name}'}}]->(dst)\n"
                f"SET r.is_value_object = {is_value_object_cypher}\n"
                f"RETURN src, dst, r"
            )

        # í•„ë“œ í• ë‹¹ íŒ¨í„´ì— ë”°ë¥¸ ì—°ê´€ ê´€ê³„ ì„¸ë¶„í™” (ASSOCIATION â†’ COMPOSITION)
        field_assignments = analysis.get("field_assignments") or []
        src_start_line = node.parent.start_line if node.parent else node.start_line
        for assign in field_assignments:
            field_name = escape_for_cypher(assign.get("field_name") or "")
            value_source = assign.get("value_source") or ""

            if not field_name or not value_source:
                continue

            # value_sourceê°€ "new"ì¸ ê²½ìš°ì—ë§Œ COMPOSITIONìœ¼ë¡œ ë³€ê²½ (parameterëŠ” ASSOCIATION ìœ ì§€)
            if value_source == "new":
                # FIELD ë…¸ë“œì˜ target_classê°€ ìˆìœ¼ë©´ (í´ë˜ìŠ¤ íƒ€ì… í•„ë“œ) ê¸°ì¡´ ASSOCIATIONì„ COMPOSITIONìœ¼ë¡œ ë³€ê²½
                queries.append(
                    f"MATCH (field:FIELD {{name: '{field_name}', {self.node_base_props}}})\n"
                    f"WHERE field.target_class IS NOT NULL\n"
                    f"MATCH (src:{node.class_kind or 'CLASS'} {{startLine: {src_start_line}, {self.node_base_props}}})"
                    f"-[r:ASSOCIATION {{source_member: '{field_name}'}}]->(dst)\n"
                    f"WITH src, dst, COALESCE(r.multiplicity, '1') AS mult, r\n"
                    f"DELETE r\n"
                    f"MERGE (src)-[r2:COMPOSITION {{source_member: '{field_name}', multiplicity: mult}}]->(dst)\n"
                    f"RETURN src, dst, r2"
                )

        return queries
