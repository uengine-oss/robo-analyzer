"""Framework ì½”ë“œ ë¶„ì„ê¸° - Java/Kotlin AST â†’ Neo4j ê·¸ë˜í”„

í´ë˜ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨ ìƒì„±ì— í•„ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

ë¶„ì„ íŒŒì´í”„ë¼ì¸:
1. AST ìˆ˜ì§‘ (StatementCollector)
2. ì •ì  ê·¸ë˜í”„ ìƒì„± (CLASS, METHOD, FIELD ë…¸ë“œ)
3. ìƒì†/êµ¬í˜„ ê´€ê³„ ì¶”ì¶œ (EXTENDS, IMPLEMENTS)
4. LLM ë°°ì¹˜ ë¶„ì„ (ìš”ì•½, ë©”ì„œë“œ ì½œ ì¶”ì¶œ)
5. í´ë˜ìŠ¤ ìš”ì•½ ë° User Story ìƒì„±

ë¦¬íŒ©í† ë§: BaseAstProcessor ìƒì†ìœ¼ë¡œ ê³µí†µ ë¡œì§ ì¬ì‚¬ìš©
"""

from __future__ import annotations

import asyncio
import json
import logging
import re
from typing import Any, Dict, List, Optional, Tuple, Set

from config.settings import settings
from util.rule_loader import RuleLoader
# Exceptions: ëª¨ë“  ì»¤ìŠ¤í…€ ì˜ˆì™¸ëŠ” RuntimeErrorë¡œ ëŒ€ì²´ë¨
from util.text_utils import calculate_code_token, escape_for_cypher, log_process

from analyzer.strategy.base.statement_node import StatementNode
from analyzer.strategy.base.batch import AnalysisBatch
from analyzer.strategy.base.processor import BaseAstProcessor


# ==================== ìƒìˆ˜ ì •ì˜ ====================
# ë…¸ë“œ íƒ€ì… ë¶„ë¥˜
NON_ANALYSIS_TYPES = frozenset(["FILE", "PACKAGE", "IMPORT"])
CLASS_TYPES = frozenset(["CLASS", "INTERFACE", "ENUM"])
INHERITANCE_TYPES = frozenset(["EXTENDS", "IMPLEMENTS"])
FIELD_TYPES = frozenset(["FIELD"])
METHOD_TYPES = frozenset(["METHOD", "CONSTRUCTOR"])
METHOD_SIGNATURE_TYPES = frozenset(["METHOD_SIGNATURE"])

# ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¤ëŠ” ìƒìˆ˜
MAX_BATCH_TOKEN = settings.batch.framework_max_batch_token
MAX_CONCURRENCY = settings.concurrency.framework_max_concurrency
INHERITANCE_CONCURRENCY = settings.concurrency.inheritance_concurrency
FIELD_CONCURRENCY = settings.concurrency.field_concurrency
METHOD_CONCURRENCY = settings.concurrency.method_concurrency
STATIC_QUERY_BATCH_SIZE = settings.batch.static_query_batch_size
MAX_SUMMARY_CHUNK_TOKEN = settings.batch.max_summary_chunk_token
MAX_CONTEXT_TOKEN = settings.batch.max_context_token
PARENT_EXPAND_THRESHOLD = settings.batch.parent_expand_threshold

# Java í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ê¸°ë³¸ íƒ€ì… - í´ë˜ìŠ¤ ìƒì„± ì œì™¸ ëŒ€ìƒ
JAVA_BUILTIN_TYPES = frozenset([
    # ê¸°ë³¸ íƒ€ì… ë° ë˜í¼
    "int", "long", "double", "float", "boolean", "char", "byte", "short", "void",
    "Integer", "Long", "Double", "Float", "Boolean", "Character", "Byte", "Short",
    # ê¸°ë³¸ í´ë˜ìŠ¤
    "String", "Object", "Class", "Enum", "System", "Math", "Runtime",
    # ì»¬ë ‰ì…˜
    "List", "ArrayList", "LinkedList", "Set", "HashSet", "TreeSet", "LinkedHashSet",
    "Map", "HashMap", "TreeMap", "LinkedHashMap", "ConcurrentHashMap",
    "Collection", "Collections", "Arrays", "Iterator", "Iterable",
    "Queue", "Deque", "Stack", "Vector", "PriorityQueue",
    # ìœ í‹¸ë¦¬í‹°
    "Optional", "Stream", "Collectors", "Comparator", "Comparable",
    "Date", "Calendar", "LocalDate", "LocalTime", "LocalDateTime", "Instant",
    "UUID", "Random", "Scanner", "Pattern", "Matcher",
    # ì˜ˆì™¸
    "Exception", "RuntimeException", "Throwable", "Error",
    "IOException", "SQLException", "NullPointerException", "IllegalArgumentException",
    # I/O
    "File", "Path", "Files", "InputStream", "OutputStream", "Reader", "Writer",
    "BufferedReader", "BufferedWriter", "PrintWriter", "FileReader", "FileWriter",
    # ê¸°íƒ€
    "StringBuilder", "StringBuffer", "BigDecimal", "BigInteger",
    "Logger", "Log", "LogFactory",
])

# ìœ í‹¸ë¦¬í‹°/í—¬í¼ í´ë˜ìŠ¤ íŒ¨í„´ - CALLS ê´€ê³„ ìƒì„± ì œì™¸ ëŒ€ìƒ
UTILITY_CLASS_PATTERNS = frozenset([
    "Debug", "Logger", "Log", "LogFactory", "LogManager",
    "Utils", "Utility", "Utilities", "Helper", "Helpers",
    "Constants", "Config", "Configuration", "Settings",
    "Validator", "Validation", "Formatter", "Converter",
    "StringUtils", "DateUtils", "NumberUtils", "CollectionUtils",
    "Assert", "Assertions", "Preconditions", "Check",
])


# ==================== ë°ì´í„° í´ë˜ìŠ¤ ====================
class ClassInfo:
    """í´ë˜ìŠ¤/ì¸í„°í˜ì´ìŠ¤ ì •ë³´"""
    __slots__ = ('key', 'name', 'kind', 'node_start', 'node_end', 'pending_nodes', 'finalized')
    
    def __init__(
        self,
        key: str,
        name: str,
        kind: str,
        node_start: int,
        node_end: int,
        pending_nodes: int = 0,
        finalized: bool = False,
    ):
        self.key = key
        self.name = name
        self.kind = kind
        self.node_start = node_start
        self.node_end = node_end
        self.pending_nodes = pending_nodes
        self.finalized = finalized


# ==================== í—¬í¼ í•¨ìˆ˜ ====================
def _is_valid_class_name_for_calls(name: str) -> bool:
    """calls ê´€ê³„ ìƒì„±ì— ìœ íš¨í•œ í´ë˜ìŠ¤ëª…ì¸ì§€ ê²€ì¦."""
    if not name:
        return False
    if name in JAVA_BUILTIN_TYPES:
        return False
    if name in UTILITY_CLASS_PATTERNS:
        return False
    if len(name) == 1:
        return False
    if name[0].islower() and len(name) <= 3:
        return False
    if name.islower() and len(name) <= 6:
        return False
    return True


# ==================== RuleLoader í—¬í¼ ====================
def _rule_loader() -> RuleLoader:
    return RuleLoader(target_lang="framework")


def analyze_code(code: str, context: str, ranges: list, count: int, api_key: str, locale: str) -> Dict[str, Any]:
    """ì½”ë“œ ë²”ìœ„ë³„ ë¶„ì„"""
    inputs = {"code": code, "ranges": ranges, "count": count, "locale": locale}
    if context.strip():
        inputs["context"] = context
    return _rule_loader().execute(
        "analysis",
        inputs,
        api_key,
    )


def analyze_class_summary_only(summaries: dict, api_key: str, locale: str, previous_summary: str = "") -> Dict[str, Any]:
    """í´ë˜ìŠ¤ ì „ì²´ ìš”ì•½ ìƒì„± (Summaryë§Œ)."""
    return _rule_loader().execute(
        "class_summary_only",
        {"summaries": summaries, "locale": locale, "previous_summary": previous_summary},
        api_key,
    )


def analyze_class_user_story(summary: str, api_key: str, locale: str) -> Dict[str, Any]:
    """í´ë˜ìŠ¤ User Story + AC ìƒì„±."""
    return _rule_loader().execute(
        "class_user_story",
        {"summary": summary, "locale": locale},
        api_key,
    )


def analyze_inheritance(declaration_code: str, api_key: str, locale: str) -> Dict[str, Any]:
    """ìƒì†/êµ¬í˜„ ê´€ê³„ ì¶”ì¶œ."""
    return _rule_loader().execute(
        "inheritance",
        {"declaration_code": declaration_code, "locale": locale},
        api_key,
    )


def analyze_field(declaration_code: str, api_key: str, locale: str) -> Dict[str, Any]:
    """í•„ë“œ ì •ë³´ ì¶”ì¶œ."""
    return _rule_loader().execute(
        "field",
        {"declaration_code": declaration_code, "locale": locale},
        api_key,
    )


def analyze_method(declaration_code: str, api_key: str, locale: str) -> Dict[str, Any]:
    """ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ë¶„ì„."""
    return _rule_loader().execute(
        "method",
        {"declaration_code": declaration_code, "locale": locale},
        api_key,
    )


def analyze_parent_context(skeleton_code: str, ancestor_context: str, api_key: str, locale: str) -> Dict[str, Any]:
    """ë¶€ëª¨ ë…¸ë“œì˜ ìŠ¤ì¼ˆë ˆí†¤ ì½”ë“œì—ì„œ í•µì‹¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    return _rule_loader().execute(
        "parent_context",
        {"skeleton_code": skeleton_code, "ancestor_context": ancestor_context, "locale": locale},
        api_key,
    )


# ==================== ë…¸ë“œ ìˆ˜ì§‘ê¸° ====================
class StatementCollector:
    """ASTë¥¼ í›„ìœ„ìˆœíšŒí•˜ì—¬ StatementNodeì™€ í´ë˜ìŠ¤ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    
    file_contentëŠ” ë” ì´ìƒ í•„ìš”í•˜ì§€ ì•ŠìŒ - AST JSONì˜ code ì†ì„± ì‚¬ìš©.
    """

    def __init__(self, antlr_data: Dict[str, Any], directory: str, file_name: str):
        self.antlr_data = antlr_data
        self.directory = directory
        self.file_name = file_name
        self.nodes: List[StatementNode] = []
        self.classes: Dict[str, ClassInfo] = {}
        self._node_id = 0

    def _parse_code_to_lines(self, code: str, start_line: int, end_line: int) -> List[Tuple[int, str]]:
        """JSON code ì†ì„±ì„ [(line_no, text), ...] í˜•íƒœë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.
        
        Args:
            code: '1: public class...\n2: ...' ë˜ëŠ” '1: public class...\r\n2: ...' í˜•íƒœì˜ ë¬¸ìì—´
            start_line: ë…¸ë“œ ì‹œì‘ ë¼ì¸ (fallbackìš©)
            end_line: ë…¸ë“œ ì¢…ë£Œ ë¼ì¸ (fallbackìš©)
            
        Returns:
            [(line_no, text), ...] í˜•íƒœì˜ íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        if not code:
            return []
        
        # \r\n ë˜ëŠ” \nìœ¼ë¡œ ë¶„ë¦¬
        lines = code.replace('\r\n', '\n').split('\n')
        parsed_lines: List[Tuple[int, str]] = []
        
        for line in lines:
            if not line:
                continue
            # '123: text' í˜•íƒœ íŒŒì‹±
            match = re.match(r'^(\d+):\s?(.*)', line)
            if match:
                line_no = int(match.group(1))
                text = match.group(2)
                parsed_lines.append((line_no, text))
            else:
                # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ì „ì²´ ë¼ì¸ì„ í…ìŠ¤íŠ¸ë¡œ (fallback)
                if parsed_lines:
                    last_no = parsed_lines[-1][0]
                    parsed_lines.append((last_no + 1, line))
                else:
                    parsed_lines.append((start_line, line))
        
        return parsed_lines

    def collect(self) -> Tuple[List[StatementNode], Dict[str, ClassInfo]]:
        """AST ì „ì—­ì„ í›„ìœ„ ìˆœíšŒí•˜ì—¬ ë…¸ë“œ ëª©ë¡ê³¼ í´ë˜ìŠ¤ ì •ë³´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        self._visit(self.antlr_data, None, None, None, None)
        return self.nodes, self.classes

    def _make_class_key(self, class_name: Optional[str], start_line: int) -> str:
        """í´ë˜ìŠ¤ ê³ ìœ í‚¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        base = class_name or f"anonymous_{start_line}"
        return f"{self.directory}:{self.file_name}:{base}:{start_line}"

    def _extract_class_name(self, code: str, node_type: str) -> Optional[str]:
        """ì½”ë“œì—ì„œ í´ë˜ìŠ¤/ì¸í„°í˜ì´ìŠ¤ ì´ë¦„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        patterns = {
            "CLASS": r"\bclass\s+(\w+)",
            "INTERFACE": r"\binterface\s+(\w+)",
            "ENUM": r"\benum\s+(\w+)",
            "RECORD": r"\brecord\s+(\w+)",
            "ANNOTATION_TYPE": r"@interface\s+(\w+)",
        }
        pattern = patterns.get(node_type)
        if pattern:
            match = re.search(pattern, code, re.IGNORECASE)
            if match:
                return match.group(1)
        return None

    def _visit(
        self,
        node: Dict[str, Any],
        parent: Optional[StatementNode],
        current_class: Optional[str],
        current_class_name: Optional[str],
        current_class_kind: Optional[str],
    ) -> Optional[StatementNode]:
        """ì¬ê·€ì ìœ¼ë¡œ ASTë¥¼ ë‚´ë ¤ê°€ë©° StatementNodeë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        node_type = node["type"]
        start_line = node["startLine"]
        end_line = node["endLine"]
        children = node.get("children", []) or []

        # AST JSONì˜ code ì†ì„±ì—ì„œ ë¼ì¸ ì •ë³´ ì¶”ì¶œ
        raw_code = node.get('code', '')
        line_entries = self._parse_code_to_lines(raw_code, start_line, end_line)
        code = "\n".join(f"{ln}: {txt}" for ln, txt in line_entries)

        class_key = current_class
        class_name = current_class_name
        class_kind = current_class_kind

        # í´ë˜ìŠ¤/ì¸í„°í˜ì´ìŠ¤ ë…¸ë“œ ì²˜ë¦¬
        if node_type in CLASS_TYPES:
            # JSONì—ì„œ name ì§ì ‘ ì¶”ì¶œ (ì •ê·œì‹ ì¶”ì¶œë³´ë‹¤ ì •í™•)
            name_from_json = node.get('name')
            if name_from_json:
                extracted_name = name_from_json
            else:
                # fallback: ê¸°ì¡´ ì •ê·œì‹ ì¶”ì¶œ (deprecated)
                extracted_name = self._extract_class_name(code, node_type)
            
            class_key = self._make_class_key(extracted_name, start_line)
            class_name = extracted_name
            class_kind = node_type
            if class_key not in self.classes:
                self.classes[class_key] = ClassInfo(
                    key=class_key,
                    name=extracted_name or class_key,
                    kind=node_type,
                    node_start=start_line,
                    node_end=end_line,
                )
                log_process("ANALYZE", "COLLECT", f"ğŸ“‹ í´ë˜ìŠ¤ ë°œê²¬: {extracted_name} ({node_type}, ë¼ì¸ {start_line}~{end_line})")

        # ìì‹ ë…¸ë“œ ìˆ˜ì§‘
        child_nodes: List[StatementNode] = []
        for ch in children:
            cn = self._visit(ch, None, class_key, class_name, class_kind)
            if cn:
                child_nodes.append(cn)

        # ë¶„ì„ ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨
        analyzable = node_type not in NON_ANALYSIS_TYPES and node_type not in CLASS_TYPES and node_type not in FIELD_TYPES
        token = calculate_code_token(code)

        self._node_id += 1
        st = StatementNode(
            node_id=self._node_id,
            start_line=start_line,
            end_line=end_line,
            node_type=node_type,
            code=code,
            token=token,
            has_children=bool(child_nodes),
            analyzable=analyzable,
            # í†µí•© í•„ë“œ
            unit_key=class_key,
            unit_name=class_name,
            unit_kind=class_kind,
            # AST JSON ë©”íƒ€ë°ì´í„° (ì„ íƒì )
            signature=node.get('signature'),
            modifiers=node.get('modifiers'),
            return_type=node.get('returnType'),
            parameters=node.get('parameters'),
            generic_type=node.get('genericType'),
            extends_type=node.get('extendsType'),
            implements_types=node.get('implementsTypes'),
            field_type=node.get('fieldType'),
            lines=line_entries,
        )
        for c in child_nodes:
            c.parent = st
        st.children.extend(child_nodes)

        # ë¶„ì„ ëŒ€ìƒ ë…¸ë“œ ì¹´ìš´íŠ¸
        # analyzable=Trueì¸ ë…¸ë“œëŠ” ë°°ì¹˜ì— í¬í•¨ë˜ë¯€ë¡œ, completion_eventëŠ” ë°°ì¹˜ ì™„ë£Œ ì‹œì—ë§Œ ì„¤ì •
        # analyzable=Falseì¸ ë…¸ë“œëŠ” ë°°ì¹˜ì— í¬í•¨ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ìˆ˜ì§‘ ì‹œ ì²˜ë¦¬
        if not analyzable and node_type not in CLASS_TYPES:
            # ë°°ì¹˜ì— í¬í•¨ë˜ì§€ ì•ŠëŠ” ë…¸ë“œëŠ” ìˆ˜ì§‘ ì‹œ summary + completion_event ì„¤ì •
            st.summary = st.get_raw_code()
            st.completion_event.set()
        elif analyzable and class_key and class_key in self.classes:
            # í´ë˜ìŠ¤ì— ì†í•œ ë¶„ì„ ëŒ€ìƒ ë…¸ë“œ
            self.classes[class_key].pending_nodes += 1
        # else: analyzable=Trueì´ì§€ë§Œ class_key ì—†ìŒ
        # â†’ ë°°ì¹˜ì—ì„œ LLM ë¶„ì„ í›„ completion_event ì„¤ì •ë¨

        self.nodes.append(st)
        log_process(
            "ANALYZE",
            "COLLECT",
            f"âœ… {node_type} ë…¸ë“œ ìˆ˜ì§‘ ì™„ë£Œ: ë¼ì¸ {start_line}~{end_line}, í† í° {token}, ìì‹ {len(child_nodes)}ê°œ",
        )
        return st


# ==================== AST í”„ë¡œì„¸ì„œ ë³¸ì²´ ====================
class FrameworkAstProcessor(BaseAstProcessor):
    """Framework AST ì²˜ë¦¬ ë° LLM ë¶„ì„ íŒŒì´í”„ë¼ì¸
    
    BaseAstProcessorë¥¼ ìƒì†í•˜ì—¬ ê³µí†µ íŒŒì´í”„ë¼ì¸ ì¬ì‚¬ìš©.
    Framework ì „ìš© ë¡œì§ë§Œ êµ¬í˜„.
    """

    def __init__(
        self,
        antlr_data: dict,
        directory: str,
        file_name: str,
        api_key: str,
        locale: str,
        last_line: int,
    ):
        """Framework Analyzer ì´ˆê¸°í™”
        
        file_contentëŠ” ë” ì´ìƒ í•„ìš”í•˜ì§€ ì•ŠìŒ - AST JSONì˜ code ì†ì„± ì‚¬ìš©.
        """
        super().__init__(
            antlr_data=antlr_data,
            directory=directory,
            file_name=file_name,
            api_key=api_key,
            locale=locale,
            last_line=last_line,
        )
        
        # í•„ë“œ íƒ€ì… ìºì‹œ
        self._field_type_cache: Optional[Dict[str, Dict[str, str]]] = None

    # =========================================================================
    # BaseAstProcessor ì¶”ìƒ ë©”ì„œë“œ êµ¬í˜„
    # =========================================================================
    
    def _collect_nodes(self) -> Tuple[List[StatementNode], Dict[str, ClassInfo]]:
        """AST ìˆ˜ì§‘"""
        collector = StatementCollector(
            self.antlr_data, self.directory, self.file_name
        )
        nodes, classes = collector.collect()
        
        # í•„ë“œ íƒ€ì… ìºì‹œ ì´ˆê¸°í™”
        self._field_type_cache = {key: {} for key in classes} if classes else {}
        
        return nodes, classes

    def _get_excluded_context_types(self) -> Set[str]:
        """ì»¨í…ìŠ¤íŠ¸ ìƒì„±ì—ì„œ ì œì™¸í•  ë…¸ë“œ íƒ€ì…"""
        return CLASS_TYPES

    async def _extract_parent_context(self, skeleton_code: str, ancestor_context: str) -> str:
        """ë¶€ëª¨ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        result = await asyncio.to_thread(
            analyze_parent_context, skeleton_code, ancestor_context, self.api_key, self.locale
        )
        if isinstance(result, dict):
            return result.get("context_summary", "")
        raise ValueError(f"parent_context ê·œì¹™ì´ dictê°€ ì•„ë‹Œ ê°’ì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤: {type(result)}")

    def _build_static_node_queries(self, node: StatementNode) -> List[str]:
        """ì •ì  ë…¸ë“œ ìƒì„± ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        queries: List[str] = []
        label = node.node_type
        
        # name ì†ì„± ê²°ì •
        if label == "FILE":
            node_name = self.file_name
        elif label in CLASS_TYPES and node.unit_name:
            node_name = node.unit_name
        else:
            node_name = f"{label}[{node.start_line}]"
        
        escaped_name = escape_for_cypher(node_name)
        has_children = "true" if node.has_children else "false"
        escaped_code = escape_for_cypher(node.code)

        base_set = [
            f"__cy_n__.endLine = {node.end_line}",
            f"__cy_n__.name = '{escaped_name}'",
            f"__cy_n__.node_code = '{escaped_code}'",
            f"__cy_n__.token = {node.token}",
            f"__cy_n__.has_children = {has_children}",
        ]
        
        # AST JSON ë©”íƒ€ë°ì´í„° ì†ì„± ì¶”ê°€ (ìˆëŠ” ê²½ìš°ë§Œ)
        if node.signature:
            base_set.append(f"__cy_n__.signature = '{escape_for_cypher(node.signature)}'")
        if node.modifiers:
            base_set.append(f"__cy_n__.modifiers = '{escape_for_cypher(node.modifiers)}'")
        if node.return_type:
            base_set.append(f"__cy_n__.returnType = '{escape_for_cypher(node.return_type)}'")
        if node.parameters:
            base_set.append(f"__cy_n__.parameters = '{escape_for_cypher(node.parameters)}'")
        if node.generic_type:
            base_set.append(f"__cy_n__.genericType = '{escape_for_cypher(node.generic_type)}'")
        if node.extends_type:
            base_set.append(f"__cy_n__.extendsType = '{escape_for_cypher(node.extends_type)}'")
        if node.implements_types:
            base_set.append(f"__cy_n__.implementsTypes = '{escape_for_cypher(node.implements_types)}'")
        if node.field_type:
            base_set.append(f"__cy_n__.fieldType = '{escape_for_cypher(node.field_type)}'")

        # CLASS/INTERFACE ë“±: class_nameê³¼ type ì†ì„± ì¶”ê°€
        if label in CLASS_TYPES and node.unit_name:
            base_set.append(f"__cy_n__.class_name = '{escape_for_cypher(node.unit_name)}'")
            base_set.append(f"__cy_n__.type = '{label}'")
        elif node.unit_name:
            base_set.append(f"__cy_n__.class_name = '{escape_for_cypher(node.unit_name)}'")

        if node.has_children:
            # Frameworkìš© preserve_types ì„¤ì •
            preserve_types = INHERITANCE_TYPES | METHOD_TYPES | METHOD_SIGNATURE_TYPES
            escaped_placeholder = escape_for_cypher(node.get_placeholder_code(preserve_types))
            base_set.append(f"__cy_n__.summarized_code = '{escaped_placeholder}'")

        base_set_str = ", ".join(base_set)
        
        # CLASS/INTERFACE/ENUM ë…¸ë“œ: MERGEë¡œ ìƒì„± (ì¤‘ë³µ ë°©ì§€)
        if label in ("CLASS", "INTERFACE", "ENUM") and node.unit_name:
            escaped_class_name = escape_for_cypher(node.unit_name)
            queries.append(
                f"MERGE (__cy_n__:{label} {{class_name: '{escaped_class_name}'}})\n"
                f"SET __cy_n__.startLine = {node.start_line}, __cy_n__.directory = '{escape_for_cypher(self.full_directory)}', __cy_n__.file_name = '{self.file_name}', {base_set_str}\n"
                f"RETURN __cy_n__"
            )
        else:
            queries.append(
                f"MERGE (__cy_n__:{label} {{startLine: {node.start_line}, {self.node_base_props}}})\n"
                f"SET {base_set_str}\n"
                f"RETURN __cy_n__"
            )
        return queries

    def _build_relationship_queries(self) -> List[str]:
        """ì •ì  ê´€ê³„ ì¿¼ë¦¬ (HAS_METHOD, HAS_FIELD, CONTAINS, PARENT_OF)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        queries: List[str] = []
        
        for node in self._nodes or []:
            if not node.parent:
                continue
            
            parent = node.parent
            
            # File â†’ ìµœìƒìœ„ íƒ€ì…ë§Œ CONTAINS
            if parent.node_type == "FILE" and node.node_type in CLASS_TYPES:
                queries.append(self._build_contains_query(parent, node))
            # Class â†’ Method: HAS_METHOD
            elif node.node_type in METHOD_TYPES:
                queries.append(self._build_has_method_query(parent, node))
            # Class â†’ Field: HAS_FIELD
            elif node.node_type in FIELD_TYPES:
                queries.append(self._build_has_field_query(parent, node))
            # ê·¸ ì™¸: PARENT_OF
            else:
                queries.append(self._build_parent_of_query(parent, node))
        
        return queries
    
    def _build_has_method_query(self, parent: StatementNode, child: StatementNode) -> str:
        """HAS_METHOD ê´€ê³„ ì¿¼ë¦¬"""
        return (
            f"MATCH (__cy_p__:{parent.node_type} {{startLine: {parent.start_line}, {self.node_base_props}}})\n"
            f"MATCH (__cy_c__:{child.node_type} {{startLine: {child.start_line}, {self.node_base_props}}})\n"
            f"MERGE (__cy_p__)-[__cy_r__:HAS_METHOD]->(__cy_c__)\n"
            f"RETURN __cy_r__"
        )
    
    def _build_has_field_query(self, parent: StatementNode, child: StatementNode) -> str:
        """HAS_FIELD ê´€ê³„ ì¿¼ë¦¬"""
        return (
            f"MATCH (__cy_p__:{parent.node_type} {{startLine: {parent.start_line}, {self.node_base_props}}})\n"
            f"MATCH (__cy_c__:{child.node_type} {{startLine: {child.start_line}, {self.node_base_props}}})\n"
            f"MERGE (__cy_p__)-[__cy_r__:HAS_FIELD]->(__cy_c__)\n"
            f"RETURN __cy_r__"
        )
    
    async def _run_preprocessing(self) -> List[str]:
        """ì„ í–‰ ì²˜ë¦¬: ìƒì†/êµ¬í˜„, í•„ë“œ, ë©”ì„œë“œ ë¶„ì„"""
        queries: List[str] = []
        
        # ìƒì†/êµ¬í˜„, í•„ë“œ, ë©”ì„œë“œ ë…¸ë“œ ë¶„ë¥˜
        inheritance_nodes = []
        field_nodes = []
        method_nodes = []
            
        for node in self._nodes or []:
            if node.node_type in INHERITANCE_TYPES:
                inheritance_nodes.append(node)
            elif node.node_type in FIELD_TYPES:
                field_nodes.append(node)
            elif node.node_type in METHOD_SIGNATURE_TYPES:
                method_nodes.append(node)
        
        # ë³‘ë ¬ ì²˜ë¦¬
        tasks = []
        if inheritance_nodes:
            tasks.append(self._analyze_inheritance_nodes(inheritance_nodes))
        if field_nodes:
            tasks.append(self._analyze_field_nodes(field_nodes))
        if method_nodes:
            tasks.append(self._analyze_method_nodes(method_nodes))
        
        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            for result in results:
                if isinstance(result, list):
                    queries.extend(result)
                elif isinstance(result, Exception):
                    # ì„ í–‰ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨
                    raise RuntimeError(f"ì„ í–‰ ì²˜ë¦¬ ì‹¤íŒ¨: {result}") from result
        
        return queries

    async def _invoke_llm(self, batch: AnalysisBatch) -> Optional[Dict[str, Any]]:
        """LLM í˜¸ì¶œ (ì¼ë°˜ ë¶„ì„ë§Œ)"""
        if not batch.ranges:
            raise RuntimeError(f"ë°°ì¹˜ #{batch.batch_id}ì— ë¶„ì„í•  ë²”ìœ„ê°€ ì—†ìŠµë‹ˆë‹¤")

        code, context = batch.build_payload()
        result = await asyncio.to_thread(
            analyze_code,
            code,
            context,
            batch.ranges,
            len(batch.ranges),
            self.api_key,
            self.locale,
        )
        return result

    def _build_analysis_queries(
        self, 
        batch: AnalysisBatch, 
        llm_result: Any,
        unit_summary_store: Optional[Dict[str, Dict[str, str]]] = None,
    ) -> List[str]:
        """LLM ë¶„ì„ ê²°ê³¼ë¥¼ ì¿¼ë¦¬ë¡œ ë³€í™˜"""
        queries: List[str] = []
        
        # íƒ€ì… ê²€ì¦ (ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ë°œìƒ â†’ ì „ì²´ ë¶„ì„ ì¤‘ë‹¨)
        llm_result = self.validate_dict_result(llm_result, "llm_result", batch.batch_id)
        analysis_list = llm_result.get("analysis") or []
        
        for node, analysis in zip(batch.nodes, analysis_list):
            if not analysis:
                continue
            
            # ìš”ì•½ ì—…ë°ì´íŠ¸
            summary = analysis.get("summary") or ""
            if summary:
                escaped_summary = escape_for_cypher(str(summary))
                queries.append(
                    f"MATCH (__cy_n__:{node.node_type} {{startLine: {node.start_line}, {self.node_base_props}}})\n"
                    f"SET __cy_n__.summary = '{escaped_summary}'\n"
                    f"RETURN __cy_n__"
                )
                
                # í´ë˜ìŠ¤ë³„ summary ì €ì¥
                if unit_summary_store is not None and node.unit_key:
                    if node.unit_key in unit_summary_store:
                        key = f"{node.node_type}_{node.start_line}_{node.end_line}"
                        unit_summary_store[node.unit_key][key] = summary
            
            # DEPENDENCY ê´€ê³„ (localDependencies)
            for dep in analysis.get("localDependencies", []) or []:
                if not dep:
                    continue
                dep_type = dep.get("type", "") if isinstance(dep, dict) else str(dep)
                if not dep_type or not _is_valid_class_name_for_calls(dep_type):
                    continue
                source_member = dep.get("sourceMember", "unknown") if isinstance(dep, dict) else "unknown"
                
                if not node.unit_kind or not node.parent:
                    continue
                
                queries.append(
                    f"MATCH (__cy_src__:{node.unit_kind} {{startLine: {node.parent.start_line}, {self.node_base_props}}})\n"
                    f"MATCH (__cy_dst__) WHERE (__cy_dst__:CLASS OR __cy_dst__:INTERFACE OR __cy_dst__:ENUM)\n"
                    f"  AND toLower(__cy_dst__.class_name) = toLower('{escape_for_cypher(dep_type)}')\n"
                    f"  AND __cy_src__ <> __cy_dst__ AND NOT (__cy_src__)-[:ASSOCIATION|COMPOSITION]->(__cy_dst__)\n"
                    f"MERGE (__cy_src__)-[__cy_r__:DEPENDENCY {{usage: 'local', source_member: '{escape_for_cypher(source_member)}'}}]->(__cy_dst__)\n"
                    f"RETURN __cy_r__"
                )
            
            # CALLS ê´€ê³„
            for call_str in analysis.get("calls", []) or []:
                if not call_str or not isinstance(call_str, str):
                    continue
                parts = call_str.split(".", 1)
                if len(parts) != 2:
                    continue
                target_class, method_name = parts
                
                if not _is_valid_class_name_for_calls(target_class):
                    continue
                
                if node.unit_kind and node.parent:
                    queries.append(
                        f"MATCH (__cy_src__:{node.unit_kind} {{startLine: {node.parent.start_line}, {self.node_base_props}}})\n"
                        f"MATCH (__cy_dst__) WHERE (__cy_dst__:CLASS OR __cy_dst__:INTERFACE OR __cy_dst__:ENUM)\n"
                        f"  AND toLower(__cy_dst__.class_name) = toLower('{escape_for_cypher(target_class)}')\n"
                        f"MERGE (__cy_src__)-[__cy_r__:CALLS {{method: '{escape_for_cypher(method_name)}'}}]->(__cy_dst__)\n"
                        f"RETURN __cy_r__"
                    )
        
        return queries

    async def _process_unit_summaries(
        self, 
        unit_summary_store: Dict[str, Dict[str, str]]
    ) -> List[str]:
        """í´ë˜ìŠ¤ë³„ summary ì²˜ë¦¬"""
        queries: List[str] = []
        
        classes = self._unit_info
        if not classes:
            return queries
        
        for class_key, info in classes.items():
            summaries = unit_summary_store.get(class_key, {})
            if not summaries:
                continue
            
            # í´ë˜ìŠ¤ ë…¸ë“œ ì°¾ê¸°
            class_node = next(
                (n for n in self._nodes if n.start_line == info.node_start and n.node_type == info.kind),
                None,
            )
            if not class_node:
                continue
            
            if not class_node.ok:
                log_process("ANALYZE", "SUMMARY", f"âš ï¸ {info.name}: í•˜ìœ„ ë¶„ì„ ì‹¤íŒ¨ë¡œ ìµœì¢… summary ìƒì„± ìŠ¤í‚µ")
                continue
            
            all_user_stories: List[Dict[str, Any]] = []
            final_summary = ""
            
            # ì²­í¬ ë¶„í• 
            chunks = self._split_summaries_by_token(summaries, MAX_SUMMARY_CHUNK_TOKEN)
            if not chunks:
                continue
            
            log_process("ANALYZE", "SUMMARY", f"ğŸ“¦ {info.name}: summary ì²­í¬ ë¶„í•  ì™„ë£Œ ({len(chunks)}ê°œ ì²­í¬)")
            
            # ì²­í¬ë³„ ì²˜ë¦¬ (ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ë°œìƒ â†’ ì „ì²´ ë¶„ì„ ì¤‘ë‹¨)
            async def process_chunk(chunk_idx: int, chunk: dict) -> str:
                chunk_tokens = calculate_code_token(json.dumps(chunk, ensure_ascii=False))
                log_process("ANALYZE", "SUMMARY", f"  â†’ ì²­í¬ {chunk_idx + 1}/{len(chunks)} ì²˜ë¦¬ ì‹œì‘ (í† í°: {chunk_tokens})")
                
                summary_result = await asyncio.to_thread(
                    analyze_class_summary_only, chunk, self.api_key, self.locale, ""
                )
                validated = self.validate_dict_result(summary_result, "ì²­í¬ ë¶„ì„")
                return validated.get('summary', '')
            
            chunk_results_raw = await asyncio.gather(
                *[process_chunk(idx, chunk) for idx, chunk in enumerate(chunks)]
            )
            
            chunk_results = [r for r in chunk_results_raw if r]
            
            if not chunk_results:
                raise RuntimeError(f"{info.name}: ì²­í¬ ì²˜ë¦¬ ê²°ê³¼ê°€ ëª¨ë‘ ë¹„ì–´ìˆìŒ")
            
            # ì²­í¬ í†µí•©
            if len(chunk_results) == 1:
                final_summary = chunk_results[0]
            else:
                combined_summaries = {f"CHUNK_{idx + 1}": s for idx, s in enumerate(chunk_results)}
                result = await asyncio.to_thread(
                    analyze_class_summary_only, combined_summaries, self.api_key, self.locale, ""
                )
                validated = self.validate_dict_result(result, "ì²­í¬ í†µí•©")
                final_summary = validated.get('summary') or "\n\n".join(chunk_results)
            
            log_process("ANALYZE", "SUMMARY", f"âœ… {info.name}: summary í†µí•© ì™„ë£Œ")
            
            # User Story ìƒì„± (ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ë°œìƒ)
            if final_summary:
                us_result = await asyncio.to_thread(
                    analyze_class_user_story, final_summary, self.api_key, self.locale
                )
                validated = self.validate_dict_result(us_result, "User Story")
                all_user_stories = validated.get('user_stories', []) or []
            
            if all_user_stories:
                log_process("ANALYZE", "SUMMARY", f"âœ… {info.name}: User Story {len(all_user_stories)}ê°œ")
            else:
                log_process("ANALYZE", "SUMMARY", f"âœ… {info.name}: User Story ì—†ìŒ")
            
            if not final_summary:
                continue
            
            # Neo4j ì¿¼ë¦¬ ìƒì„±
            escaped_summary = escape_for_cypher(str(final_summary))
            
            queries.append(
                f"MATCH (__cy_n__:{info.kind} {{startLine: {info.node_start}, {self.node_base_props}}})\n"
                f"SET __cy_n__.summary = '{escaped_summary}'\n"
                f"RETURN __cy_n__"
            )
            
            # User Story ë…¸ë“œ ìƒì„±
            if all_user_stories:
                class_name_escaped = escape_for_cypher(info.name)
                for us_idx, us in enumerate(all_user_stories, 1):
                    us_id = us.get('id', f"US-{us_idx}")
                    role = escape_for_cypher(us.get('role', ''))
                    goal = escape_for_cypher(us.get('goal', ''))
                    benefit = escape_for_cypher(us.get('benefit', ''))
                    
                    queries.append(
                        f"MATCH (__cy_c__:{info.kind} {{startLine: {info.node_start}, {self.node_base_props}}})\n"
                        f"MERGE (__cy_us__:UserStory {{id: '{escape_for_cypher(us_id)}', class_name: '{class_name_escaped}', {self.node_base_props}}})\n"
                        f"SET __cy_us__.role = '{role}', __cy_us__.goal = '{goal}', __cy_us__.benefit = '{benefit}'\n"
                        f"MERGE (__cy_c__)-[:HAS_USER_STORY]->(__cy_us__)\n"
                        f"RETURN __cy_us__"
                    )
                    
                    for ac_idx, ac in enumerate(us.get('acceptance_criteria', []) or [], 1):
                        ac_id = ac.get('id', f"AC-{us_idx}-{ac_idx}")
                        ac_title = escape_for_cypher(ac.get('title', ''))
                        ac_given = escape_for_cypher(ac.get('given', ''))
                        ac_when = escape_for_cypher(ac.get('when', ''))
                        ac_then = escape_for_cypher(ac.get('then', ''))
                        
                        queries.append(
                            f"MATCH (__cy_us__:UserStory {{id: '{escape_for_cypher(us_id)}', class_name: '{class_name_escaped}', {self.node_base_props}}})\n"
                            f"MERGE (__cy_ac__:AcceptanceCriteria {{id: '{escape_for_cypher(ac_id)}', user_story_id: '{escape_for_cypher(us_id)}', {self.node_base_props}}})\n"
                            f"SET __cy_ac__.title = '{ac_title}', __cy_ac__.given = '{ac_given}', __cy_ac__.when = '{ac_when}', __cy_ac__.then = '{ac_then}'\n"
                            f"MERGE (__cy_us__)-[:HAS_AC]->(__cy_ac__)\n"
                            f"RETURN __cy_ac__"
                        )
        
        return queries
    
    # =========================================================================
    # Framework ì „ìš© ë©”ì„œë“œ
    # =========================================================================

    async def _analyze_inheritance_nodes(self, nodes: List[StatementNode]) -> List[str]:
        """ìƒì†/êµ¬í˜„ ë…¸ë“œ ë¶„ì„"""
        queries: List[str] = []
        semaphore = asyncio.Semaphore(INHERITANCE_CONCURRENCY)
        
        async def analyze_one(node: StatementNode) -> List[str]:
            async with semaphore:
                try:
                    result = await asyncio.to_thread(
                        analyze_inheritance, node.code, self.api_key, self.locale
                    )
                    return self._build_inheritance_queries(node, result)
                except Exception as e:
                    log_process("ANALYZE", "INHERITANCE", f"âŒ ìƒì† ë¶„ì„ ì‹¤íŒ¨ (node={node.start_line}): {e}", logging.ERROR, e)
                    raise
        
        results = await asyncio.gather(*[analyze_one(n) for n in nodes])
        for r in results:
            queries.extend(r)
        
        return queries

    async def _analyze_field_nodes(self, nodes: List[StatementNode]) -> List[str]:
        """í•„ë“œ ë…¸ë“œ ë¶„ì„"""
        queries: List[str] = []
        semaphore = asyncio.Semaphore(FIELD_CONCURRENCY)
        
        async def analyze_one(node: StatementNode) -> List[str]:
            async with semaphore:
                try:
                    result = await asyncio.to_thread(
                        analyze_field, node.code, self.api_key, self.locale
                    )
                    return self._build_field_queries(node, result)
                except Exception as e:
                    log_process("ANALYZE", "FIELD", f"âŒ í•„ë“œ ë¶„ì„ ì‹¤íŒ¨ (node={node.start_line}): {e}", logging.ERROR, e)
                    raise
        
        results = await asyncio.gather(*[analyze_one(n) for n in nodes])
        for r in results:
            queries.extend(r)
        
        return queries

    async def _analyze_method_nodes(self, nodes: List[StatementNode]) -> List[str]:
        """ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ë¶„ì„"""
        queries: List[str] = []
        semaphore = asyncio.Semaphore(METHOD_CONCURRENCY)
        
        async def analyze_one(node: StatementNode) -> List[str]:
            async with semaphore:
                try:
                    result = await asyncio.to_thread(
                        analyze_method, node.code, self.api_key, self.locale
                    )
                    return self._build_method_queries(node, result)
                except Exception as e:
                    log_process("ANALYZE", "METHOD", f"âŒ ë©”ì„œë“œ ë¶„ì„ ì‹¤íŒ¨ (node={node.start_line}): {e}", logging.ERROR, e)
                    raise
        
        results = await asyncio.gather(*[analyze_one(n) for n in nodes])
        for r in results:
            queries.extend(r)
        
        return queries

    def _build_inheritance_queries(self, node: StatementNode, analysis: Dict[str, Any]) -> List[str]:
        """ìƒì†/êµ¬í˜„ ë¶„ì„ ê²°ê³¼ë¥¼ ì¿¼ë¦¬ë¡œ ë³€í™˜"""
        if not isinstance(analysis, dict):
            raise RuntimeError(f"ìƒì† ë¶„ì„ ê²°ê³¼ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (node={node.start_line}): {type(analysis)}")

        queries: List[str] = []
        relations = analysis.get("relations") or []

        for rel in relations:
            to_type = escape_for_cypher(rel.get("toType") or "")
            rel_type = rel.get("relationType") or "EXTENDS"

            if not to_type:
                continue

            src_match = f"MATCH (__cy_src__:{node.unit_kind or 'CLASS'} {{startLine: {node.parent.start_line if node.parent else node.start_line}, {self.node_base_props}}})"

            queries.append(
                f"{src_match}\n"
                f"MATCH (__cy_dst__) WHERE (__cy_dst__:CLASS OR __cy_dst__:INTERFACE OR __cy_dst__:ENUM)\n"
                f"  AND toLower(__cy_dst__.class_name) = toLower('{to_type}')\n"
                f"MERGE (__cy_src__)-[__cy_r__:{rel_type}]->(__cy_dst__)\n"
                f"RETURN __cy_src__, __cy_dst__, __cy_r__"
            )

        return queries

    def _build_field_queries(self, node: StatementNode, analysis: Dict[str, Any]) -> List[str]:
        """í•„ë“œ ë¶„ì„ ê²°ê³¼ë¥¼ ì¿¼ë¦¬ë¡œ ë³€í™˜"""
        if not isinstance(analysis, dict):
            raise RuntimeError(f"í•„ë“œ ë¶„ì„ ê²°ê³¼ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (node={node.start_line}): {type(analysis)}")

        queries: List[str] = []
        fields = analysis.get("fields") or []

        for field_info in fields:
            field_name = escape_for_cypher(field_info.get("field_name") or "")
            field_type_raw = field_info.get("field_type") or ""
            field_type = escape_for_cypher(field_type_raw)
            target_class_raw = field_info.get("target_class")
            target_class = escape_for_cypher(target_class_raw) if target_class_raw else None
            visibility = escape_for_cypher(field_info.get("visibility") or "private")
            is_static = "true" if field_info.get("is_static") else "false"
            is_final = "true" if field_info.get("is_final") else "false"
            multiplicity = escape_for_cypher(field_info.get("multiplicity") or "1")
            association_type = field_info.get("association_type") or "ASSOCIATION"

            if not field_name:
                continue

            # í•„ë“œ íƒ€ì… ìºì‹œ ì—…ë°ì´íŠ¸
            if node.unit_key and self._field_type_cache and node.unit_key in self._field_type_cache:
                original_field_name = field_info.get("field_name") or ""
                self._field_type_cache[node.unit_key][original_field_name] = field_type_raw

            target_class_set = f", __cy_f__.target_class = '{target_class}'" if target_class else ""
            queries.append(
                f"MATCH (__cy_f__:FIELD {{startLine: {node.start_line}, {self.node_base_props}}})\n"
                f"SET __cy_f__.name = '{field_name}', __cy_f__.field_type = '{field_type}', "
                f"__cy_f__.visibility = '{visibility}', __cy_f__.is_static = {is_static}, __cy_f__.is_final = {is_final}{target_class_set}\n"
                f"RETURN __cy_f__"
            )

            if target_class:
                src_match = f"MATCH (__cy_src__:{node.unit_kind or 'CLASS'} {{startLine: {node.parent.start_line if node.parent else node.start_line}, {self.node_base_props}}})"
                queries.append(
                    f"{src_match}\n"
                    f"MATCH (__cy_dst__) WHERE (__cy_dst__:CLASS OR __cy_dst__:INTERFACE OR __cy_dst__:ENUM)\n"
                    f"  AND toLower(__cy_dst__.class_name) = toLower('{target_class}')\n"
                    f"MERGE (__cy_src__)-[__cy_r__:{association_type} {{source_member: '{field_name}', multiplicity: '{multiplicity}'}}]->(__cy_dst__)\n"
                    f"RETURN __cy_src__, __cy_dst__, __cy_r__"
                )

        return queries

    def _build_method_queries(self, node: StatementNode, analysis: Dict[str, Any]) -> List[str]:
        """ë©”ì„œë“œ ë¶„ì„ ê²°ê³¼ë¥¼ ì¿¼ë¦¬ë¡œ ë³€í™˜"""
        if not isinstance(analysis, dict):
            raise RuntimeError(f"ë©”ì„œë“œ ë¶„ì„ ê²°ê³¼ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (node={node.start_line}): {type(analysis)}")

        queries: List[str] = []
        
        method_name = escape_for_cypher(analysis.get("method_name") or "")
        return_type = escape_for_cypher(analysis.get("return_type") or "void")
        visibility = escape_for_cypher(analysis.get("visibility") or "public")
        is_static = "true" if analysis.get("is_static") else "false"
        method_kind = escape_for_cypher(analysis.get("method_type") or "normal")
        parameters = analysis.get("parameters") or []
        dependencies = analysis.get("dependencies") or []

        queries.append(
            f"MATCH (__cy_m__:{node.node_type} {{startLine: {node.start_line}, {self.node_base_props}}})\n"
            f"SET __cy_m__.name = '{method_name}', __cy_m__.return_type = '{return_type}', "
            f"__cy_m__.visibility = '{visibility}', __cy_m__.is_static = {is_static}, "
            f"__cy_m__.method_type = '{method_kind}'\n"
            f"RETURN __cy_m__"
        )

        # íŒŒë¼ë¯¸í„° ë…¸ë“œ
        for idx, param in enumerate(parameters):
            param_name = escape_for_cypher(param.get("name") or "")
            param_type = escape_for_cypher(param.get("type") or "")
            if not param_name:
                continue
            queries.append(
                f"MATCH (__cy_m__:{node.node_type} {{startLine: {node.start_line}, {self.node_base_props}}})\n"
                f"MERGE (__cy_p__:Parameter {{name: '{param_name}', method_start_line: {node.start_line}, {self.node_base_props}}})\n"
                f"SET __cy_p__.type = '{param_type}', __cy_p__.index = {idx}\n"
                f"MERGE (__cy_m__)-[__cy_r__:HAS_PARAMETER]->(__cy_p__)\n"
                f"RETURN __cy_m__, __cy_p__, __cy_r__"
            )

        # DEPENDENCY ê´€ê³„
        for dep in dependencies:
            target_type = escape_for_cypher(dep.get("target_class") or "")
            usage = escape_for_cypher(dep.get("usage") or "parameter")
            is_value_object_cypher = "true" if dep.get("is_value_object") else "false"

            if not target_type:
                continue

            src_match = f"MATCH (__cy_src__:{node.unit_kind or 'CLASS'} {{startLine: {node.parent.start_line if node.parent else node.start_line}, {self.node_base_props}}})"
            queries.append(
                f"{src_match}\n"
                f"MATCH (__cy_dst__) WHERE (__cy_dst__:CLASS OR __cy_dst__:INTERFACE OR __cy_dst__:ENUM)\n"
                f"  AND toLower(__cy_dst__.class_name) = toLower('{target_type}')\n"
                f"  AND __cy_src__ <> __cy_dst__\n"
                f"  AND NOT (__cy_src__)-[:ASSOCIATION|COMPOSITION]->(__cy_dst__)\n"
                f"MERGE (__cy_src__)-[__cy_r__:DEPENDENCY {{usage: '{usage}', source_member: '{method_name}'}}]->(__cy_dst__)\n"
                f"SET __cy_r__.is_value_object = {is_value_object_cypher}\n"
                f"RETURN __cy_src__, __cy_dst__, __cy_r__"
            )

        # í•„ë“œ í• ë‹¹ íŒ¨í„´
        field_assignments = analysis.get("field_assignments") or []
        src_start_line = node.parent.start_line if node.parent else node.start_line
        for assign in field_assignments:
            field_name = escape_for_cypher(assign.get("field_name") or "")
            value_source = assign.get("value_source") or ""

            if not field_name or not value_source:
                continue

            if value_source == "new":
                queries.append(
                    f"MATCH (__cy_field__:FIELD {{name: '{field_name}', {self.node_base_props}}})\n"
                    f"WHERE __cy_field__.target_class IS NOT NULL\n"
                    f"MATCH (__cy_src__:{node.unit_kind or 'CLASS'} {{startLine: {src_start_line}, {self.node_base_props}}})"
                    f"-[__cy_r__:ASSOCIATION {{source_member: '{field_name}'}}]->(__cy_dst__)\n"
                    f"WITH __cy_src__, __cy_dst__, COALESCE(__cy_r__.multiplicity, '1') AS mult, __cy_r__\n"
                    f"DELETE __cy_r__\n"
                    f"MERGE (__cy_src__)-[__cy_r2__:COMPOSITION {{source_member: '{field_name}', multiplicity: mult}}]->(__cy_dst__)\n"
                    f"RETURN __cy_src__, __cy_dst__, __cy_r2__"
                )

        return queries
