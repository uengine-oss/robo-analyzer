"""Framework ì½”ë“œ ë¶„ì„ ì „ëµ - Java, Kotlin ë“±

AST ê¸°ë°˜ Java ì½”ë“œ ë¶„ì„ â†’ Neo4j í´ë˜ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨ ê·¸ë˜í”„ ìƒì„±.

ë¶„ì„ íë¦„ (2ë‹¨ê³„ + ì´ì¤‘ ë³‘ë ¬):
1. [Phase 1] ëª¨ë“  íŒŒì¼ AST ê·¸ë˜í”„ ìƒì„± (ë³‘ë ¬)
   - ì •ì  ë…¸ë“œ ìƒì„±: CLASS, INTERFACE, METHOD, FIELD
   - ì •ì  ê´€ê³„ ìƒì„±: HAS_METHOD, HAS_FIELD, CONTAINS
   
2. [Phase 2] ëª¨ë“  íŒŒì¼ LLM ë¶„ì„ (íŒŒì¼ ë³‘ë ¬ + ì²­í¬ ë³‘ë ¬)
   - ì½”ë“œ ìš”ì•½ ë° ë¶„ì„
   - CALLS ê´€ê³„ ìƒì„± (MATCHë¡œ ê¸°ì¡´ ë…¸ë“œ ì¡°íšŒ)
   - DEPENDENCY ê´€ê³„ ìƒì„±
   
3. [Phase 3] User Story ë¬¸ì„œ ìƒì„± (BaseStreamingAnalyzer ê³µí†µ)

íŒŒì¼ ìƒíƒœ ê´€ë¦¬:
- Phase1 ì‹¤íŒ¨ íŒŒì¼ì€ Phase2 ìŠ¤í‚µ (í† í° ì ˆê°)
- íŒŒì¼ë³„ SUCCESS/FAILED/SKIPPED ìƒíƒœ ì¶”ì 

Phase ë¡œì§ì€ ê° phase íŒŒì¼ì— ë¶„ë¦¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤:
- ast_phase.py: AST ê·¸ë˜í”„ ìƒì„± (Phase 1)
- llm_phase.py: LLM ë¶„ì„ (Phase 2)
"""

import asyncio
import json
import logging
import os
from typing import Any, AsyncGenerator, Optional, List

import aiofiles

from analyzer.neo4j_client import Neo4jClient
from analyzer.strategy.base_analyzer import BaseStreamingAnalyzer, AnalysisStats
from analyzer.strategy.base.file_context import FileStatus, FileAnalysisContext
from config.settings import settings
from util.stream_event import emit_message
from util.text_utils import (
    generate_user_story_document,
    log_process,
)

# Phase íŒŒì¼ë“¤ì—ì„œ import
from analyzer.strategy.framework.ast_phase import run_phase1
from analyzer.strategy.framework.llm_phase import run_phase2


class FrameworkAnalyzer(BaseStreamingAnalyzer):
    """Java/Framework ì½”ë“œ ë¶„ì„ ì „ëµ
    
    2ë‹¨ê³„ ë¶„ì„ + ì´ì¤‘ ë³‘ë ¬ ì²˜ë¦¬:
    - Phase 1: ëª¨ë“  íŒŒì¼ AST ê·¸ë˜í”„ ìƒì„± (ë³‘ë ¬)
    - Phase 2: ëª¨ë“  íŒŒì¼ LLM ë¶„ì„ (ë³‘ë ¬) - Phase1 ì‹¤íŒ¨ íŒŒì¼ ì œì™¸
    - Phase 3: User Story ë¬¸ì„œ ìƒì„± (ë¶€ëª¨ í´ë˜ìŠ¤ ê³µí†µ)
    
    íŒŒì´í”„ë¼ì¸ íŠ¹ì„±:
    - ë³‘ë ¬ ì²˜ë¦¬: íŒŒì¼ ë‹¨ìœ„ë¡œ ë™ì‹œ ë¶„ì„
    - ë™ì‹œì„± ë³´í˜¸: Cypher ì¿¼ë¦¬ ë½ ì‚¬ìš©
    - í”„ë¡œì„¸ì„œ ì¬ì‚¬ìš©: Phase 1ì—ì„œ ìƒì„±í•œ í”„ë¡œì„¸ì„œë¥¼ Phase 2ì—ì„œ ì¬ì‚¬ìš©
    - í† í° ì ˆê°: Phase1 ì‹¤íŒ¨ íŒŒì¼ì€ Phase2 ìŠ¤í‚µ
    """

    # =========================================================================
    # ì „ëµ ë©”íƒ€ë°ì´í„° (BaseStreamingAnalyzer êµ¬í˜„)
    # =========================================================================
    
    @property
    def strategy_name(self) -> str:
        return "í”„ë ˆì„ì›Œí¬"
    
    @property
    def strategy_emoji(self) -> str:
        return "ğŸš€"
    
    @property
    def file_type_description(self) -> str:
        return "Java/Kotlin íŒŒì¼"

    def __init__(self):
        self._cypher_lock = asyncio.Lock()  # Cypher ì¿¼ë¦¬ ë™ì‹œì„± ë³´í˜¸
        self._file_semaphore: Optional[asyncio.Semaphore] = None

    # =========================================================================
    # ë©”ì¸ íŒŒì´í”„ë¼ì¸ (BaseStreamingAnalyzer êµ¬í˜„)
    # =========================================================================

    async def run_pipeline(
        self,
        file_names: list[tuple[str, str]],
        client: Neo4jClient,
        orchestrator: Any,
        stats: AnalysisStats,
    ) -> AsyncGenerator[bytes, None]:
        """Framework ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        íë¦„:
        1. íŒŒì¼ ë¡œë“œ (ë³‘ë ¬)
        2. Phase 1: AST ê·¸ë˜í”„ ìƒì„± (ë³‘ë ¬)
        3. Phase 2: LLM ë¶„ì„ (ë³‘ë ¬) - Phase1 ì‹¤íŒ¨ íŒŒì¼ ì œì™¸ (í† í° ì ˆê°)
        
        Note: User Story PhaseëŠ” ë¶€ëª¨ í´ë˜ìŠ¤ì—ì„œ ì²˜ë¦¬
        """
        total_files = len(file_names)
        self._file_semaphore = asyncio.Semaphore(settings.concurrency.file_concurrency)

        # LLM ìºì‹œ ìƒíƒœ í‘œì‹œ
        if settings.llm.cache_enabled:
            cache_path = settings.llm.cache_db_path
            if not os.path.isabs(cache_path):
                cache_path = os.path.join(settings.path.base_dir, cache_path)
            cache_exists = os.path.exists(cache_path)
            cache_size = os.path.getsize(cache_path) if cache_exists else 0
            cache_size_str = f"{cache_size / 1024:.1f}KB" if cache_size < 1024*1024 else f"{cache_size / (1024*1024):.1f}MB"
            yield emit_message(f"ğŸ—„ï¸ LLM ìºì‹œ: í™œì„±í™” ({cache_size_str if cache_exists else 'ì‹ ê·œ'})")
        else:
            yield emit_message("ğŸ”„ LLM ìºì‹œ: ë¹„í™œì„±í™” (ë§¤ë²ˆ ìƒˆë¡œìš´ LLM í˜¸ì¶œ)")

        yield emit_message(f"âš¡ ë³‘ë ¬ ì²˜ë¦¬: íŒŒì¼ {settings.concurrency.file_concurrency}ê°œ ë™ì‹œ")

        # ========== íŒŒì¼ ë¡œë“œ ==========
        yield emit_message("")
        yield self.emit_separator()
        yield self.emit_phase_header(1, "ğŸ—ï¸ AST êµ¬ì¡° ê·¸ë˜í”„ ìƒì„±", f"{total_files}ê°œ íŒŒì¼ ë³‘ë ¬")
        yield self.emit_separator()

        contexts = await self._load_all_files(file_names, orchestrator)
        yield emit_message(f"   âœ“ {len(contexts)}ê°œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ")

        # ========== Phase 1: AST ê·¸ë˜í”„ ìƒì„± (ë³‘ë ¬) ==========
        async for chunk in run_phase1(self, contexts, client, orchestrator, stats):
            yield chunk

        # Phase 1 ê²°ê³¼ ìš”ì•½
        ph1_ok_count = sum(1 for c in contexts if c.status == FileStatus.PH1_OK)
        ph1_fail_count = sum(1 for c in contexts if c.status == FileStatus.PH1_FAIL)
        
        yield emit_message("")
        yield self.emit_phase_complete(1, f"{stats.static_nodes_created}ê°œ ë…¸ë“œ ìƒì„±")
        if ph1_fail_count > 0:
            yield self.emit_warning(f"Phase 1 ì‹¤íŒ¨: {ph1_fail_count}ê°œ íŒŒì¼ â†’ Phase 2 ìŠ¤í‚µ (í† í° ì ˆê°)")

        # ========== Phase 2: LLM ë¶„ì„ (ë³‘ë ¬) - Phase1 ì„±ê³µ íŒŒì¼ë§Œ ==========
        ph2_targets = [c for c in contexts if c.status == FileStatus.PH1_OK]
        
        yield emit_message("")
        yield self.emit_separator()
        yield self.emit_phase_header(2, "ğŸ¤– AI ë¶„ì„", f"{len(ph2_targets)}ê°œ íŒŒì¼ ë³‘ë ¬")
        yield self.emit_separator()
        
        if ph1_fail_count > 0:
            yield emit_message(f"   â„¹ï¸ {ph1_fail_count}ê°œ íŒŒì¼ì€ Phase 1 ì‹¤íŒ¨ë¡œ ìŠ¤í‚µë¨ (í† í° ì ˆê°)")

        async for chunk in run_phase2(self, ph2_targets, client, orchestrator, stats):
            yield chunk

        yield emit_message("")
        yield self.emit_phase_complete(2, f"{stats.llm_batches_executed}ê°œ ë¶„ì„ ì™„ë£Œ")

    # =========================================================================
    # User Story ë¬¸ì„œ ìƒì„± (BaseStreamingAnalyzer êµ¬í˜„)
    # =========================================================================

    async def build_user_story_doc(
        self,
        client: Neo4jClient,
        orchestrator: Any,
    ) -> Optional[str]:
        """ë¶„ì„ëœ í´ë˜ìŠ¤/ì¸í„°í˜ì´ìŠ¤ì—ì„œ User Story ë¬¸ì„œ ìƒì„±"""
        query = """
            MATCH (__cy_n__)
            WHERE (__cy_n__:CLASS OR __cy_n__:INTERFACE)
              AND __cy_n__.summary IS NOT NULL
            OPTIONAL MATCH (__cy_n__)-[:HAS_USER_STORY]->(__cy_us__:UserStory)
            OPTIONAL MATCH (__cy_us__)-[:HAS_AC]->(__cy_ac__:AcceptanceCriteria)
            WITH __cy_n__, __cy_us__, collect(DISTINCT {
                id: __cy_ac__.id,
                title: __cy_ac__.title,
                given: __cy_ac__.given,
                when: __cy_ac__.when,
                then: __cy_ac__.then
            }) AS acceptance_criteria
            WITH __cy_n__, collect(DISTINCT {
                id: __cy_us__.id,
                role: __cy_us__.role,
                goal: __cy_us__.goal,
                benefit: __cy_us__.benefit,
                acceptance_criteria: acceptance_criteria
            }) AS user_stories
            RETURN __cy_n__.class_name AS name, 
                   __cy_n__.summary AS summary,
                   user_stories AS user_stories, 
                   labels(__cy_n__)[0] AS type
            ORDER BY __cy_n__.file_name, __cy_n__.startLine
        """
        
        async with self._cypher_lock:
            results = await client.execute_queries([query])
        
        if not results or not results[0]:
            log_process(
                "ANALYZE", "USER_STORY",
                "User Story ìƒì„± ìŠ¤í‚µ: ë¶„ì„ëœ í´ë˜ìŠ¤/ì¸í„°í˜ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤",
                logging.INFO
            )
            return None
        
        filtered = [
            r for r in results[0]
            if r.get("summary") or (r.get("user_stories") and len(r["user_stories"]) > 0)
        ]
        
        if not filtered:
            return None
        
        log_process("ANALYZE", "USER_STORY", f"User Story ìƒì„± | ëŒ€ìƒ={len(filtered)}ê°œ í´ë˜ìŠ¤")
        return generate_user_story_document(
            results=filtered,
            source_name="ROBO",
            source_type="Java í´ë˜ìŠ¤/ì¸í„°í˜ì´ìŠ¤",
        )

    # =========================================================================
    # íŒŒì¼ ë¡œë“œ
    # =========================================================================

    async def _load_all_files(
        self,
        file_names: list[tuple[str, str]],
        orchestrator: Any,
    ) -> List[FileAnalysisContext]:
        """ëª¨ë“  íŒŒì¼ì˜ AST JSONì„ ë³‘ë ¬ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.
        
        source íŒŒì¼ì€ ë” ì´ìƒ ì½ì§€ ì•ŠìŠµë‹ˆë‹¤ - AST JSONì˜ code ì†ì„± ì‚¬ìš©.
        """
        
        async def load_single(directory: str, file_name: str) -> FileAnalysisContext:
            base_name = os.path.splitext(file_name)[0]
            ast_path = os.path.join(orchestrator.dirs["analysis"], directory, f"{base_name}.json")

            async with aiofiles.open(ast_path, "r", encoding="utf-8") as ast_file:
                ast_content = await ast_file.read()
                return FileAnalysisContext(
                    directory=directory,
                    file_name=file_name,
                    ast_data=json.loads(ast_content),
                )

        tasks = [load_single(d, f) for d, f in file_names]
        return await asyncio.gather(*tasks)
