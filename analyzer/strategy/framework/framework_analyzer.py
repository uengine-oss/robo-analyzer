"""Framework ì½”ë“œ ë¶„ì„ ì „ëµ - Java, Kotlin ë“±

AST ê¸°ë°˜ Java ì½”ë“œ ë¶„ì„ â†’ Neo4j í´ë˜ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨ ê·¸ë˜í”„ ìƒì„±.

ë¶„ì„ íë¦„ (2ë‹¨ê³„ + ì´ì¤‘ ë³‘ë ¬):
1. [Phase 1] ëª¨ë“  íŒŒì¼ AST ê·¸ë˜í”„ ìƒì„± (ë³‘ë ¬ 5ê°œ)
   - ì •ì  ë…¸ë“œ ìƒì„±: CLASS, INTERFACE, METHOD, FIELD
   - ì •ì  ê´€ê³„ ìƒì„±: HAS_METHOD, HAS_FIELD, CONTAINS
   
2. [Phase 2] ëª¨ë“  íŒŒì¼ LLM ë¶„ì„ (íŒŒì¼ ë³‘ë ¬ 5ê°œ + ì²­í¬ ë³‘ë ¬)
   - ì½”ë“œ ìš”ì•½ ë° ë¶„ì„
   - CALLS ê´€ê³„ ìƒì„± (MATCHë¡œ ê¸°ì¡´ ë…¸ë“œ ì¡°íšŒ)
   - DEPENDENCY ê´€ê³„ ìƒì„±
   
3. [Phase 3] í´ë˜ìŠ¤ ìš”ì•½ ë° User Story ìƒì„±
"""

import asyncio
import json
import logging
import os
from dataclasses import dataclass
from typing import Any, AsyncGenerator, Optional

import aiofiles

from analyzer.neo4j_client import Neo4jClient
from analyzer.strategy.base_analyzer import AnalyzerStrategy
from analyzer.strategy.framework.ast_processor import FrameworkAstProcessor
from config.settings import settings
from util.exception import AnalysisError, CodeProcessError
from util.stream_utils import (
    emit_complete,
    emit_data,
    emit_error,
    emit_message,
    format_graph_result,
)
from util.utility_tool import (
    escape_for_cypher,
    generate_user_story_document,
    log_process,
)


@dataclass
class FileAnalysisContext:
    """íŒŒì¼ ë¶„ì„ ì»¨í…ìŠ¤íŠ¸"""
    directory: str
    file_name: str
    ast_data: dict
    source_lines: list[str]
    processor: Optional[FrameworkAstProcessor] = None


class FrameworkAnalyzer(AnalyzerStrategy):
    """Java/Framework ì½”ë“œ ë¶„ì„ ì „ëµ
    
    2ë‹¨ê³„ ë¶„ì„ + ì´ì¤‘ ë³‘ë ¬ ì²˜ë¦¬:
    - Phase 1: ëª¨ë“  íŒŒì¼ AST ê·¸ë˜í”„ ìƒì„± (ë³‘ë ¬)
    - Phase 2: ëª¨ë“  íŒŒì¼ LLM ë¶„ì„ (ë³‘ë ¬)
    """

    def __init__(self):
        self._cypher_lock = asyncio.Lock()  # Cypher ì¿¼ë¦¬ ë™ì‹œì„± ë³´í˜¸
        self._file_semaphore: Optional[asyncio.Semaphore] = None

    async def analyze(
        self,
        file_names: list[tuple[str, str]],
        orchestrator: Any,
        **kwargs,
    ) -> AsyncGenerator[bytes, None]:
        """íŒŒì¼ ëª©ë¡ì„ 2ë‹¨ê³„ë¡œ ë¶„ì„í•˜ì—¬ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤."""
        client = Neo4jClient()
        total_files = len(file_names)
        self._file_semaphore = asyncio.Semaphore(settings.concurrency.file_concurrency)
        
        # ì „ì²´ í†µê³„
        stats = {
            "total_nodes": 0,
            "total_rels": 0,
            "phase1_nodes": 0,
            "phase2_updates": 0,
        }

        try:
            # ========== ì´ˆê¸°í™” ==========
            yield emit_message("ğŸš€ í”„ë ˆì„ì›Œí¬ ì½”ë“œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤")
            yield emit_message(f"ğŸ“¦ í”„ë¡œì íŠ¸: {orchestrator.project_name}")
            yield emit_message(f"ğŸ“Š ë¶„ì„ ëŒ€ìƒ: {total_files}ê°œ íŒŒì¼")
            yield emit_message(f"âš¡ ë³‘ë ¬ ì²˜ë¦¬: íŒŒì¼ {settings.concurrency.file_concurrency}ê°œ ë™ì‹œ")
            
            await client.ensure_constraints()
            yield emit_message("ğŸ”Œ Neo4j ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì™„ë£Œ")

            # ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ í™•ì¸
            if await client.check_nodes_exist(orchestrator.user_id, file_names):
                yield emit_message("ğŸ”„ ì´ì „ ë¶„ì„ ê²°ê³¼ ë°œê²¬ â†’ ì¦ë¶„ ì—…ë°ì´íŠ¸ ëª¨ë“œ")
            else:
                yield emit_message("ğŸ†• ìƒˆë¡œìš´ ë¶„ì„ ì‹œì‘")

            # ========== Phase 1: AST ê·¸ë˜í”„ ìƒì„± (ë³‘ë ¬) ==========
            yield emit_message("")
            yield emit_message("â”" * 50)
            yield emit_message(f"ğŸ—ï¸ [Phase 1] AST êµ¬ì¡° ê·¸ë˜í”„ ìƒì„± ({total_files}ê°œ íŒŒì¼ ë³‘ë ¬)")
            yield emit_message("â”" * 50)

            # íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ (ë³‘ë ¬)
            contexts = await self._load_all_files(file_names, orchestrator)
            yield emit_message(f"   âœ“ {len(contexts)}ê°œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ")

            # Phase 1: ì •ì  ê·¸ë˜í”„ ìƒì„± (ë³‘ë ¬)
            async for chunk in self._run_phase1(contexts, client, orchestrator, stats):
                yield chunk

            yield emit_message("")
            yield emit_message(f"   âœ… Phase 1 ì™„ë£Œ: {stats['phase1_nodes']}ê°œ ë…¸ë“œ ìƒì„±")

            # ========== Phase 2: LLM ë¶„ì„ (ë³‘ë ¬) ==========
            yield emit_message("")
            yield emit_message("â”" * 50)
            yield emit_message(f"ğŸ¤– [Phase 2] AI ë¶„ì„ ({total_files}ê°œ íŒŒì¼ ë³‘ë ¬)")
            yield emit_message("â”" * 50)

            async for chunk in self._run_phase2(contexts, client, orchestrator, stats):
                yield chunk

            yield emit_message("")
            yield emit_message(f"   âœ… Phase 2 ì™„ë£Œ: {stats['phase2_updates']}ê°œ ë¶„ì„ ì™„ë£Œ")

            # ========== Phase 3: User Story ìƒì„± ==========
            yield emit_message("")
            yield emit_message("â”" * 50)
            yield emit_message("ğŸ“ [Phase 3] User Story ë¬¸ì„œ ìƒì„±")
            yield emit_message("â”" * 50)
            
            user_story_doc = await self._create_user_story_doc(client, orchestrator)
            if user_story_doc:
                yield emit_data(
                    graph={"Nodes": [], "Relationships": []},
                    line_number=0,
                    analysis_progress=100,
                    current_file="user_stories.md",
                    user_story_document=user_story_doc,
                    event_type="user_story_document",
                )
                yield emit_message("   âœ“ User Story ë¬¸ì„œ ìƒì„± ì™„ë£Œ")
            else:
                yield emit_message("   â„¹ï¸ ì¶”ì¶œí•  User Story ì—†ìŒ")
            
            # ========== ì™„ë£Œ ==========
            yield emit_message("")
            yield emit_message("â”" * 50)
            yield emit_message("âœ… ëª¨ë“  ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            yield emit_message(f"   ğŸ“Š ì´ ë…¸ë“œ: {stats['total_nodes']}ê°œ")
            yield emit_message(f"   ğŸ”— ì´ ê´€ê³„: {stats['total_rels']}ê°œ")
            yield emit_message("â”" * 50)
            yield emit_complete()
            
        except AnalysisError as e:
            log_process("ANALYZE", "ERROR", f"ë¶„ì„ ì˜¤ë¥˜: {e}", logging.ERROR, e)
            yield emit_error(str(e))
            raise
        except Exception as e:
            error_msg = f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}"
            log_process("ANALYZE", "ERROR", error_msg, logging.ERROR, e)
            yield emit_error(error_msg)
            raise CodeProcessError(error_msg) from e
        finally:
            await client.close()

    async def _load_all_files(
        self,
        file_names: list[tuple[str, str]],
        orchestrator: Any,
    ) -> list[FileAnalysisContext]:
        """ëª¨ë“  íŒŒì¼ì˜ ASTì™€ ì†ŒìŠ¤ì½”ë“œë¥¼ ë³‘ë ¬ë¡œ ë¡œë“œí•©ë‹ˆë‹¤."""
        
        async def load_single(directory: str, file_name: str) -> FileAnalysisContext:
            src_path = os.path.join(orchestrator.dirs["src"], directory, file_name)
            base_name = os.path.splitext(file_name)[0]
            ast_path = os.path.join(orchestrator.dirs["analysis"], directory, f"{base_name}.json")

            async with aiofiles.open(ast_path, "r", encoding="utf-8") as ast_file, \
                       aiofiles.open(src_path, "r", encoding="utf-8") as src_file:
                ast_content, source_lines = await asyncio.gather(
                    ast_file.read(),
                    src_file.readlines(),
                )
                return FileAnalysisContext(
                    directory=directory,
                    file_name=file_name,
                    ast_data=json.loads(ast_content),
                    source_lines=source_lines,
                )

        tasks = [load_single(d, f) for d, f in file_names]
        return await asyncio.gather(*tasks)

    async def _run_phase1(
        self,
        contexts: list[FileAnalysisContext],
        client: Neo4jClient,
        orchestrator: Any,
        stats: dict,
    ) -> AsyncGenerator[bytes, None]:
        """Phase 1: ëª¨ë“  íŒŒì¼ì˜ AST ê·¸ë˜í”„ë¥¼ ë³‘ë ¬ë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
        
        completed = 0
        total = len(contexts)
        results_queue: asyncio.Queue = asyncio.Queue()

        async def process_file(ctx: FileAnalysisContext):
            async with self._file_semaphore:
                try:
                    processor = FrameworkAstProcessor(
                        antlr_data=ctx.ast_data,
                        file_content="".join(ctx.source_lines),
                        directory=ctx.directory,
                        file_name=ctx.file_name,
                        user_id=orchestrator.user_id,
                        api_key=orchestrator.api_key,
                        locale=orchestrator.locale,
                        project_name=orchestrator.project_name,
                        last_line=len(ctx.source_lines),
                    )
                    ctx.processor = processor
                    
                    # ì •ì  ê·¸ë˜í”„ ìƒì„±
                    queries = processor.build_static_graph_queries()
                    
                    if queries:
                        # Cypher ì¿¼ë¦¬ ì‹¤í–‰ (ë½ ì‚¬ìš©)
                        async with self._cypher_lock:
                            graph = await client.run_graph_query(queries)
                        
                        node_count = len(graph.get("Nodes", []))
                        rel_count = len(graph.get("Relationships", []))
                        
                        await results_queue.put({
                            "type": "success",
                            "file": ctx.file_name,
                            "graph": graph,
                            "node_count": node_count,
                            "rel_count": rel_count,
                        })
                    else:
                        await results_queue.put({
                            "type": "success",
                            "file": ctx.file_name,
                            "graph": {"Nodes": [], "Relationships": []},
                            "node_count": 0,
                            "rel_count": 0,
                        })
                        
                except Exception as e:
                    log_process("ANALYZE", "ERROR", f"Phase 1 ì˜¤ë¥˜ ({ctx.file_name}): {e}", logging.ERROR, e)
                    await results_queue.put({
                        "type": "error",
                        "file": ctx.file_name,
                        "message": str(e),
                    })

        # ëª¨ë“  íŒŒì¼ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘
        tasks = [asyncio.create_task(process_file(ctx)) for ctx in contexts]

        # ê²°ê³¼ ìˆ˜ì‹  ë° ìŠ¤íŠ¸ë¦¬ë°
        while completed < total:
            result = await asyncio.wait_for(results_queue.get(), timeout=300.0)
            completed += 1
            
            if result["type"] == "error":
                yield emit_message(f"   âŒ [{completed}/{total}] {result['file']}: {result['message']}")
            else:
                stats["phase1_nodes"] += result["node_count"]
                stats["total_nodes"] += result["node_count"]
                stats["total_rels"] += result["rel_count"]
                
                graph = result["graph"]
                graph_msg = format_graph_result(graph)
                
                yield emit_message(f"   âœ“ [{completed}/{total}] {result['file']}")
                if graph_msg:
                    for line in graph_msg.split("\n")[:3]:  # ìµœëŒ€ 3ì¤„
                        yield emit_message(f"      {line}")
                
                yield emit_data(
                    graph=graph,
                    line_number=0,
                    analysis_progress=int(completed / total * 50),
                    current_file=result["file"],
                )

        # ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
        await asyncio.gather(*tasks, return_exceptions=True)

    async def _run_phase2(
        self,
        contexts: list[FileAnalysisContext],
        client: Neo4jClient,
        orchestrator: Any,
        stats: dict,
    ) -> AsyncGenerator[bytes, None]:
        """Phase 2: ëª¨ë“  íŒŒì¼ì˜ LLM ë¶„ì„ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        
        completed = 0
        total = len(contexts)
        results_queue: asyncio.Queue = asyncio.Queue()

        async def analyze_file(ctx: FileAnalysisContext):
            async with self._file_semaphore:
                try:
                    if not ctx.processor:
                        raise AnalysisError(f"Phase 1ì—ì„œ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì‹¤íŒ¨: {ctx.file_name}")
                    
                    # LLM ë¶„ì„ ì‹¤í–‰
                    analysis_queries = await ctx.processor.run_llm_analysis()
                    
                    if analysis_queries:
                        # Cypher ì¿¼ë¦¬ ì‹¤í–‰ (ë½ ì‚¬ìš©)
                        async with self._cypher_lock:
                            graph = await client.run_graph_query(analysis_queries)
                        
                        await results_queue.put({
                            "type": "success",
                            "file": ctx.file_name,
                            "graph": graph,
                            "query_count": len(analysis_queries),
                        })
                    else:
                        await results_queue.put({
                            "type": "success",
                            "file": ctx.file_name,
                            "graph": {"Nodes": [], "Relationships": []},
                            "query_count": 0,
                        })
                        
                except Exception as e:
                    log_process("ANALYZE", "ERROR", f"Phase 2 ì˜¤ë¥˜ ({ctx.file_name}): {e}", logging.ERROR, e)
                    await results_queue.put({
                        "type": "error",
                        "file": ctx.file_name,
                        "message": str(e),
                    })

        # ëª¨ë“  íŒŒì¼ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘
        tasks = [asyncio.create_task(analyze_file(ctx)) for ctx in contexts]

        # ê²°ê³¼ ìˆ˜ì‹  ë° ìŠ¤íŠ¸ë¦¬ë°
        while completed < total:
            result = await asyncio.wait_for(results_queue.get(), timeout=600.0)
            completed += 1
            
            if result["type"] == "error":
                yield emit_message(f"   âŒ [{completed}/{total}] {result['file']}: {result['message']}")
            else:
                stats["phase2_updates"] += 1
                graph = result["graph"]
                stats["total_nodes"] += len(graph.get("Nodes", []))
                stats["total_rels"] += len(graph.get("Relationships", []))
                
                graph_msg = format_graph_result(graph)
                yield emit_message(f"   âœ“ [{completed}/{total}] {result['file']} (ì¿¼ë¦¬ {result['query_count']}ê°œ)")
                if graph_msg:
                    for line in graph_msg.split("\n")[:3]:
                        yield emit_message(f"      {line}")
                
                yield emit_data(
                    graph=graph,
                    line_number=0,
                    analysis_progress=50 + int(completed / total * 50),
                    current_file=result["file"],
                )

        # ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
        await asyncio.gather(*tasks, return_exceptions=True)

    async def _create_user_story_doc(
        self,
        client: Neo4jClient,
        orchestrator: Any,
    ) -> str:
        """ë¶„ì„ëœ í´ë˜ìŠ¤ì—ì„œ User Story ë¬¸ì„œ ìƒì„±"""
        try:
            query = f"""
                MATCH (n)
                WHERE (n:CLASS OR n:INTERFACE)
                  AND n.user_id = '{escape_for_cypher(orchestrator.user_id)}'
                  AND n.project_name = '{escape_for_cypher(orchestrator.project_name)}'
                  AND n.summary IS NOT NULL
                OPTIONAL MATCH (n)-[:HAS_USER_STORY]->(us:UserStory)
                OPTIONAL MATCH (us)-[:HAS_AC]->(ac:AcceptanceCriteria)
                WITH n, us, collect(DISTINCT {{
                    id: ac.id,
                    title: ac.title,
                    given: ac.given,
                    when: ac.when,
                    then: ac.then
                }}) AS acceptance_criteria
                WITH n, collect(DISTINCT {{
                    id: us.id,
                    role: us.role,
                    goal: us.goal,
                    benefit: us.benefit,
                    acceptance_criteria: acceptance_criteria
                }}) AS user_stories
                RETURN n.class_name AS name, 
                       n.summary AS summary,
                       user_stories AS user_stories, 
                       labels(n)[0] AS type
                ORDER BY n.file_name, n.startLine
            """
            
            async with self._cypher_lock:
                results = await client.execute_queries([query])
            
            if not results or not results[0]:
                log_process(
                    "ANALYZE", "USER_STORY",
                    f"âš ï¸ Neo4j ì¿¼ë¦¬ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. í´ë˜ìŠ¤/ì¸í„°í˜ì´ìŠ¤ì— summaryê°€ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.",
                    logging.WARNING
                )
                raise AnalysisError("User Story ìƒì„±ì„ ìœ„í•œ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤ (Neo4jì— summaryê°€ ìˆëŠ” í´ë˜ìŠ¤/ì¸í„°í˜ì´ìŠ¤ê°€ ì—†ìŒ)")
            
            filtered = [
                r for r in results[0]
                if r.get("summary") or (r.get("user_stories") and len(r["user_stories"]) > 0)
            ]
            
            if not filtered:
                raise AnalysisError("User Story ìƒì„± ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤ (ìš”ì•½ëœ í´ë˜ìŠ¤ ì—†ìŒ)")
            
            log_process("ANALYZE", "USER_STORY", f"User Story ìƒì„± | ëŒ€ìƒ={len(filtered)}ê°œ í´ë˜ìŠ¤")
            return generate_user_story_document(
                results=filtered,
                source_name=orchestrator.project_name,
                source_type="Java í´ë˜ìŠ¤/ì¸í„°í˜ì´ìŠ¤",
            )
            
        except Exception as exc:
            log_process(
                "ANALYZE", "USER_STORY", 
                f"User Story ë¬¸ì„œ ìƒì„± ì‹¤íŒ¨: {exc}",
                logging.ERROR, exc
            )
            raise AnalysisError(f"User Story ìƒì„± ì‹¤íŒ¨: {exc}") from exc

