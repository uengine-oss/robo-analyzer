import asyncio
import json
import logging
import os
from typing import Any, AsyncGenerator

import aiofiles

from understand.neo4j_connection import Neo4jConnection
from understand.strategy.base_strategy import UnderstandStrategy
from understand.strategy.framework.analysis import FrameworkAnalyzer
from util.utility_tool import (
    emit_data,
    emit_error,
    emit_message,
    escape_for_cypher,
    generate_user_story_document,
    aggregate_user_stories_from_results,
)


class FrameworkUnderstandStrategy(UnderstandStrategy):
    """Java/Framework ì½”ë“œ ê¸°ë°˜ í´ë˜ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨ ê·¸ë˜í”„ êµ¬ì¶• ì „ëµ"""

    @staticmethod
    def _calculate_progress(current_line: int, total_lines: int) -> int:
        """í˜„ì¬ ì§„í–‰ë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤ (0-99%)."""
        return min(int((current_line / total_lines) * 100), 99) if current_line > 0 else 0

    async def understand(self, file_names: list, orchestrator: Any, **kwargs) -> AsyncGenerator[bytes, None]:
        connection = Neo4jConnection()
        events_from_analyzer = asyncio.Queue()
        events_to_analyzer = asyncio.Queue()
        total_files = len(file_names)

        try:
            yield emit_message("ğŸš€ í”„ë ˆì„ì›Œí¬ ì½”ë“œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤")
            yield emit_message(f"ğŸ“¦ í”„ë¡œì íŠ¸ '{orchestrator.project_name}'ì—ì„œ {total_files}ê°œ íŒŒì¼ì„ ë¶„ì„í•©ë‹ˆë‹¤")
            
            await connection.ensure_constraints()
            yield emit_message("ğŸ”Œ ë°ì´í„°ë² ì´ìŠ¤ì— ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤")

            if await connection.node_exists(orchestrator.user_id, file_names):
                yield emit_message("ğŸ”„ ì´ì „ ë¶„ì„ ê²°ê³¼ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤")

            yield emit_message(f"ğŸ” í´ë˜ìŠ¤ ë° ì¸í„°í˜ì´ìŠ¤ êµ¬ì¡°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤ ({total_files}ê°œ íŒŒì¼)")

            for file_idx, (directory, file_name) in enumerate(file_names, 1):
                yield emit_message(f"ğŸ“„ [{file_idx}/{total_files}] {file_name} ë¶„ì„ ì¤‘...")
                if directory:
                    yield emit_message(f"   ğŸ“ {directory}")
                
                async for chunk in self._analyze_file(
                    directory,
                    file_name,
                    file_names,
                    connection,
                    events_from_analyzer,
                    events_to_analyzer,
                    orchestrator,
                ):
                    yield chunk
                
                yield emit_message(f"   âœ“ {file_name} ì™„ë£Œ")

            yield emit_message(f"ğŸ‰ ì½”ë“œ êµ¬ì¡° ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ ({total_files}ê°œ íŒŒì¼)")
            
            # User Story ë¬¸ì„œ ìƒì„±
            yield emit_message("ğŸ“ ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ì„ ì •ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            user_story_doc = await self._generate_user_story_document(connection, orchestrator)
            if user_story_doc:
                yield emit_data(
                    graph={"Nodes": [], "Relationships": []},
                    line_number=0,
                    analysis_progress=100,
                    current_file="user_stories.md",
                    user_story_document=user_story_doc,
                    event_type="user_story_document"
                )
                yield emit_message("ğŸ“‹ User Story ë¬¸ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤")
            else:
                yield emit_message("â„¹ï¸ ì¶”ì¶œí•  User Storyê°€ ì—†ìŠµë‹ˆë‹¤")
            
            yield emit_message("âœ… ëª¨ë“  ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        finally:
            await connection.close()

    async def _load_assets(self, orchestrator, directory: str, file_name: str) -> tuple:
        src_file_path = os.path.join(orchestrator.dirs["src"], directory, file_name)
        base_name = os.path.splitext(file_name)[0]
        analysis_file_path = os.path.join(orchestrator.dirs["analysis"], directory, f"{base_name}.json")

        async with aiofiles.open(analysis_file_path, "r", encoding="utf-8") as antlr_file, aiofiles.open(
            src_file_path, "r", encoding="utf-8"
        ) as source_file:
            antlr_data, source_content = await asyncio.gather(antlr_file.read(), source_file.readlines())
            return json.loads(antlr_data), source_content

    async def _analyze_file(
        self,
        directory: str,
        file_name: str,
        file_pairs: list,
        connection: Neo4jConnection,
        events_from_analyzer: asyncio.Queue,
        events_to_analyzer: asyncio.Queue,
        orchestrator: Any,
    ) -> AsyncGenerator[bytes, None]:
        current_file = f"{directory}/{file_name}" if directory else file_name

        antlr_data, source_content = await self._load_assets(orchestrator, directory, file_name)
        last_line = len(source_content)
        source_raw = "".join(source_content)
        analyzer = FrameworkAnalyzer(
            antlr_data=antlr_data,
            file_content=source_raw,
            directory=directory,
            file_name=file_name,
            user_id=orchestrator.user_id,
            api_key=orchestrator.api_key,
            locale=orchestrator.locale,
            project_name=orchestrator.project_name,
            send_queue=events_from_analyzer,
            receive_queue=events_to_analyzer,
            last_line=last_line,
        )
        analysis_task = asyncio.create_task(analyzer.run())

        analyzed_blocks = 0
        static_blocks = 0
        total_llm_batches = 0

        while True:
            event = await events_from_analyzer.get()
            event_type = event.get("type")
            logging.info("Analysis Event: %s, type: %s", current_file, event_type)

            # ë¶„ì„ ì™„ë£Œ
            if event_type == "end_analysis":
                logging.info("Understanding Completed for %s", current_file)
                yield emit_data(graph={"Nodes": [], "Relationships": []}, line_number=last_line, analysis_progress=100, current_file=current_file)
                break

            # ì˜¤ë¥˜ ë°œìƒ
            if event_type == "error":
                error_message = event.get("message", f"Understanding failed for {file_name}")
                logging.error("Understanding Failed for %s: %s", file_name, error_message)
                yield emit_message(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {error_message}")
                yield emit_error(error_message)
                return

            next_line = event.get("line_number", 0)
            progress = self._calculate_progress(next_line, last_line)

            # ì •ì  ê·¸ë˜í”„ ìƒì„±
            if event_type == "static_graph":
                if static_blocks == 0:
                    yield emit_message("   ğŸ—ï¸ í´ë˜ìŠ¤/ë©”ì„œë“œ êµ¬ì¡°ë¥¼ ê·¸ë˜í”„ë¡œ êµ¬ì„± ì¤‘...")
                static_blocks += 1
                graph_result = await connection.execute_query_and_return_graph(event.get("query_data", []))
                yield emit_data(graph=graph_result, line_number=next_line, analysis_progress=progress, current_file=current_file)
                await events_to_analyzer.put({"type": "process_completed"})
                continue

            # ì •ì  ê·¸ë˜í”„ ì™„ë£Œ
            if event_type == "static_complete":
                yield emit_message(f"   âœ“ êµ¬ì¡° ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ ({static_blocks}ê°œ)")
                await events_to_analyzer.put({"type": "process_completed"})
                continue

            # LLM ë¶„ì„ ì‹œì‘
            if event_type == "llm_start":
                total_llm_batches = event.get("total_batches", 0)
                yield emit_message(f"   ğŸ¤– AIê°€ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ë¶„ì„í•©ë‹ˆë‹¤ ({total_llm_batches}ê°œ ë¸”ë¡)")
                await events_to_analyzer.put({"type": "process_completed"})
                continue

            # LLM ë¶„ì„ ì§„í–‰
            if event_type == "analysis_code":
                analyzed_blocks += 1
                graph_result = await connection.execute_query_and_return_graph(event.get("query_data", []))
                yield emit_data(graph=graph_result, line_number=next_line, analysis_progress=progress, current_file=current_file)
                await events_to_analyzer.put({"type": "process_completed"})

        await analysis_task

    async def _generate_user_story_document(
        self,
        connection: Neo4jConnection,
        orchestrator,
    ) -> str:
        """ë¶„ì„ëœ ëª¨ë“  í´ë˜ìŠ¤ì—ì„œ User Storyë¥¼ ìˆ˜ì§‘í•˜ì—¬ ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            # ëª¨ë“  í´ë˜ìŠ¤/ì¸í„°í˜ì´ìŠ¤ì˜ user_stories ì†ì„± ì¡°íšŒ
            query = f"""
                MATCH (n)
                WHERE (n:CLASS OR n:INTERFACE)
                  AND n.user_id = '{escape_for_cypher(orchestrator.user_id)}'
                  AND n.project_name = '{escape_for_cypher(orchestrator.project_name)}'
                  AND n.user_stories IS NOT NULL
                RETURN n.class_name AS name, n.user_stories AS user_stories, labels(n)[0] AS type
                ORDER BY n.file_name, n.startLine
            """
            
            results = await connection.execute_queries([query])
            
            if not results or not results[0]:
                return ""
            
            # ëª¨ë“  User Story ì§‘ê³„
            all_user_stories = aggregate_user_stories_from_results(results[0])
            
            if not all_user_stories:
                return ""
            
            # ë¬¸ì„œ ìƒì„±
            document = generate_user_story_document(
                user_stories=all_user_stories,
                source_name=orchestrator.project_name,
                source_type="Java í´ë˜ìŠ¤/ì¸í„°í˜ì´ìŠ¤"
            )
            
            return document
            
        except Exception as exc:
            logging.error("User Story ë¬¸ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜: %s", exc)
            return ""

