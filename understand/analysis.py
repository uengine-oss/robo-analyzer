import asyncio
from collections import defaultdict
import json
import logging
import re
from prompt.understand_summarized_prompt import understand_summary
import tiktoken
from prompt.understand_prompt import understand_code
from prompt.understand_variables_prompt import understand_variables
from util.exception import (LLMCallError, UnderstandingError, ProcessAnalyzeCodeError)
from util.utility_tool import calculate_code_token


encoder = tiktoken.get_encoding("cl100k_base")



# ==================== ì„¹ì…˜: ìƒìˆ˜ ì •ì˜ ====================
# ë³¸ ëª¨ë“ˆ ì „ë°˜ì—ì„œ ì‚¬ìš©í•˜ëŠ” êµ¬ë¬¸ íƒ€ì…/ë¶„ì„ ì œì–´ìš© ìƒìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
PROCEDURE_TYPES = ["PROCEDURE", "FUNCTION", "CREATE_PROCEDURE_BODY", "TRIGGER"]
NON_ANALYSIS_TYPES = ["CREATE_PROCEDURE_BODY", "FILE", "PROCEDURE","FUNCTION", "DECLARE", "TRIGGER", "FOLDER", "SPEC"]
NON_NEXT_RECURSIVE_TYPES = ["FUNCTION", "PROCEDURE", "PACKAGE_VARIABLE", "TRIGGER"]


# ==================== ì„¹ì…˜: ìœ í‹¸ë¦¬í‹° í—¬í¼ ====================
# ê³µí†µì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë¬¸ìì—´ ì²˜ë¦¬, ë²”ìœ„ ì¶”ì¶œ, í† í° ê¸°ì¤€ íŒë‹¨ ë“±ì˜ í—¬í¼ì…ë‹ˆë‹¤.
def get_statement_type(start_line: int, end_line: int, node_statement_types: set[str]) -> str | None:
    """ì—­í• :
    - ì €ì¥ëœ `"TYPE_start_end"` í‘œê¸° ì§‘í•©ì—ì„œ `(start_line, end_line)`ê³¼ ì¼ì¹˜í•˜ëŠ” í•­ëª©ì„ ì°¾ì•„ TYPEì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
    - start_line (int): êµ¬ë¬¸ ë…¸ë“œì˜ ì‹œì‘ ë¼ì¸ ë²ˆí˜¸.
    - end_line (int): êµ¬ë¬¸ ë…¸ë“œì˜ ì¢…ë£Œ ë¼ì¸ ë²ˆí˜¸.
    - node_statement_types (set[str]): `"TYPE_start_end"` í˜•ì‹ì˜ ì‹ë³„ì ë¬¸ìì—´ ì§‘í•©.

    ë°˜í™˜ê°’:
    - Optional[str]: ë§¤ì¹­ë˜ëŠ” TYPE ë¬¸ìì—´. ë§¤ì¹­ì´ ì—†ìœ¼ë©´ None.
    """
    entry = next((e for e in node_statement_types if e.endswith(f"_{start_line}_{end_line}")), None)
    return entry.rsplit('_', 2)[0] if entry else None


def get_table_relationship(statement_type: str | None) -> str | None:
    """ì—­í• :
    - êµ¬ë¬¸ íƒ€ì…ì„ í…Œì´ë¸” ê´€ê³„ ë¼ë²¨ë¡œ ë§¤í•‘í•©ë‹ˆë‹¤. SELECTâ†’FROM, DMLâ†’WRITES, EXECUTE_IMMEDIATEâ†’EXECUTE.

    ë§¤ê°œë³€ìˆ˜:
    - statement_type (Optional[str]): êµ¬ë¬¸ íƒ€ì… ë¼ë²¨.

    ë°˜í™˜ê°’:
    - Optional[str]: í…Œì´ë¸” ê´€ê³„ ë¼ë²¨(FROM/WRITES/EXECUTE). ë§¤í•‘ë˜ì§€ ì•Šìœ¼ë©´ None.
    """
    if statement_type in ["SELECT", "FETCH"]:
        return "FROM"
    if statement_type in ["UPDATE", "INSERT", "DELETE", "MERGE"]:
        return "WRITES"
    if statement_type in ["EXECUTE_IMMEDIATE", "ASSIGNMENT", "CALL"]:
        return "EXECUTE"
    return None


def parse_table_identifier(qualified_table_name: str) -> tuple[str | None, str, str | None]:
    """ì—­í• :
    - 'SCHEMA.TABLE@DBLINK' í˜•íƒœì˜ í‘œê¸°ì—ì„œ (ìŠ¤í‚¤ë§ˆ, í…Œì´ë¸”, DBë§í¬ëª…)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

    ë°˜í™˜ê°’:
    - (schema, table, db_link)
    """
    qualified = qualified_table_name.strip().upper()
    link_name = None
    if '@' in qualified:
        left, link_name = qualified.split('@', 1)
    else:
        left = qualified

    if '.' in left:
        schema, table = left.split('.', 1)
    else:
        schema, table = None, left
    return schema, table, (link_name or None)


def is_over_token_limit(node_token: int, sp_token: int, context_len: int) -> bool:
    """ì—­í• :
    - í† í° ì„ê³„ì¹˜(ê°œë³„ ë…¸ë“œ/ëˆ„ì /ë²”ìœ„ ê°œìˆ˜) ë„ë‹¬ ì—¬ë¶€ë¥¼ íŒë‹¨í•´ ë°°ì¹˜ í”ŒëŸ¬ì‹œ í•„ìš”ì„±ì„ ê²°ì •í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
    - node_token (int): í˜„ì¬ ë…¸ë“œ ì½”ë“œì˜ í† í° ìˆ˜.
    - sp_token (int): ëˆ„ì ëœ ìŠ¤í† ì–´ë“œ í”„ë¡œì‹œì € ì»¨í…ìŠ¤íŠ¸ í† í° ìˆ˜.
    - context_len (int): ëˆ„ì ëœ ë¶„ì„ ë²”ìœ„(context_range) êµ¬ê°„ ìˆ˜.

    ë°˜í™˜ê°’:
    - bool: ì„ê³„ì¹˜ ë„ë‹¬ ì‹œ True, ì•„ë‹ˆë©´ False.
    """
    return (
        (node_token >= 1000 and context_len and node_token + sp_token >= 1000)
        or (sp_token >= 1000 and context_len)
        or (context_len >= 10)
    )


def escape_for_cypher_multiline(text: str) -> str:
    """ì—­í• :
    - Cypher ì¿¼ë¦¬ì— ì•ˆì „í•˜ê²Œ í¬í•¨ë˜ë„ë¡ ê°œí–‰ê³¼ ì‘ì€ë”°ì˜´í‘œë¥¼ ì´ìŠ¤ì¼€ì´í”„í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
    - text (str): ì›ë³¸ ë¬¸ìì—´.

    ë°˜í™˜ê°’:
    - str: ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬ëœ ë¬¸ìì—´.
    """
    return text.replace('\n', '\\n').replace("'", "\\'")
    

def extract_code_within_range(code: str, context_range: list[dict]) -> tuple[str, int]:
    """ì—­í• :
    - ë¼ì¸ ë²ˆí˜¸ ì ‘ë‘ê°€ í¬í•¨ëœ `code`ì—ì„œ `context_range`ì˜ ìµœì†Œ ì‹œì‘~ìµœëŒ€ ì¢…ë£Œ ë¼ì¸ ì‚¬ì´ë§Œ ë°œì·Œí•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
    - code (str): ë¼ì¸ ë²ˆí˜¸ ì ‘ë‘ê°€ í¬í•¨ëœ ëˆ„ì  ì½”ë“œ ë¬¸ìì—´.
    - context_range (list[dict]): `{"startLine": int, "endLine": int}` í˜•íƒœì˜ êµ¬ê°„ ë¦¬ìŠ¤íŠ¸.

    ë°˜í™˜ê°’:
    - tuple[str, int]: (ë°œì·Œëœ ì½”ë“œ ë¬¸ìì—´, ìµœì¢… end_line).
    """
    try:
        if not (code and context_range):
            return "", 0

        start_line = min(range_item['startLine'] for range_item in context_range)
        end_line = max(range_item['endLine'] for range_item in context_range)
        code_lines = code.split('\n')
        line_number_pattern = r'^(\d+)(?:~\d+)?:\s'
        
        extracted_lines = []
        for line in code_lines:
            match = re.match(line_number_pattern, line)
            if match:
                line_number = int(match.group(1))
                if start_line <= line_number <= end_line:
                    extracted_lines.append(line)

        extracted_code = '\n'.join(extracted_lines)
        return extracted_code, end_line
    
    except Exception as e:
        err_msg = f"Understanding ê³¼ì •ì—ì„œ ë²”ìœ„ë‚´ì— ì½”ë“œ ì¶”ì¶œ ë„ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        logging.error(err_msg)
        raise ProcessAnalyzeCodeError(err_msg)


def get_procedure_name(code: str) -> tuple[str | None, str | None]:
    """ì—­í• :
    - PL/SQL ì„ ì–¸ë¶€(ë˜ëŠ” CREATE êµ¬ë¬¸)ì—ì„œ PROCEDURE/FUNCTION/TRIGGERì˜ ì´ë¦„ì„ ì •ê·œì‹ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
    - code (str): ë¼ì¸ ë²ˆí˜¸ ì ‘ë‘ê°€ í¬í•¨ë  ìˆ˜ ìˆëŠ” ì›ë³¸ ì½”ë“œ ì¡°ê°.

    ë°˜í™˜ê°’:
    - tuple[Optional[str], Optional[str]]: (schema_name, name)

    """
    try:
        normalized = re.sub(r'^\d+\s*:\s*', '', code, flags=re.MULTILINE)

        # ì „ì²´ ì‹ë³„ìì—ì„œ ìµœëŒ€ 2ê°œì˜ ì ê¹Œì§€ í—ˆìš©
        pattern = re.compile(
            r"\b(?:CREATE\s+(?:OR\s+REPLACE\s+)?)?"
            r"(?:PROCEDURE|FUNCTION|TRIGGER)\s+"
            r"((?:\"[^\"]+\"|[A-Za-z_][\w$#]*)"
            r"(?:\s*\.\s*(?:\"[^\"]+\"|[A-Za-z_][\w$#]*)){0,2})",
            re.IGNORECASE
        )

        match = pattern.search(normalized)
        if not match:
            return None, None

        full = match.group(1)
        parts = [p.strip().strip('"') for p in re.split(r"\s*\.\s*", full)]
        if len(parts) == 3:
            return parts[0], f"{parts[1]}.{parts[2]}"
        if len(parts) == 2:
            return parts[0], parts[1]
        if len(parts) == 1:
            return None, parts[0]
        return None, None
    except Exception as e:
        logging.error(f"í”„ë¡œì‹œì €/í•¨ìˆ˜/íŠ¸ë¦¬ê±° ëª… ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None, None


def summarize_with_placeholders(file_content: str, node: dict) -> str:
    """ì—­í• :
    - ë…¸ë“œ ë²”ìœ„ì˜ ì½”ë“œë¥¼ ê°€ì ¸ì˜¤ë˜ ìì‹ ë²”ìœ„ëŠ” `"start: ... code ..."` í”Œë ˆì´ìŠ¤í™€ë”ë¡œ ì¹˜í™˜í•©ë‹ˆë‹¤.
    - ë¼ì¸ ë²ˆí˜¸ ì ‘ë‘ë¥¼ ìœ ì§€í•˜ì—¬ ì¶”í›„ ë¼ì¸ ë§¤í•‘ì„ ê°€ëŠ¥ì¼€ í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
    - file_content (str): ì „ì²´ íŒŒì¼ ë‚´ìš©.
    - node (dict): `{"startLine": int, "endLine": int, "children": list[dict]}` í˜•íƒœì˜ ë…¸ë“œ.

    ë°˜í™˜ê°’:
    - str: ìš”ì•½ëœ ì½”ë“œ ë¬¸ìì—´.
    """

    def summarize_code(start_line, end_line, children):

        lines = file_content.split('\n')  
        code_lines = lines[start_line-1:end_line]
        summarized_code = []
        last_end_line = start_line - 1
        line_number_pattern = r'^\d+\s*:'

        for child in children:
            before_child_code = code_lines[last_end_line-start_line+1:child['startLine']-start_line]
            
            for i, line in enumerate(before_child_code):
                line_number = i + last_end_line + 1
                if re.match(line_number_pattern, line):
                    summarized_code.append(f"{line}\n")
                else:
                    summarized_code.append(f"{line_number}: {line}\n")
            
            summarized_code.append(f"{child['startLine']}: ... code ...\n")
            last_end_line = child['endLine']

        after_last_child_code = code_lines[last_end_line-start_line+1:]
        
        for i, line in enumerate(after_last_child_code):
            line_number = i + last_end_line + 1
            if re.match(line_number_pattern, line):
                summarized_code.append(f"{line}\n")
            else:
                summarized_code.append(f"{line_number}: {line}\n")
        
        return ''.join(summarized_code)
    

    try:
        if not node.get('children'):
            lines = file_content.split('\n')  
            code_lines = lines[node['startLine']-1:node['endLine']] 
            line_number_pattern = r'^\d+\s*:'

            result = []
            for i, line in enumerate(code_lines):
                line_number = i + node['startLine']
                if re.match(line_number_pattern, line):
                    result.append(f"{line}\n")
                else:
                    result.append(f"{line_number}: {line}\n")
            return ''.join(result)
        else:
            return summarize_code(node['startLine'], node['endLine'], node.get('children', []))
    
    except Exception as e:
        err_msg = f"Understanding ê³¼ì •ì—ì„œ ì½”ë“œë¥¼ ìš”ì•½í•˜ëŠ” ë„ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        logging.error(err_msg)
        raise ProcessAnalyzeCodeError(err_msg)


def build_sp_code(current_schedule: dict, schedule_stack: list) -> str:
    """ì—­í• :
    - í˜„ì¬ ìŠ¤ì¼€ì¤„ì—ì„œ ì‹œì‘í•˜ì—¬ ìƒìœ„ ìŠ¤ì¼€ì¤„ì„ ì—­ìˆœ ì ìš©, ...code... í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‹¤ì œ ìš”ì•½ ì½”ë“œë¡œ ì¹˜í™˜í•˜ì—¬, ì‹¤ì œ ë¶„ì„í•  sp ì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤

    ë§¤ê°œë³€ìˆ˜:
    - current_schedule (dict): í˜„ì¬ ë…¸ë“œì˜ ìš”ì•½ ìŠ¤ì¼€ì¤„.
    - schedule_stack (list[dict]): ìƒìœ„ ë…¸ë“œë“¤ì˜ ìš”ì•½ ìŠ¤ì¼€ì¤„ ìŠ¤íƒ.

    ë°˜í™˜ê°’:
    - str: ë¶„ì„í•  sp ì½”ë“œ.
    """
    try:
        focused_code = current_schedule["code"]
        current_start_line = current_schedule["startLine"]
        for schedule in reversed(schedule_stack):
            placeholder = f"{current_start_line}: ... code ..."
            schedule_code = schedule["code"]
            replaced = False
            if placeholder in schedule_code:
                schedule_code = schedule_code.replace(placeholder, focused_code, 1)
                replaced = True
            else:
                # ìš”ì•½ ì•µì»¤(ì˜ˆ: "{start}~{end}: summary")ë„ ì¹˜í™˜ ëŒ€ìƒìœ¼ë¡œ í—ˆìš©
                anchor_pat = re.compile(rf"^{current_start_line}~\d+:\s.*$", re.MULTILINE)
                if anchor_pat.search(schedule_code):
                    schedule_code = anchor_pat.sub(focused_code, schedule_code, count=1)
                    replaced = True
            if replaced:
                focused_code = schedule_code
                current_start_line = schedule["startLine"]
        return focused_code

    except Exception as e:
        err_msg = f"Understanding ê³¼ì •ì—ì„œ ë¶„ì„í•  ì½”ë“œ ìƒì„± ë„ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        logging.error(err_msg)
        raise ProcessAnalyzeCodeError(err_msg)
    

def get_original_node_code(file_content: str, start_line: int, end_line: int) -> str:
    """ì—­í• :
    - ì§€ì • ë¼ì¸ ë²”ìœ„ë¥¼ ì¶”ì¶œí•˜ê³  ê° ë¼ì¸ì— ë¼ì¸ ë²ˆí˜¸ ì ‘ë‘(`"N: "`)ê°€ ì¡´ì¬í•˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
    - file_content (str): ì „ì²´ íŒŒì¼ ë‚´ìš©.
    - start_line (int): ì¶”ì¶œ ì‹œì‘ ë¼ì¸.
    - end_line (int): ì¶”ì¶œ ì¢…ë£Œ ë¼ì¸(0ì´ë©´ ì‹œì‘ ë¼ì¸ê³¼ ë™ì¼ ì²˜ë¦¬).

    ë°˜í™˜ê°’:
    - str: ë¼ì¸ ë²ˆí˜¸ ì ‘ë‘ê°€ ë³´ì¥ëœ í…ìŠ¤íŠ¸.
    """
    try:
        if end_line == 0:
            end_line = start_line
        lines = file_content.split('\n')
        extracted_lines = lines[start_line-1:end_line]
        line_number_pattern = r'^\d+\s*:'
        extracted_node_code = []
        for i, line in enumerate(extracted_lines):
            if re.match(line_number_pattern, line):
                extracted_node_code.append(line)
            else:
                extracted_node_code.append(f"{i + start_line}: {line}")
        
        return '\n'.join(extracted_node_code)
    
    except Exception as e:
        err_msg = f"Understanding ê³¼ì •ì—ì„œ ë…¸ë“œì— ë§ê²Œ ì½”ë“œë¥¼ ì¶”ì¶œ ë„ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        logging.error(err_msg)
        raise ProcessAnalyzeCodeError(err_msg)


def clean_field_name(field_name: str) -> str:
    """ì—­í• :
    - `"{TYPE:NAME}"` ë˜ëŠ” ìœ ì‚¬ í‘œê¸°ì—ì„œ NAMEë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
    - field_name (str): ì›ë³¸ í•„ë“œ í‘œê¸° ë¬¸ìì—´.

    ë°˜í™˜ê°’:
    - str: ì¶”ì¶œëœ ì´ë¦„ ë˜ëŠ” ì›ë³¸ ë¬¸ìì—´.
    """
    match = re.search(r'\{(.+?)\}', field_name)
    if match:
        return match.group(1)
    return field_name


# ==================== ì„¹ì…˜: ë¶„ì„ Understanding íŒŒì´í”„ë¼ì¸(ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸) ====================
class Analyzer:
    """ë¶„ì„ íŒŒì´í”„ë¼ì¸ì„ ë‹´ë‹¹í•˜ëŠ” ìƒíƒœ ë³´ìœ í˜• í´ë˜ìŠ¤.

    ì—­í• :
    - ANTLR ASTë¥¼ DFSë¡œ ìˆœíšŒí•˜ë©° ìš”ì•½ ì½”ë“œ ëˆ„ì , ë°°ì¹˜ í”ŒëŸ¬ì‹œ, LLM ë¶„ì„, Neo4j ì‚¬ì´í¼ ìƒì„±ê¹Œì§€ ì „ì²´ íë¦„ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
    - ì¤‘ì²© í•¨ìˆ˜ì™€ nonlocal ê³µìœ  ìƒíƒœë¥¼ ì œê±°í•˜ì—¬ ê°€ë…ì„±ê³¼ ìœ ì§€ë³´ìˆ˜ì„±ì„ í–¥ìƒí•©ë‹ˆë‹¤.

    ë³´ì¥ ì‚¬í•­:
    - ê¸°ì¡´ ê¸°ëŠ¥/ì‚¬ì´ë“œì´í™íŠ¸(í í”„ë¡œí† ì½œ, ì‚¬ì´í¼ ì¿¼ë¦¬, í† í° ì„ê³„ì¹˜, ìš”ì•½/ê´€ê³„ ìƒì„±)ì™€ ì™„ì „ ë™ì¼í•˜ê²Œ ë™ì‘í•©ë‹ˆë‹¤.
    """

    def __init__(self, antlr_data: dict, file_content: str, send_queue: asyncio.Queue, receive_queue: asyncio.Queue, last_line: int, folder_name: str, file_name: str, user_id: str, api_key: str, locale: str):
        """ìƒì„±ì

        ë§¤ê°œë³€ìˆ˜:
        - antlr_data: ANTLR íŒŒì„œê°€ ìƒì„±í•œ AST ë£¨íŠ¸ ë…¸ë“œ
        - file_content: ë¼ì¸ ë²ˆí˜¸ ì ‘ë‘ê°€ í¬í•¨ëœ ì›ë³¸ ì½”ë“œ í…ìŠ¤íŠ¸
        - send_queue: ìƒì„±ëœ ì‚¬ì´í¼ ì¿¼ë¦¬ ë°°ì¹˜ë¥¼ ì†¡ì‹ í•˜ëŠ” í
        - receive_queue: ì‚¬ì´í¼ ì²˜ë¦¬ ì™„ë£Œ ì‹ í˜¸ë¥¼ ìˆ˜ì‹ í•˜ëŠ” í
        - last_line: íŒŒì¼ ë§ˆì§€ë§‰ ë¼ì¸ ë²ˆí˜¸(ì”ì—¬ ë°°ì¹˜ í”ŒëŸ¬ì‹œìš©)
        - folder_name: í´ë” ì´ë¦„(Neo4j í‚¤)
        - file_name: íŒŒì¼ ì´ë¦„(Neo4j í‚¤)
        - user_id: ì‚¬ìš©ì ì‹ë³„ì(Neo4j íŒŒí‹°ì…”ë‹)
        - api_key: LLM í˜¸ì¶œì— ì‚¬ìš©í•  API í‚¤
        - locale: ë¡œì¼€ì¼ ì½”ë“œ('ko'|'en')
        """
        self.antlr_data = antlr_data
        self.file_content = file_content
        self.send_queue = send_queue
        self.receive_queue = receive_queue
        self.last_line = last_line
        self.folder_name = folder_name
        self.file_name = file_name
        self.user_id = user_id
        self.api_key = api_key
        self.locale = locale

        self.schedule_stack = []
        self.context_range = []
        self.cypher_query = []
        self.summary_dict = {}
        self.node_statement_types = set()
        self.procedure_name = None
        self.extract_code = ""
        self.focused_code = ""
        self.sp_token_count = 0


    async def run(self):
        """ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰.

        - DFS ìˆœíšŒ ì‹œì‘â†’ì”ì—¬ ë°°ì¹˜ í”ŒëŸ¬ì‹œâ†’ì™„ë£Œ ì´ë²¤íŠ¸ ì†¡ì‹ ê¹Œì§€ ë‹´ë‹¹í•©ë‹ˆë‹¤.
        - ì˜¤ë¥˜ ë°œìƒ ì‹œ íë¡œ ì—ëŸ¬ ì´ë²¤íŠ¸ë¥¼ ì „ì†¡í•˜ê³  ì˜ˆì™¸ë¥¼ ì „íŒŒí•©ë‹ˆë‹¤.
        """
        logging.info(f"ğŸ“‹ [{self.folder_name}/{self.file_name}] ì½”ë“œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤ (ì´ {self.last_line}ì¤„)")
        try:
            await self.analyze_statement_tree(self.antlr_data, self.schedule_stack)

            if self.context_range and self.focused_code:
                self.extract_code, _ = extract_code_within_range(self.focused_code, self.context_range)
                await self.send_analysis_event_and_wait(self.last_line)
            logging.info(f"âœ… [{self.folder_name}/{self.file_name}] ì½”ë“œ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤")
            await self.send_queue.put({"type": "end_analysis"})

        except UnderstandingError as e:
            await self.send_queue.put({'type': 'error', 'message': str(e)})
            raise
        except Exception as e:
            err_msg = f"Understanding ê³¼ì •ì—ì„œ Traverseë¡œ ìŠ¤í† ì–´ë“œ í”„ë¡œì‹œì € ì½”ë“œë¥¼ ìˆœíšŒí•˜ëŠ” ë„ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            logging.error(err_msg)
            await self.send_queue.put({'type': 'error', 'message': err_msg})
            raise ProcessAnalyzeCodeError(err_msg)


    async def execute_analysis_and_reset_state(self, statement_type: str) -> list:
        """ëˆ„ì  ì»¨í…ìŠ¤íŠ¸ë¥¼ LLMì— ì „ë‹¬í•´ ì‹¤ì œ ë¶„ì„ì„ ì‹¤í–‰í•˜ê³ , ë‚´ë¶€ ìƒíƒœë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        ë§¤ê°œë³€ìˆ˜:
        - statement_type: í”ŒëŸ¬ì‹œ ê¸°ì¤€ ìƒìœ„ êµ¬ë¬¸ íƒ€ì…(PROCEDURE/FUNCTION ë“±)

        ë°˜í™˜ê°’:
        - list[str]: ìƒì„±ëœ ì‚¬ì´í¼ ì¿¼ë¦¬ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
        """
        try:
            context_range_count = len(self.context_range)
            self.context_range = sorted(self.context_range, key=lambda x: x['startLine'])

            analysis_result = understand_code(self.extract_code, self.context_range, context_range_count, self.api_key, self.locale)
            cypher_queries = await self.process_analysis_output_to_cypher(analysis_result)

            actual_count = len(analysis_result["analysis"])
            if actual_count != context_range_count:
                logging.error(f"ë¶„ì„ ê²°ê³¼ ê°œìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì˜ˆìƒ: {context_range_count}, ì‹¤ì œ: {actual_count}")

            if statement_type in PROCEDURE_TYPES:
                logging.info(f"[{self.folder_name}-{self.file_name}] {self.procedure_name} í”„ë¡œì‹œì €ì˜ ìš”ì•½ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ")
                summary = understand_summary(self.summary_dict, self.api_key, self.locale)
                self.cypher_query.append(f"""
                    MATCH (n:{statement_type})
                    WHERE n.folder_name = '{self.folder_name}' AND n.file_name = '{self.file_name}'
                        AND n.procedure_name = '{self.procedure_name}'
                        AND n.user_id = '{self.user_id}'
                    SET n.summary = {json.dumps(summary['summary'])}
                """)
                self.schedule_stack.clear()
                self.node_statement_types.clear()
                self.summary_dict.clear()

            self.focused_code = ""
            self.extract_code = ""
            self.sp_token_count = 0
            self.context_range.clear()
            return cypher_queries

        except UnderstandingError:
            raise
        except Exception as e:
            err_msg = f"Understanding ê³¼ì •ì—ì„œ LLMì˜ ê²°ê³¼ ì²˜ë¦¬ë¥¼ ì¤€ë¹„ ë° ì‹œì‘í•˜ëŠ” ë„ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            logging.error(err_msg)
            raise ProcessAnalyzeCodeError(err_msg)


    async def process_analysis_output_to_cypher(self, analysis_result: dict) -> list:
        """LLM ë¶„ì„ ê²°ê³¼ë¥¼ ì²˜ë¦¬í•˜ì—¬ Neo4j ì‚¬ì´í¼ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        ë§¤ê°œë³€ìˆ˜:
        - analysis_result: LLM ë¶„ì„ ê²°ê³¼(JSON í˜¸í™˜ dict)

        ë°˜í™˜ê°’:
        - list[str]: í˜„ì¬ê¹Œì§€ ëˆ„ì ëœ ì‚¬ì´í¼ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        try:
            for result in analysis_result['analysis']:
                start_line = result['startLine']
                end_line = result['endLine']
                summary = result['summary']
                local_tables = result.get('localTables', [])
                db_links = result.get('dbLinks', [])
                called_nodes = result.get('calls', [])
                variables = result.get('variables', [])
                var_range = f"{start_line}_{end_line}"

                statement_type = get_statement_type(start_line, end_line, self.node_statement_types)
                table_relationship_type = get_table_relationship(statement_type)

                summary_key = f"{statement_type}_{start_line}_{end_line}"
                self.summary_dict[summary_key] = summary

                summary_query = f"""
                    MATCH (n:{statement_type} {{startLine: {start_line}, folder_name: '{self.folder_name}', file_name: '{self.file_name}', user_id: '{self.user_id}'}})
                    SET n.summary = {json.dumps(summary)}
                """
                self.cypher_query.append(summary_query)

                pattern = re.compile(rf"^{start_line}: \.\.\. code \.\.\.$", re.MULTILINE)
                for schedule in self.schedule_stack:
                    if pattern.search(schedule["code"]):
                        schedule["code"] = pattern.sub(f"{start_line}~{end_line}: {summary}", schedule["code"])

                for var_name in variables:
                    variable_usage_query = f"""
                        MATCH (v:Variable {{name: '{var_name}', folder_name: '{self.folder_name}', file_name: '{self.file_name}', procedure_name: '{self.procedure_name}', user_id: '{self.user_id}'}})
                        SET v.`{var_range}` = 'Used'
                    """
                    self.cypher_query.append(variable_usage_query)

                if statement_type in ["CALL", "ASSIGNMENT"]:
                    if statement_type == "ASSIGNMENT" and called_nodes:
                        label_change_query = f"""
                            MATCH (a:ASSIGNMENT {{startLine: {start_line}, folder_name: '{self.folder_name}', file_name: '{self.file_name}', user_id: '{self.user_id}'}})
                            REMOVE a:ASSIGNMENT
                            SET a:CALL, a.name = 'CALL[{start_line}]'
                        """
                        self.cypher_query.append(label_change_query)
                        statement_type = "CALL"

                    if called_nodes:
                        for name in called_nodes:
                            if '.' in name:
                                package_name, proc_name = name.split('.')
                                package_name = package_name.upper()
                                proc_name = proc_name.upper()

                                call_relation_query = f"""
                                    MATCH (c:{statement_type} {{startLine: {start_line}, folder_name: '{self.folder_name}', file_name: '{self.file_name}', user_id: '{self.user_id}'}}) 
                                    OPTIONAL MATCH (p)
                                    WHERE (p:PROCEDURE OR p:FUNCTION)
                                    AND p.folder_name = '{package_name}' 
                                    AND p.procedure_name = '{proc_name}'
                                    AND p.user_id = '{self.user_id}'
                                    WITH c, p
                                    FOREACH(ignoreMe IN CASE WHEN p IS NULL THEN [1] ELSE [] END |
                                        CREATE (new:PROCEDURE:FUNCTION {{folder_name: '{package_name}', procedure_name: '{proc_name}', user_id: '{self.user_id}'}})
                                        MERGE (c)-[:CALL {{scope: 'external'}}]->(new)
                                    )
                                    FOREACH(ignoreMe IN CASE WHEN p IS NOT NULL THEN [1] ELSE [] END |
                                        MERGE (c)-[:CALL {{scope: 'external'}}]->(p)
                                    )
                                """
                                self.cypher_query.append(call_relation_query)
                            else:
                                call_relation_query = f"""
                                    MATCH (c:{statement_type} {{startLine: {start_line}, folder_name: '{self.folder_name}', file_name: '{self.file_name}', user_id: '{self.user_id}'}})
                                    WITH c
                                    MATCH (p {{folder_name: '{self.folder_name}', file_name: '{self.file_name}', procedure_name: '{name}', user_id: '{self.user_id}'}} )
                                    WHERE p:PROCEDURE OR p:FUNCTION
                                    MERGE (c)-[:CALL {{scope: 'internal'}}]->(p)
                                """
                                self.cypher_query.append(call_relation_query)

                for tn in local_tables:
                    qualified = str(tn).strip().upper()
                    if not qualified:
                        continue
                    schema_part, name_part, _ = parse_table_identifier(qualified)
                    relationship_label = table_relationship_type
                    if not relationship_label:
                        continue

                    merge_table = (
                        f"MERGE (t:Table {{user_id: '{self.user_id}', name: '{name_part}', schema: '{schema_part}'}})\n"
                        if schema_part else
                        f"MERGE (t:Table {{user_id: '{self.user_id}', name: '{name_part}'}})\n"
                    )

                    table_relationship_query = f"""
                        MERGE (n:{statement_type} {{startLine: {start_line}, folder_name: '{self.folder_name}', file_name: '{self.file_name}', user_id: '{self.user_id}'}})
                        WITH n
                        {merge_table}
                        ON CREATE SET t.folder_name = '{self.folder_name}'
                        ON MATCH  SET t.folder_name = CASE WHEN coalesce(t.folder_name,'') = '' THEN '{self.folder_name}' ELSE t.folder_name END
                        WITH n, t
                        MERGE (folder:Folder {{user_id: '{self.user_id}', name: '{self.folder_name}'}})
                        MERGE (folder)-[:CONTAINS]->(t)
                        MERGE (n)-[:{relationship_label}]->(t)
                    """
                    self.cypher_query.append(table_relationship_query)

                # fkRelations ë³‘í•© ì²˜ë¦¬: FK(ì†ŒìŠ¤) â†’ PK/UK(íƒ€ê²Ÿ)
                fk_relations = result.get('fkRelations', [])
                for fk in fk_relations:
                    src_table = (fk.get('sourceTable') or '').strip().upper()
                    src_column = (fk.get('sourceColumn') or '').strip()
                    tgt_table = (fk.get('targetTable') or '').strip().upper()
                    tgt_column = (fk.get('targetColumn') or '').strip()
                    if not (src_table and src_column and tgt_table and tgt_column):
                        continue

                    # íŒŒì‹±: SCHEMA.TABLE
                    def split_table(qualified_table: str) -> tuple[str | None, str]:
                        if '.' in qualified_table:
                            s, t = qualified_table.split('.', 1)
                            return (s or '').upper(), (t or '').upper()
                        return None, qualified_table.upper()

                    src_schema, src_table_name = split_table(src_table)
                    tgt_schema, tgt_table_name = split_table(tgt_table)

                    # Table ë…¸ë“œ MERGE (ì†ŒìŠ¤/íƒ€ê²Ÿ)
                    src_t_merge_key = {
                        'user_id': self.user_id,
                        'schema': (src_schema or ''),
                        'name': src_table_name,
                    }
                    tgt_t_merge_key = {
                        'user_id': self.user_id,
                        'schema': (tgt_schema or ''),
                        'name': tgt_table_name,
                    }
                    src_t_merge_key_str = ', '.join(f"`{k}`: '{v}'" for k, v in src_t_merge_key.items())
                    tgt_t_merge_key_str = ', '.join(f"`{k}`: '{v}'" for k, v in tgt_t_merge_key.items())

                    # FK_TO_TABLE ê´€ê³„ (ë…¸ë“œ ìƒì„± ê¸ˆì§€: MATCHë¡œ ë°”ì¸ë”©, ê´€ê³„ë§Œ MERGE)
                    self.cypher_query.append(
                        f"MATCH (st:Table {{{src_t_merge_key_str}}}), (tt:Table {{{tgt_t_merge_key_str}}}) MERGE (st)-[:FK_TO_TABLE]->(tt)"
                    )

                    # Column ë…¸ë“œ MERGE ë° FK_TO ê´€ê³„
                    src_fqn = '.'.join([p for p in [(src_schema or ''), src_table_name, src_column] if p]).lower()
                    tgt_fqn = '.'.join([p for p in [(tgt_schema or ''), tgt_table_name, tgt_column] if p]).lower()

                    src_c_key = { 'user_id': self.user_id, 'name': src_column, 'fqn': src_fqn }
                    tgt_c_key = { 'user_id': self.user_id, 'name': tgt_column, 'fqn': tgt_fqn }
                    src_c_key_str = ', '.join(f"`{k}`: '{v}'" for k, v in src_c_key.items())
                    tgt_c_key_str = ', '.join(f"`{k}`: '{v}'" for k, v in tgt_c_key.items())

                    # ì»¬ëŸ¼ ë…¸ë“œ ìƒì„± ê¸ˆì§€: MATCHë¡œ ë°”ì¸ë”©, ê´€ê³„ë§Œ MERGE
                    self.cypher_query.append(
                        f"MATCH (sc:Column {{{src_c_key_str}}}), (dc:Column {{{tgt_c_key_str}}}) MERGE (sc)-[:FK_TO]->(dc)"
                    )

                for link_item in db_links:
                    mode = (link_item.get('mode') or 'r').lower()
                    name = (link_item.get('name') or '').strip().upper()
                    schema_part, name_part, link_name = parse_table_identifier(name)
                    relationship_label = table_relationship_type
                    if not relationship_label:
                        continue

                    merge_table = (
                        f"MERGE (t:Table {{user_id: '{self.user_id}', name: '{name_part}', schema: '{schema_part}'}})\n"
                        if schema_part else
                        f"MERGE (t:Table {{user_id: '{self.user_id}', name: '{name_part}'}})\n"
                    )

                    table_relationship_query = f"""
                        MERGE (n:{statement_type} {{startLine: {start_line}, folder_name: '{self.folder_name}', file_name: '{self.file_name}', user_id: '{self.user_id}'}})
                        WITH n
                        {merge_table}
                        ON CREATE SET t.folder_name = ''
                        SET t.db_link = '{link_name}'
                        WITH n, t
                        MERGE (l:DBLink {{user_id: '{self.user_id}', name: '{link_name}'}})
                        MERGE (l)-[:CONTAINS]->(t)
                        MERGE (n)-[:DB_LINK {{mode: '{mode}'}}]->(t)
                    """
                    self.cypher_query.append(table_relationship_query)

            return self.cypher_query

        except Exception as e:
            err_msg = f"Understanding ê³¼ì •ì—ì„œ LLMì˜ ê²°ê³¼ë¥¼ ì´ìš©í•´ ì‚¬ì´í¼ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ëŠ” ë„ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            logging.error(err_msg)
            raise ProcessAnalyzeCodeError(err_msg)


    def analyze_variable_declarations(self, declaration_code: str, node_startLine: int, statement_type: str):
        """ë³€ìˆ˜ ì„ ì–¸ë¶€(SPEC/DECLARE/PACKAGE_VARIABLE)ë¥¼ ë¶„ì„í•˜ê³  Variable ë…¸ë“œ/Scope ê´€ê³„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        ë§¤ê°œë³€ìˆ˜:
        - declaration_code: ì„ ì–¸ë¶€ ì½”ë“œ ì¡°ê°
        - node_startLine: ì„ ì–¸ ë…¸ë“œ ì‹œì‘ ë¼ì¸
        - statement_type: ì„ ì–¸ ìœ í˜•(SPEC/DECLARE/PACKAGE_VARIABLE)
        """
        try:
            role = ('íŒ¨í‚¤ì§€ ì „ì—­ ë³€ìˆ˜' if statement_type == 'PACKAGE_VARIABLE' else
                    'ë³€ìˆ˜ ì„ ì–¸ë° ì´ˆê¸°í™”' if statement_type == 'DECLARE' else
                    'í•¨ìˆ˜ ë° í”„ë¡œì‹œì € ì…ë ¥ ë§¤ê°œë³€ìˆ˜' if statement_type == 'SPEC' else
                    'ì•Œ ìˆ˜ ì—†ëŠ” ë§¤ê°œë³€ìˆ˜')
            logging.info(f"[{self.folder_name}-{self.file_name}] {self.procedure_name}ì˜ ë³€ìˆ˜ ë¶„ì„ ì‹œì‘")
            analysis_result = understand_variables(declaration_code, self.api_key, self.locale)
            logging.info(f"[{self.folder_name}-{self.file_name}] {self.procedure_name}ì˜ ë³€ìˆ˜ ë¶„ì„ ì™„ë£Œ")
            var_summary = json.dumps(analysis_result.get("summary", "unknown"))
            for variable in analysis_result["variables"]:
                var_parameter_type = variable["parameter_type"]
                var_name = variable["name"]
                var_type = variable["type"]
                var_value = variable["value"]
                var_value = '' if var_value is None else var_value

                if statement_type == 'DECLARE':
                    cypher_query = f"""
                    MERGE (v:Variable {{name: '{var_name}', folder_name: '{self.folder_name}', file_name: '{self.file_name}', type: '{var_type}', parameter_type: '{var_parameter_type}', procedure_name: '{self.procedure_name}', role: '{role}', scope: 'Local', value: {json.dumps(var_value)}, user_id: '{self.user_id}'}})
                    WITH v
                    MATCH (p:{statement_type} {{startLine: {node_startLine}, folder_name: '{self.folder_name}', file_name: '{self.file_name}', procedure_name: '{self.procedure_name}', user_id: '{self.user_id}'}})
                    SET p.summary = {var_summary}
                    WITH p, v
                    MERGE (p)-[:SCOPE]->(v)
                    WITH v
                    MERGE (folder:Folder {{user_id: '{self.user_id}', name: '{self.folder_name}'}})
                    MERGE (folder)-[:CONTAINS]->(v)
                    """
                    self.cypher_query.append(cypher_query)
                elif statement_type == 'PACKAGE_VARIABLE':
                    cypher_query = f"""
                    MERGE (v:Variable {{name: '{var_name}', folder_name: '{self.folder_name}', file_name: '{self.file_name}', type: '{var_type}', parameter_type: '{var_parameter_type}', role: '{role}', scope: 'Global', value: {json.dumps(var_value)}, user_id: '{self.user_id}'}})
                    WITH v
                    MATCH (p:{statement_type} {{startLine: {node_startLine}, folder_name: '{self.folder_name}', file_name: '{self.file_name}', user_id: '{self.user_id}'}})
                    SET p.summary = {var_summary}
                    WITH p, v
                    MERGE (p)-[:SCOPE]->(v)
                    WITH v
                    MERGE (folder:Folder {{user_id: '{self.user_id}', name: '{self.folder_name}'}})
                    MERGE (folder)-[:CONTAINS]->(v)
                    """
                    self.cypher_query.append(cypher_query)
                else:
                    cypher_query = f"""
                    MERGE (v:Variable {{name: '{var_name}', folder_name: '{self.folder_name}', file_name: '{self.file_name}', type: '{var_type}', parameter_type: '{var_parameter_type}', procedure_name: '{self.procedure_name}', role: '{role}', scope: 'Local', value: {json.dumps(var_value)}, user_id: '{self.user_id}'}})
                    WITH v
                    MATCH (p:{statement_type} {{startLine: {node_startLine}, folder_name: '{self.folder_name}', file_name: '{self.file_name}', procedure_name: '{self.procedure_name}', user_id: '{self.user_id}'}})
                    SET p.summary = {var_summary}
                    WITH p, v
                    MERGE (p)-[:SCOPE]->(v)
                    WITH v
                    MERGE (folder:Folder {{user_id: '{self.user_id}', name: '{self.folder_name}'}})
                    MERGE (folder)-[:CONTAINS]->(v)
                    """
                    self.cypher_query.append(cypher_query)

        except LLMCallError:
            raise
        except Exception as e:
            err_msg = f"Understanding ê³¼ì •ì—ì„œ í”„ë¡œì‹œì € ì„ ì–¸ë¶€ ë¶„ì„ ë° ë³€ìˆ˜ ë…¸ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            logging.error(err_msg)
            raise ProcessAnalyzeCodeError(err_msg)


    async def send_analysis_event_and_wait(self, node_end_line: int, statement_type: str = None):
        """ë¶„ì„ ê²°ê³¼ ì´ë²¤íŠ¸ë¥¼ ì†¡ì‹ í•˜ê³  ì²˜ë¦¬ ì™„ë£Œ ì´ë²¤íŠ¸ë¥¼ ìˆ˜ì‹ í•  ë•Œê¹Œì§€ ëŒ€ê¸°í•©ë‹ˆë‹¤.

        ë§¤ê°œë³€ìˆ˜:
        - node_end_line: í•´ë‹¹ ë°°ì¹˜ì˜ ê¸°ì¤€ì´ ë˜ëŠ” ë§ˆì§€ë§‰ ë¼ì¸ ë²ˆí˜¸
        - statement_type: í”ŒëŸ¬ì‹œ ê¸°ì¤€ ìƒìœ„ êµ¬ë¬¸ íƒ€ì…
        """
        try:
            logging.info(f"ğŸ¤– [{self.folder_name}-{self.file_name}] AI ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            results = await self.execute_analysis_and_reset_state(statement_type)
            logging.info(f"ğŸ“¤ [{self.folder_name}-{self.file_name}] ë¶„ì„ ê²°ê³¼ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤ (Cypher ì¿¼ë¦¬ {len(results)}ê°œ)")
            await self.send_queue.put({"type": "analysis_code", "query_data": results, "line_number": node_end_line})

            while True:
                response = await self.receive_queue.get()
                if response['type'] == 'process_completed':
                    logging.info(f"âœ… [{self.folder_name}] NEO4Jì— ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤\n")
                    self.cypher_query.clear();
                    break;

        except UnderstandingError:
            raise
        except Exception as e:
            err_msg = f"Understanding ê³¼ì •ì—ì„œ ì´ë²¤íŠ¸ë¥¼ ì†¡ì‹ í•˜ê³  ìˆ˜ì‹ í•˜ëŠ” ë„ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            logging.error(err_msg)
            raise ProcessAnalyzeCodeError(err_msg)


    async def analyze_statement_tree(self, node: dict, schedule_stack: list, parent_startLine: int = None, parent_statementType: str = None):
        """ë¬¸(statement) íŠ¸ë¦¬ë¥¼ ë¶„ì„í•˜ë©° ë…¸ë“œ/ê´€ê³„ ìƒì„±, ìš”ì•½ ì¡°ë¦½, ë°°ì¹˜ í”ŒëŸ¬ì‹œë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

        ë§¤ê°œë³€ìˆ˜:
        - node: í˜„ì¬ ë°©ë¬¸í•  ë…¸ë“œ
        - schedule_stack: ìƒìœ„ ë…¸ë“œë“¤ì˜ ìš”ì•½ ìŠ¤ì¼€ì¤„ ìŠ¤íƒ
        - parent_startLine: ë¶€ëª¨ ë…¸ë“œ ì‹œì‘ ë¼ì¸
        - parent_statementType: ë¶€ëª¨ ë…¸ë“œ íƒ€ì…
        """
        start_line, end_line, statement_type = node['startLine'], node['endLine'], node['type']
        summarized_code = summarize_with_placeholders(self.file_content, node)
        node_code = get_original_node_code(self.file_content, start_line, end_line)
        node_size = calculate_code_token(node_code)
        children = node.get('children', [])
        has_children_value = str(bool(children)).lower()
        logging.info(f"ğŸš€ ë…¸ë“œ ì •ë³´ : íƒ€ì… :{statement_type} ì‹œì‘ ë¼ì¸ :{start_line} ì¢…ë£Œ ë¼ì¸ :{end_line} ì‚¬ì´ì¦ˆ :{node_size} ìì‹ ì—¬ë¶€ :{has_children_value}")

        current_schedule = {
            "startLine": start_line,
            "endLine": end_line,
            "code": summarized_code,
            "child": children,
            "type": statement_type
        }

        if statement_type in PROCEDURE_TYPES:
            self.schema_name, self.procedure_name = get_procedure_name(node_code)
            logging.info(f"ğŸš€ í”„ë¡œì‹œì €/í•¨ìˆ˜/íŠ¸ë¦¬ê±° ì´ë¦„: {self.procedure_name}")

        self.extract_code, line_number = extract_code_within_range(self.focused_code, self.context_range)

        self.sp_token_count = calculate_code_token(self.extract_code)
        if is_over_token_limit(node_size, self.sp_token_count, len(self.context_range)):
            logging.info(f"âš ï¸ [{self.folder_name}-{self.file_name}] ë¦¬ë¯¸íŠ¸ì— ë„ë‹¬í•˜ì—¬ ì¤‘ê°„ ë¶„ì„ì„ ì‹¤í–‰í•©ë‹ˆë‹¤ (í† í°: {self.sp_token_count})")
            await self.send_analysis_event_and_wait(line_number)

        if not self.focused_code:
            self.focused_code = build_sp_code(current_schedule, schedule_stack)
        else:
            placeholder = f"{start_line}: ... code ..."
            if not self.focused_code or placeholder not in self.focused_code:
                self.focused_code = build_sp_code(current_schedule, schedule_stack)
            else:
                self.focused_code = self.focused_code.replace(placeholder, summarized_code, 1)

        if not children and statement_type not in NON_ANALYSIS_TYPES:
            self.context_range.append({"startLine": start_line, "endLine": end_line})
            self.cypher_query.append(f"""
                MERGE (n:{statement_type} {{startLine: {start_line}, folder_name: '{self.folder_name}', file_name: '{self.file_name}', user_id: '{self.user_id}'}})
                SET n.endLine = {end_line},
                    n.name = '{statement_type}[{start_line}]',
                    n.node_code = '{node_code.replace("'", "\\'")}',
                    n.token = {node_size},
                    n.procedure_name = '{self.procedure_name}',
                    n.has_children = {has_children_value}
                WITH n
                MERGE (folder:Folder {{user_id: '{self.user_id}', name: '{self.folder_name}'}})
                MERGE (folder)-[:CONTAINS]->(n)
            """)
        else:
            if statement_type == "FILE":
                file_summary = 'File Start Node' if self.locale == 'en' else 'íŒŒì¼ ë…¸ë“œ'
                self.cypher_query.append(f"""
                    MERGE (n:{statement_type} {{startLine: {start_line}, folder_name: '{self.folder_name}', file_name: '{self.file_name}', user_id: '{self.user_id}'}})
                    SET n.endLine = {end_line},
                        n.name = '{self.file_name}',
                        n.summary = '{file_summary}',
                        n.has_children = {has_children_value}
                    WITH n
                    MERGE (folder:Folder {{user_id: '{self.user_id}', name: '{self.folder_name}'}})
                    MERGE (folder)-[:CONTAINS]->(n)
                """)
            elif statement_type in ["PROCEDURE", "FUNCTION"]:
                self.cypher_query.append(f"""
                    MERGE (n:{statement_type} {{procedure_name: '{self.procedure_name}', folder_name: '{self.folder_name}', file_name: '{self.file_name}', user_id: '{self.user_id}'}})
                    SET n.startLine = {start_line},
                        n.endLine = {end_line},
                        n.name = '{statement_type}[{start_line}]',
                        n.summarized_code = '{escape_for_cypher_multiline(summarized_code)}',
                        n.node_code = '{escape_for_cypher_multiline(node_code)}',
                        n.token = {node_size},
                        n.has_children = {has_children_value}
                    WITH n
                    REMOVE n:{('FUNCTION' if statement_type == 'PROCEDURE' else 'PROCEDURE')}
                    WITH n
                    MERGE (folder:Folder {{user_id: '{self.user_id}', name: '{self.folder_name}'}})
                    MERGE (folder)-[:CONTAINS]->(n)
                """)
            else:
                self.cypher_query.append(f"""
                    MERGE (n:{statement_type} {{startLine: {start_line}, folder_name: '{self.folder_name}', file_name: '{self.file_name}', user_id: '{self.user_id}'}})
                    SET n.endLine = {end_line},
                        n.name = '{statement_type}[{start_line}]',
                        n.summarized_code = '{escape_for_cypher_multiline(summarized_code)}',
                        n.node_code = '{escape_for_cypher_multiline(node_code)}',
                        n.token = {node_size},
                        n.procedure_name = '{self.procedure_name}',
                        n.has_children = {has_children_value}
                    WITH n
                    MERGE (folder:Folder {{user_id: '{self.user_id}', name: '{self.folder_name}'}})
                    MERGE (folder)-[:CONTAINS]->(n)
                """)

        if (self.procedure_name and statement_type in ["SPEC", "DECLARE"]) or statement_type == "PACKAGE_VARIABLE":
            self.analyze_variable_declarations(node_code, start_line, statement_type)

        schedule_stack.append(current_schedule)
        self.node_statement_types.add(f"{statement_type}_{start_line}_{end_line}")

        if parent_statementType:
            self.cypher_query.append(f"""
                MATCH (parent:{parent_statementType} {{startLine: {parent_startLine}, folder_name: '{self.folder_name}', file_name: '{self.file_name}', user_id: '{self.user_id}'}})
                WITH parent
                MATCH (child:{statement_type} {{startLine: {start_line}, folder_name: '{self.folder_name}', file_name: '{self.file_name}', user_id: '{self.user_id}'}})
                MERGE (parent)-[:PARENT_OF]->(child)
            """)
        prev_statement = prev_id = None

        for child in children:
            await self.analyze_statement_tree(child, schedule_stack, start_line, statement_type)

            if prev_id and prev_statement not in NON_NEXT_RECURSIVE_TYPES:
                self.cypher_query.append(f"""
                    MATCH (prev:{prev_statement} {{startLine: {prev_id}, folder_name: '{self.folder_name}', file_name: '{self.file_name}', user_id: '{self.user_id}'}})
                    WITH prev
                    MATCH (current:{child['type']} {{startLine: {child['startLine']}, folder_name: '{self.folder_name}', file_name: '{self.file_name}', user_id: '{self.user_id}'}})
                    MERGE (prev)-[:NEXT]->(current)
                """)
            prev_statement, prev_id = child['type'], child['startLine']

        if children:
            if (statement_type in PROCEDURE_TYPES) and (self.context_range and self.focused_code):
                self.extract_code, line_number = extract_code_within_range(self.focused_code, self.context_range)
                logging.info(f"ğŸ“¤ [{self.folder_name}-{self.file_name}] ì¤‘ê°„ í”ŒëŸ¬ì‹œ ì‹¤í–‰ (í† í°: {self.sp_token_count})")
                await self.send_analysis_event_and_wait(line_number, statement_type)
            elif statement_type not in NON_ANALYSIS_TYPES:
                self.context_range.append({"startLine": start_line, "endLine": end_line})

        schedule_stack[:] = filter(lambda schedule: schedule['child'] and schedule['endLine'] > current_schedule['startLine'], schedule_stack)

