"""ë¦¬íŒ©í„°ë§ëœ Understanding íŒŒì´í”„ë¼ì¸ì˜ í•µì‹¬ êµ¬í˜„.

ì´ ëª¨ë“ˆì€ AST ìˆ˜ì§‘, ë°°ì¹˜ ê³„íš, ë³‘ë ¬ LLM í˜¸ì¶œ, Neo4j ë°˜ì˜ê¹Œì§€ì˜ ì „ ê³¼ì •ì„
ë¹„ë™ê¸° íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ êµ¬ì„±í•œë‹¤. í•¨ìˆ˜ë§ˆë‹¤ docstringì„ ì œê³µí•˜ì—¬ íë¦„ì„
ì²˜ìŒ ì ‘í•˜ëŠ” ê°œë°œìë„ ì „ì²´ ë‹¨ê³„ì™€ ë°ì´í„° ì´ë™ì„ ë¹ ë¥´ê²Œ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.
"""

from __future__ import annotations

import asyncio
import json
import logging
import re
import os
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Tuple

from understand.rules import understand_code
from util.exception import LLMCallError, ProcessAnalyzeCodeError, UnderstandingError
from util.utility_tool import calculate_code_token, escape_for_cypher, log_process
from understand.strategy.base_strategy import UnderstandingStrategy
from understand.strategy.dbms_strategy import DbmsUnderstandingStrategy


# ==================== ìƒìˆ˜ ì •ì˜ ====================
STATIC_QUERY_BATCH_SIZE = 40
VARIABLE_CONCURRENCY = int(os.getenv('VARIABLE_CONCURRENCY', '5'))
LINE_NUMBER_PATTERN = re.compile(r"^\d+\s*:")
MAX_BATCH_TOKEN = 1000
MAX_CONCURRENCY = int(os.getenv('MAX_CONCURRENCY', '5'))

# ==================== ë°ì´í„° í´ë˜ìŠ¤ ====================
@dataclass(slots=True)
class StatementNode:
    """í‰íƒ„í™”ëœ AST ë…¸ë“œë¥¼ í‘œí˜„í•©ë‹ˆë‹¤.

    - ìˆ˜ì§‘ ë‹¨ê³„ì—ì„œ ëª¨ë“  ë…¸ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    - ì´í›„ ë°°ì¹˜ê°€ ë§Œë“¤ì–´ì§ˆ ë•Œ ì´ ê°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    - LLM ìš”ì•½ì´ ëë‚˜ë©´ `summary`ì™€ `completion_event`ê°€ ì±„ì›Œì§‘ë‹ˆë‹¤.
    """
    node_id: int
    start_line: int
    end_line: int
    node_type: str
    code: str
    token: int
    has_children: bool
    procedure_key: Optional[str]
    procedure_type: Optional[str]
    procedure_name: Optional[str]
    schema_name: Optional[str]
    analyzable: bool
    dml: bool
    lines: List[Tuple[int, str]] = field(default_factory=list)
    parent: Optional[StatementNode] = None
    children: List[StatementNode] = field(default_factory=list)
    summary: Optional[str] = None
    completion_event: asyncio.Event = field(init=False, repr=False)

    def __post_init__(self):
        object.__setattr__(self, "completion_event", asyncio.Event())

    def get_raw_code(self) -> str:
        """ë¼ì¸ ë²ˆí˜¸ë¥¼ í¬í•¨í•˜ì—¬ ë…¸ë“œì˜ ì›ë¬¸ ì½”ë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return '\n'.join(f"{line_no}: {text}" for line_no, text in self.lines)

    def get_compact_code(self) -> str:
        """ìì‹ ìš”ì•½ì„ í¬í•¨í•œ ë¶€ëª¨ ì½”ë“œ(LLM ì…ë ¥ìš©)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.children:
            return self.code

        result_lines: List[str] = []
        line_index = 0
        total_lines = len(self.lines)
        sorted_children = sorted(self.children, key=lambda child: child.start_line)

        for child in sorted_children:
            # ìì‹ ì´ì „ì˜ ë¶€ëª¨ ê³ ìœ  ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬í•©ë‹ˆë‹¤.
            while line_index < total_lines and self.lines[line_index][0] < child.start_line:
                line_no, text = self.lines[line_index]
                result_lines.append(f"{line_no}: {text}")
                line_index += 1

            # ìì‹ êµ¬ê°„ì€ ìì‹ ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤ (ì—†ìœ¼ë©´ ê¸°ë³¸ placeholder).
            if child.summary:
                child_summary = child.summary.strip()
                summary_line = f"{child.start_line}~{child.end_line}: {child_summary}"
            else:
                log_process("UNDERSTAND", "COLLECT", f"âš ï¸ ë¶€ëª¨ {self.start_line}~{self.end_line}ì˜ ìì‹ {child.start_line}~{child.end_line} ìš”ì•½ ì—†ìŒ - ì›ë¬¸ ë³´ê´€")
                summary_line = '\n'.join(
                    f"{line_no}: {text}"
                    for line_no, text in child.lines
                ).strip()

            result_lines.append(summary_line)

            # ìì‹ êµ¬ê°„ ì›ë³¸ ì½”ë“œëŠ” ê±´ë„ˆëœë‹ˆë‹¤.
            while line_index < total_lines and self.lines[line_index][0] <= child.end_line:
                line_index += 1

        # ë§ˆì§€ë§‰ ìì‹ ì´í›„ ë¶€ëª¨ ì½”ë“œê°€ ë‚¨ì•„ ìˆë‹¤ë©´ ì¶”ê°€í•©ë‹ˆë‹¤.
        while line_index < total_lines:
            line_no, text = self.lines[line_index]
            result_lines.append(f"{line_no}: {text}")
            line_index += 1

        return '\n'.join(result_lines)

    def get_placeholder_code(self) -> str:
        """ìì‹ êµ¬ê°„ì„ placeholderë¡œ ìœ ì§€í•œ ì½”ë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if not self.children:
            return self.code

        result_lines: List[str] = []
        line_index = 0
        total_lines = len(self.lines)
        sorted_children = sorted(self.children, key=lambda child: child.start_line)

        for child in sorted_children:
            while line_index < total_lines and self.lines[line_index][0] < child.start_line:
                line_no, text = self.lines[line_index]
                result_lines.append(f"{line_no}: {text}")
                line_index += 1

            result_lines.append(f"{child.start_line}: ...code...")

            while line_index < total_lines and self.lines[line_index][0] <= child.end_line:
                line_index += 1

        while line_index < total_lines:
            line_no, text = self.lines[line_index]
            result_lines.append(f"{line_no}: {text}")
            line_index += 1

        return '\n'.join(result_lines)


@dataclass(slots=True)
class ProcedureInfo:
    key: str
    procedure_type: str
    procedure_name: str
    schema_name: Optional[str]
    start_line: int
    end_line: int
    pending_nodes: int = 0


@dataclass(slots=True)
class AnalysisBatch:
    batch_id: int
    nodes: List[StatementNode]
    ranges: List[Dict[str, int]]
    dml_ranges: List[Dict[str, int]]
    progress_line: int

    def build_general_payload(self) -> str:
        """ì¼ë°˜ LLM í˜¸ì¶œìš©ìœ¼ë¡œ ë…¸ë“œë“¤ì˜ compact ì½”ë“œë¥¼ ê²°í•©í•©ë‹ˆë‹¤."""
        return '\n\n'.join(node.get_compact_code() for node in self.nodes)

    def build_dml_payload(self) -> Optional[str]:
        """DML ë…¸ë“œë§Œ ì¶”ë¦° ì›ë¬¸ ì½”ë“œë¥¼ ê²°í•©í•˜ì—¬ í…Œì´ë¸” ë¶„ì„ í”„ë¡¬í”„íŠ¸ì— ì „ë‹¬í•©ë‹ˆë‹¤."""
        dml_nodes = [node for node in self.nodes if node.dml]
        if not dml_nodes:
            return None
        return '\n\n'.join(
            node.get_compact_code() if node.has_children else node.get_raw_code()
            for node in dml_nodes
        )


@dataclass(slots=True)
class BatchResult:
    """LLM í˜¸ì¶œ ê²°ê³¼ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë³´ê´€í•˜ëŠ” ë‹¨ìˆœ ì»¨í…Œì´ë„ˆ."""
    batch: AnalysisBatch
    general_result: Optional[Dict[str, Any]]
    table_result: Optional[Dict[str, Any]]


# ==================== í—¬í¼ í•¨ìˆ˜ ====================
def get_procedure_name_from_code(code: str) -> Tuple[Optional[str], Optional[str]]:
    """ì½”ë“œ ë¬¸ìì—´ì—ì„œ ìŠ¤í‚¤ë§ˆ/í”„ë¡œì‹œì € ì´ë¦„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    pattern = re.compile(
        r"\b(?:CREATE\s+(?:OR\s+REPLACE\s+)?)?(?:PROCEDURE|FUNCTION|TRIGGER)\s+"
        r"((?:\"[^\"]+\"|[A-Za-z_][\w$#]*)"
        r"(?:\s*\.\s*(?:\"[^\"]+\"|[A-Za-z_][\w$#]*)){0,2})",
        re.IGNORECASE,
    )
    prefix_pattern = re.compile(r"^\d+\s*:\s*")
    normalized = prefix_pattern.sub("", code)
    match = pattern.search(normalized)
    if not match:
        return None, None
    parts = [segment.strip().strip('"') for segment in re.split(r"\s*\.\s*", match.group(1))]
    if len(parts) == 3:
        return parts[0], f"{parts[1]}.{parts[2]}"
    if len(parts) == 2:
        return parts[0], parts[1]
    if parts:
        return None, parts[0]
    return None, None


def get_original_node_code(file_content: str, start_line: int, end_line: int) -> str:
    """íŒŒì¼ ì „ì²´ ë¬¸ìì—´ì—ì„œ íŠ¹ì • êµ¬ê°„ì„ ë¼ì¸ ë²ˆí˜¸ì™€ í•¨ê»˜ ì˜ë¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    lines = file_content.split('\n')[start_line - 1:end_line]
    result: List[str] = []
    for index, line in enumerate(lines, start=start_line):
        if LINE_NUMBER_PATTERN.match(line):
            result.append(line)
        else:
            result.append(f"{index}: {line}")
    return '\n'.join(result)


def build_statement_name(node_type: str, start_line: int) -> str:
    """ë…¸ë“œ íƒ€ì…ê³¼ ì‹œì‘ ë¼ì¸ì„ ì¡°í•©í•œ ì‹ë³„ì ë¬¸ìì—´ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    return f"{node_type}[{start_line}]"


def escape_summary(summary: str) -> str:
    """LLM ìš”ì•½ ë¬¸ìì—´ì„ JSON-safe í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    return json.dumps(summary)


# ==================== ë…¸ë“œ ìˆ˜ì§‘ê¸° ====================
class StatementCollector:
    """ASTë¥¼ í›„ìœ„ìˆœíšŒí•˜ì—¬ `StatementNode`ì™€ í”„ë¡œì‹œì € ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    def __init__(
        self,
        antlr_data: Dict[str, Any],
        file_content: str,
        folder_name: str,
        file_name: str,
        statement_kinds: Dict[str, Any],
    ):
        """ìˆ˜ì§‘ê¸°ì— í•„ìš”í•œ AST ë°ì´í„°, íŒŒì¼ ë©”íƒ€, êµ¬ë¬¸ ì •ì˜ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.antlr_data = antlr_data
        self.file_content = file_content
        self.folder_name = folder_name
        self.file_name = file_name
        self.procedure_types = statement_kinds["procedure_types"]
        self.non_analysis_types = statement_kinds["non_analysis_types"]
        self.non_next_recursive_types = statement_kinds["non_next_recursive_types"]
        self.dml_statement_types = statement_kinds["dml_statement_types"]
        self.variable_role_map = statement_kinds["variable_role_map"]
        self.variable_declaration_types = statement_kinds["variable_declaration_types"]
        self.nodes: List[StatementNode] = []
        self.procedures: Dict[str, ProcedureInfo] = {}
        self._node_id = 0
        self._file_lines = file_content.split('\n')

    def collect(self) -> Tuple[List[StatementNode], Dict[str, ProcedureInfo]]:
        """AST ì „ì—­ì„ í›„ìœ„ ìˆœíšŒí•˜ì—¬ ë…¸ë“œ ëª©ë¡ê³¼ í”„ë¡œì‹œì € ì •ë³´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        # ë£¨íŠ¸ ë…¸ë“œë¶€í„° í›„ìœ„ìˆœíšŒí•©ë‹ˆë‹¤ (ìì‹ â†’ ë¶€ëª¨ ìˆœì„œ ë³´ì¥)
        self._visit(self.antlr_data, current_proc=None, current_type=None, current_schema=None)
        return self.nodes, self.procedures

    def _make_proc_key(self, procedure_name: Optional[str], start_line: int) -> str:
        """í”„ë¡œì‹œì € ê³ ìœ í‚¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        base = procedure_name or f"anonymous_{start_line}"
        return f"{self.folder_name}:{self.file_name}:{base}:{start_line}"

    def _visit(
        self,
        node: Dict[str, Any],
        current_proc: Optional[str],
        current_type: Optional[str],
        current_schema: Optional[str],
    ) -> Optional[StatementNode]:
        """ì¬ê·€ì ìœ¼ë¡œ ASTë¥¼ ë‚´ë ¤ê°€ë©° StatementNodeë¥¼ ìƒì„±í•˜ê³  ë¶€ëª¨-ìì‹ ê´€ê³„ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤."""
        # ê° ë…¸ë“œì˜ ê¸°ë³¸ ë©”íƒ€ë°ì´í„°ë¥¼ í™•ë³´í•©ë‹ˆë‹¤.
        start_line = node['startLine']
        end_line = node['endLine']
        node_type = node['type']
        children = node.get('children', []) or []

        child_nodes: List[StatementNode] = []
        procedure_key = current_proc
        procedure_type = current_type
        schema_name = current_schema

        # LLM ì…ë ¥ ë° ìš”ì•½ ìƒì„±ì— í™œìš©í•  ì›ë³¸ ì½”ë“œë¥¼ ë¼ì¸ ë‹¨ìœ„ë¡œ ì¤€ë¹„í•©ë‹ˆë‹¤.
        line_entries = [
            (line_no, self._file_lines[line_no - 1] if 0 <= line_no - 1 < len(self._file_lines) else '')
            for line_no in range(start_line, end_line + 1)
        ]
        code = '\n'.join(f"{line_no}: {text}" for line_no, text in line_entries)

        if node_type in self.procedure_types:
            # í”„ë¡œì‹œì €/í•¨ìˆ˜ ë£¨íŠ¸ë¼ë©´ ì´ë¦„/ìŠ¤í‚¤ë§ˆë¥¼ ì¶”ì¶œí•˜ì—¬ ë³„ë„ ë²„í‚·ì„ ë§Œë“­ë‹ˆë‹¤.
            # ìƒì„±ëœ procedure_keyëŠ” í•˜ìœ„ ë…¸ë“œì™€ ìš”ì•½ ê²°ê³¼ë¥¼ ë¬¶ëŠ” ê¸°ì¤€ í‚¤ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
            schema_candidate, name_candidate = get_procedure_name_from_code(code)
            procedure_key = self._make_proc_key(name_candidate, start_line)
            procedure_type = node_type
            schema_name = schema_candidate
            if procedure_key not in self.procedures:
                self.procedures[procedure_key] = ProcedureInfo(
                    key=procedure_key,
                    procedure_type=node_type,
                    procedure_name=name_candidate or procedure_key,
                    schema_name=schema_candidate,
                    start_line=start_line,
                    end_line=end_line,
                )
                proc_name_log = name_candidate or procedure_key
                log_process("UNDERSTAND", "COLLECT", f"ğŸ“‹ í”„ë¡œì‹œì € ì„ ì–¸ ë°œê²¬: {proc_name_log} (ë¼ì¸ {start_line}~{end_line})")

        for child in children:
            child_node = self._visit(child, procedure_key, procedure_type, schema_name)
            if child_node is not None:
                child_nodes.append(child_node)

        # í›„ì† ë‹¨ê³„ì—ì„œ í™œìš©í•  ë¶„ì„ ê°€ëŠ¥ ì—¬ë¶€ ë° í† í° ì •ë³´ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        analyzable = node_type not in self.non_analysis_types
        token = calculate_code_token(code)
        dml = node_type in self.dml_statement_types
        has_children = bool(child_nodes)

        self._node_id += 1
        statement_node = StatementNode(
            node_id=self._node_id,
            start_line=start_line,
            end_line=end_line,
            node_type=node_type,
            code=code,
            token=token,
            has_children=has_children,
            procedure_key=procedure_key,
            procedure_type=procedure_type,
            procedure_name=self.procedures.get(procedure_key).procedure_name if procedure_key in self.procedures else None,
            schema_name=schema_name,
            analyzable=analyzable,
            dml=dml,
            lines=line_entries,
        )
        for child_node in child_nodes:
            child_node.parent = statement_node
        statement_node.children.extend(child_nodes)

        # í”„ë¡œì‹œì € ìš”ì•½ ì™„ë£Œ ì‹œì ì„ íŒë³„í•˜ê¸° ìœ„í•´ pending ë…¸ë“œ ìˆ˜ë¥¼ ì¶”ì í•©ë‹ˆë‹¤.
        if analyzable and procedure_key and procedure_key in self.procedures:
            self.procedures[procedure_key].pending_nodes += 1
        else:
            statement_node.completion_event.set()

        self.nodes.append(statement_node)
        log_process("UNDERSTAND", "COLLECT", f"âœ… {node_type} ë…¸ë“œ ìˆ˜ì§‘ ì™„ë£Œ: ë¼ì¸ {start_line}~{end_line}, í† í° {token}, ìì‹ {len(child_nodes)}ê°œ")
        return statement_node


# ==================== ë°°ì¹˜ í”Œë˜ë„ˆ ====================
class BatchPlanner:
    """ìˆ˜ì§‘ëœ ë…¸ë“œë¥¼ í† í° í•œë„ ë‚´ì—ì„œ ë°°ì¹˜ë¡œ ë¬¶ìŠµë‹ˆë‹¤."""
    def __init__(self, token_limit: int = MAX_BATCH_TOKEN):
        """í† í° í•œë„ë¥¼ ì§€ì •í•˜ì—¬ ë°°ì¹˜ ìƒì„±ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.token_limit = token_limit

    def plan(self, nodes: List[StatementNode], folder_file: str) -> List[AnalysisBatch]:
        """í† í° í•œë„ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ë…¸ë“œë¥¼ ë¶„í• í•˜ì—¬ ë¶„ì„ ë°°ì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        batches: List[AnalysisBatch] = []
        current_nodes: List[StatementNode] = []
        current_tokens = 0
        batch_id = 1

        for node in nodes:
            if not node.analyzable:
                continue

            # ë¶€ëª¨ ë…¸ë“œëŠ” ìì‹ ìš”ì•½ì´ ì¤€ë¹„ëœ í›„ ë‹¨ë…ìœ¼ë¡œ ì‹¤í–‰ë˜ë¯€ë¡œ ì¦‰ì‹œ ë°°ì¹˜ë¥¼ í™•ì •í•©ë‹ˆë‹¤.
            if node.has_children:
                # ë¶€ëª¨ ë…¸ë“œëŠ” ìì‹ ìš”ì•½ì´ ëª¨ë‘ ì¤€ë¹„ëœ ìƒíƒœì—ì„œ ë‹¨ë…ìœ¼ë¡œ LLMì— ì „ë‹¬í•©ë‹ˆë‹¤.
                if current_nodes:
                    # í˜„ì¬ê¹Œì§€ ëˆ„ì ëœ ë¦¬í”„ ë°°ì¹˜ë¥¼ ë¨¼ì € í™•ì •í•©ë‹ˆë‹¤.
                    log_process("UNDERSTAND", "BATCH", f"ğŸ“¦ [leaf] ë°°ì¹˜ #{batch_id} í™•ì •: ë¦¬í”„ ë…¸ë“œ {len(current_nodes)}ê°œ (í† í° {current_tokens}/{self.token_limit})")
                    batches.append(self._create_batch(batch_id, current_nodes))
                    batch_id += 1
                    current_nodes = []
                    current_tokens = 0

                log_process("UNDERSTAND", "BATCH", f"ğŸ“¦ [parent] ë°°ì¹˜ #{batch_id} í™•ì •: ë¶€ëª¨ ë…¸ë“œ ë‹¨ë… ì‹¤í–‰ (ë¼ì¸ {node.start_line}~{node.end_line}, í† í° {node.token})")
                batches.append(self._create_batch(batch_id, [node]))
                batch_id += 1
                continue

            # í˜„ì¬ ë°°ì¹˜ê°€ í† í° í•œë„ë¥¼ ì´ˆê³¼í•œë‹¤ë©´ ìŒ“ì¸ ë¦¬í”„ ë…¸ë“œë“¤ì„ ë¨¼ì € ì‹¤í–‰í•©ë‹ˆë‹¤.
            if current_nodes and current_tokens + node.token > self.token_limit:
                # í† í° í•œë„ë¥¼ ì´ˆê³¼í•˜ê¸° ì§ì „ ë°°ì¹˜ë¥¼ í™•ì •í•©ë‹ˆë‹¤.
                log_process("UNDERSTAND", "BATCH", f"ğŸ“¦ [leaf] ë°°ì¹˜ #{batch_id} í™•ì •: í† í° í•œë„ ë„ë‹¬ë¡œ ì„  ì‹¤í–‰ (ëˆ„ì  {current_tokens}/{self.token_limit})")
                batches.append(self._create_batch(batch_id, current_nodes))
                batch_id += 1
                current_nodes = []
                current_tokens = 0

            current_nodes.append(node)
            current_tokens += node.token

        if current_nodes:
            # ë‚¨ì•„ ìˆëŠ” ë…¸ë“œê°€ ìˆìœ¼ë©´ ë§ˆë¬´ë¦¬ ë°°ì¹˜ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.
            log_process("UNDERSTAND", "BATCH", f"ğŸ“¦ [leaf] ë°°ì¹˜ #{batch_id} í™•ì •: ë§ˆì§€ë§‰ ë¦¬í”„ ë…¸ë“œ {len(current_nodes)}ê°œ (í† í° {current_tokens}/{self.token_limit})")
            batches.append(self._create_batch(batch_id, current_nodes))

        return batches

    def _create_batch(self, batch_id: int, nodes: List[StatementNode]) -> AnalysisBatch:
        """ë°°ì¹˜ IDì™€ ë…¸ë“œ ë¦¬ìŠ¤íŠ¸ë¡œ AnalysisBatch ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        # LLM í˜¸ì¶œê³¼ ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•´ ë²”ìœ„ ì •ë³´ë¥¼ ë¯¸ë¦¬ ê³„ì‚°í•©ë‹ˆë‹¤.
        ranges = [{"startLine": node.start_line, "endLine": node.end_line} for node in nodes]
        dml_ranges = [
            {"startLine": node.start_line, "endLine": node.end_line, "type": node.node_type}
            for node in nodes
            if node.dml
        ]
        # ì§„í–‰ë¥  í‘œì‹œëŠ” ë°°ì¹˜ ë‚´ ê°€ì¥ ë§ˆì§€ë§‰ ë¼ì¸ ê¸°ì¤€ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        progress_line = max(node.end_line for node in nodes)
        return AnalysisBatch(
            batch_id=batch_id,
            nodes=nodes,
            ranges=ranges,
            dml_ranges=dml_ranges,
            progress_line=progress_line,
        )


# ==================== Analyzer ë³¸ì²´ ====================
class Analyzer:
    """Understanding íŒŒì´í”„ë¼ì¸ì˜ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸.

    1. ASTë¥¼ í‰íƒ„í™”(`StatementCollector`).
    2. í† í° ê¸°ì¤€ìœ¼ë¡œ ë°°ì¹˜ë¥¼ ìƒì„±(`BatchPlanner`).
    3. LLM ì›Œì»¤ë¥¼ í†µí•´ ë³‘ë ¬ ë¶„ì„(`LLMInvoker`).
    4. ê²°ê³¼ë¥¼ ìˆœì°¨ ì ìš©í•˜ê³  ìš”ì•½(`ApplyManager`).
    """
    def __init__(
        self,
        antlr_data: dict,
        file_content: str,
        send_queue: asyncio.Queue,
        receive_queue: asyncio.Queue,
        last_line: int,
        folder_name: str,
        file_name: str,
        user_id: str,
        api_key: str,
        locale: str,
        dbms: str,
        project_name: str,
        strategy: Optional[UnderstandingStrategy] = None,
    ):
        """Analyzerê°€ íŒŒì¼ ë¶„ì„ì— í•„ìš”í•œ ëª¨ë“  ì»¨í…ìŠ¤íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.antlr_data = antlr_data
        self.file_content = file_content
        self.send_queue = send_queue
        self.receive_queue = receive_queue
        self.last_line = last_line
        self.folder_name = folder_name
        self.file_name = file_name
        self.user_id = user_id
        self.api_key = api_key
        self.locale = locale
        self.dbms = (dbms or 'postgres').lower()
        self.project_name = project_name or ''
        self.strategy = strategy or DbmsUnderstandingStrategy()

        # ì „ëµìœ¼ë¡œë¶€í„° êµ¬ë¬¸ ì •ì˜ë¥¼ ë°›ì•„ ê³µí†µ êµ¬ì„±ì— ì €ì¥
        kinds = self.strategy.statement_rules()
        self.procedure_types = kinds["procedure_types"]
        self.non_analysis_types = kinds["non_analysis_types"]
        self.non_next_recursive_types = kinds["non_next_recursive_types"]
        self.dml_statement_types = kinds["dml_statement_types"]
        self.variable_role_map = kinds["variable_role_map"]
        self.variable_declaration_types = kinds["variable_declaration_types"]
        self.variable_concurrency = VARIABLE_CONCURRENCY

        self.folder_file = f"{folder_name}-{file_name}"
        self.node_base_props = (
            f"folder_name: '{folder_name}', file_name: '{file_name}', user_id: '{user_id}', project_name: '{self.project_name}'"
        )
        self.folder_props = (
            f"user_id: '{user_id}', name: '{folder_name}', project_name: '{self.project_name}'"
        )
        self.table_base_props = f"user_id: '{user_id}'"
        self.max_workers = MAX_CONCURRENCY

    async def _initialize_static_graph(self, nodes: List[StatementNode]):
        """íŒŒì¼ ë¶„ì„ ì „ì— ì •ì  ë…¸ë“œ/ê´€ê³„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if not nodes:
            return
        # 1) ë…¸ë“œ ë³¸ë¬¸ì„ Neo4jì— ë¯¸ë¦¬ ìƒì„±í•˜ê³ 
        await self._create_static_nodes(nodes)
        # 2) ë¶€ëª¨/í˜•ì œ ê´€ê³„ë¥¼ ì„ ë°˜ì˜í•˜ë©°
        await self._create_relationships(nodes)
        # 3) ë³€ìˆ˜ ì„ ì–¸ì€ ë³„ë„ í”„ë¡¬í”„íŠ¸ë¡œ ë³‘ë ¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        await self.strategy.process_variables(self, nodes)

    async def _create_static_nodes(self, nodes: List[StatementNode]):
        """ê° StatementNodeì— ëŒ€ì‘í•˜ëŠ” ê¸°ë³¸ ë…¸ë“œë¥¼ Neo4jì— ìƒì„±í•©ë‹ˆë‹¤."""
        queries: List[str] = []
        for node in nodes:
            # StatementNode ë‹¨ìœ„ë¡œ MERGE ì¿¼ë¦¬ ë¬¶ìŒì„ ìƒì„±í•©ë‹ˆë‹¤.
            queries.extend(self._build_static_node_queries(node))
            if len(queries) >= STATIC_QUERY_BATCH_SIZE:
                # ì¼ì •ëŸ‰ì´ ìŒ“ì´ë©´ ì¦‰ì‹œ ì „ì†¡í•˜ì—¬ íë¥¼ ë¹„ì›ë‹ˆë‹¤.
                await self._send_static_queries(queries, node.end_line)
                queries.clear()
        if queries:
            # ë§ˆì§€ë§‰ ë‚¨ì€ ì¿¼ë¦¬ ë¬¶ìŒë„ ì „ì†¡í•©ë‹ˆë‹¤.
            await self._send_static_queries(queries, nodes[-1].end_line)

    def _build_static_node_queries(self, node: StatementNode) -> List[str]:
        """ì •ì  ë…¸ë“œ ìƒì„±ì„ ìœ„í•œ Cypher ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        queries: List[str] = []
        label = node.node_type
        node_name = self.file_name if label == "FILE" else build_statement_name(label, node.start_line)
        escaped_name = escape_for_cypher(node_name)
        has_children = 'true' if node.has_children else 'false'
        procedure_name = escape_for_cypher(node.procedure_name or '')

        if not node.children and label not in self.non_analysis_types:
            # ë¦¬í”„ ë…¸ë“œì´ë©´ì„œ ë¶„ì„ ëŒ€ìƒì´ë©´ ìš”ì•½ ì „ node_codeë¥¼ í¬í•¨í•´ ì €ì¥í•©ë‹ˆë‹¤.
            escaped_code = escape_for_cypher(node.code)
            queries.append(
                f"MERGE (n:{label} {{startLine: {node.start_line}, {self.node_base_props}}})\n"
                f"SET n.endLine = {node.end_line}, n.name = '{escaped_name}', n.node_code = '{escaped_code}',\n"
                f"    n.token = {node.token}, n.procedure_name = '{procedure_name}', n.has_children = {has_children}\n"
                f"WITH n\n"
                f"MERGE (folder:SYSTEM {{{self.folder_props}}})\n"
                f"MERGE (folder)-[:CONTAINS]->(n)"
            )
            return queries

        escaped_code = escape_for_cypher(node.code)

        if label == "FILE":
            file_summary = 'File Start Node' if self.locale == 'en' else 'íŒŒì¼ ë…¸ë“œ'
            queries.append(
                f"MERGE (n:{label} {{startLine: {node.start_line}, {self.node_base_props}}})\n"
                f"SET n.endLine = {node.end_line}, n.name = '{self.file_name}', n.summary = '{escape_for_cypher(file_summary)}',\n"
                f"    n.has_children = {has_children}\n"
                f"WITH n\n"
                f"MERGE (folder:SYSTEM {{{self.folder_props}}})\n"
                f"MERGE (folder)-[:CONTAINS]->(n)"
            )
        else:
            placeholder_fragment = ""
            if node.has_children:
                # ë¶€ëª¨ ë…¸ë“œëŠ” summarized_codeë¥¼ ë¯¸ë¦¬ ê¸°ë¡í•´ ë‘¡ë‹ˆë‹¤.
                escaped_placeholder = escape_for_cypher(node.get_placeholder_code())
                placeholder_fragment = f", n.summarized_code = '{escaped_placeholder}'"
            queries.append(
                f"MERGE (n:{label} {{startLine: {node.start_line}, {self.node_base_props}}})\n"
                f"SET n.endLine = {node.end_line}, n.name = '{escaped_name}'{placeholder_fragment},\n"
                f"    n.node_code = '{escaped_code}', n.token = {node.token}, n.procedure_name = '{procedure_name}', n.has_children = {has_children}\n"
                f"WITH n\n"
                f"MERGE (folder:SYSTEM {{{self.folder_props}}})\n"
                f"MERGE (folder)-[:CONTAINS]->(n)"
            )
        return queries

    async def _create_relationships(self, nodes: List[StatementNode]):
        """PARENT_OF / NEXT ê´€ê³„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        queries: List[str] = []
        for node in nodes:
            for child in node.children:
                # ë¶€ëª¨-ìì‹ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•œ ê´€ê³„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
                queries.append(self._build_parent_relationship_query(node, child))
                if len(queries) >= STATIC_QUERY_BATCH_SIZE:
                    await self._send_static_queries(queries, child.end_line)
                    queries.clear()

            prev_node: Optional[StatementNode] = None
            for child in node.children:
                if prev_node and prev_node.node_type not in self.non_next_recursive_types:
                    # ë™ì¼ ë¶€ëª¨ ì•„ë˜ í˜•ì œ ë…¸ë“œ ê°„ ìˆœì„œë¥¼ NEXT ê´€ê³„ë¡œ ê¸°ë¡í•©ë‹ˆë‹¤.
                    queries.append(self._build_next_relationship_query(prev_node, child))
                    if len(queries) >= STATIC_QUERY_BATCH_SIZE:
                        await self._send_static_queries(queries, child.end_line)
                        queries.clear()
                prev_node = child

        if queries:
            await self._send_static_queries(queries, nodes[-1].end_line)

    def _build_parent_relationship_query(self, parent: StatementNode, child: StatementNode) -> str:
        """ë¶€ëª¨ì™€ ìì‹ ë…¸ë“œ ì‚¬ì´ì˜ PARENT_OF ê´€ê³„ ì¿¼ë¦¬ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤."""
        parent_match = f"MATCH (parent:{parent.node_type} {{startLine: {parent.start_line}, {self.node_base_props}}})"
        child_match = f"MATCH (child:{child.node_type} {{startLine: {child.start_line}, {self.node_base_props}}})"
        return f"{parent_match}\n{child_match}\nMERGE (parent)-[:PARENT_OF]->(child)"

    def _build_next_relationship_query(self, prev_node: StatementNode, current_node: StatementNode) -> str:
        """í˜•ì œ ë…¸ë“œ ì‚¬ì´ì˜ NEXT ê´€ê³„ ì¿¼ë¦¬ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤."""
        prev_match = f"MATCH (prev:{prev_node.node_type} {{startLine: {prev_node.start_line}, {self.node_base_props}}})"
        curr_match = f"MATCH (current:{current_node.node_type} {{startLine: {current_node.start_line}, {self.node_base_props}}})"
        return f"{prev_match}\n{curr_match}\nMERGE (prev)-[:NEXT]->(current)"

    async def _send_static_queries(self, queries: List[str], progress_line: int):
        """ì •ì  ê·¸ë˜í”„ ì´ˆê¸°í™” ì¿¼ë¦¬ë¥¼ íë¡œ ì „ì†¡í•˜ê³  ì™„ë£Œ ì‹œê¹Œì§€ ê¸°ë‹¤ë¦½ë‹ˆë‹¤."""
        if not queries:
            return
        await self.send_queue.put({
            "type": "analysis_code",
            "query_data": queries,
            "line_number": progress_line,
        })
        while True:
            response = await self.receive_queue.get()
            if response.get('type') == 'process_completed':
                break

    async def run(self):
        """íŒŒì¼ ë‹¨ìœ„ Understanding íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        log_process("UNDERSTAND", "START", f"ğŸš€ {self.folder_file} ë¶„ì„ ì‹œì‘ (ì´ {self.last_line}ì¤„)")
        try:
            collector = StatementCollector(
                self.antlr_data,
                self.file_content,
                self.folder_name,
                self.file_name,
                {
                    "procedure_types": self.procedure_types,
                    "non_analysis_types": self.non_analysis_types,
                    "non_next_recursive_types": self.non_next_recursive_types,
                    "dml_statement_types": self.dml_statement_types,
                    "variable_role_map": self.variable_role_map,
                    "variable_declaration_types": self.variable_declaration_types,
                },
            )
            # 1) ASTë¥¼ í‰íƒ„í™”í•˜ì—¬ StatementNode ëª©ë¡ì„ ì–»ìŠµë‹ˆë‹¤.
            nodes, procedures = collector.collect()
            # 2) ë¶„ì„ ì „ Neo4jì— ì •ì  êµ¬ì¡°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
            await self._initialize_static_graph(nodes)
            # 2-1) ì „ëµ ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ ì£¼ì… (Neo4j ì¿¼ë¦¬ ìƒì„±ì— í•„ìš”í•œ ë©”íƒ€ í¬í•¨)
            self.strategy.prepare_context(
                node_base_props=self.node_base_props,
                folder_props=self.folder_props,
                table_base_props=self.table_base_props,
                user_id=self.user_id,
                project_name=self.project_name,
                folder_name=self.folder_name,
                file_name=self.file_name,
                dbms=self.dbms,
                api_key=self.api_key,
                locale=self.locale,
                procedures=procedures,
                send_queue=self.send_queue,
                receive_queue=self.receive_queue,
                file_last_line=self.last_line,
            )

            planner = BatchPlanner()
            # 3) ë…¸ë“œë¥¼ í† í° ê¸°ì¤€ìœ¼ë¡œ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
            batches = planner.plan(nodes, self.folder_file)

            if not batches:
                # ë¶„ì„í•  ë…¸ë“œê°€ ì—†ë‹¤ë©´ ì¦‰ì‹œ ì¢…ë£Œ ì´ë²¤íŠ¸ë§Œ ì „ì†¡í•©ë‹ˆë‹¤.
                await self.send_queue.put({"type": "end_analysis"})
                return

            semaphore = asyncio.Semaphore(min(self.max_workers, len(batches)))

            async def worker(batch: AnalysisBatch):
                # ë¶€ëª¨ ë…¸ë“œê°€ í¬í•¨ëœ ë°°ì¹˜ë¼ë©´ ìì‹ ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
                await self._wait_for_dependencies(batch)
                batch_kind = "parent" if any(n.has_children for n in batch.nodes) else "leaf"
                async with semaphore:
                    log_process("UNDERSTAND", "LLM", f"ğŸ¤– [{batch_kind}] ë°°ì¹˜ #{batch.batch_id} LLM ìš”ì²­: ë…¸ë“œ {len(batch.nodes)}ê°œ ({self.folder_file})")
                    # LLM í˜¸ì¶œì€ ì¼ë°˜ ìš”ì•½ê³¼ í…Œì´ë¸” ìš”ì•½ì„ ë™ì‹œì— ìš”ì²­í•©ë‹ˆë‹¤.
                    general, table = await self.strategy.invoke_batch(batch)
                await self.strategy.apply_batch(batch, general, table)

            await asyncio.gather(*(worker(batch) for batch in batches))
            # ëª¨ë“  ë°°ì¹˜ ì œì¶œì´ ëë‚˜ë©´ ìš”ì•½/í…Œì´ë¸” ì„¤ëª… í›„ì²˜ë¦¬ë¥¼ ë§ˆë¬´ë¦¬í•©ë‹ˆë‹¤.
            await self.strategy.finalize()

            log_process("UNDERSTAND", "DONE", f"âœ… {self.folder_file} ë¶„ì„ ì™„ë£Œ")
            await self.send_queue.put({"type": "end_analysis"})

        except (UnderstandingError, LLMCallError) as exc:
            log_process("UNDERSTAND", "ERROR", "âŒ Understanding íŒŒì´í”„ë¼ì¸ì—ì„œ ì˜ˆì™¸ ë°œìƒ", logging.ERROR, exc)
            await self.send_queue.put({'type': 'error', 'message': str(exc)})
            raise
        except Exception as exc:
            err_msg = f"Understanding ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {exc}"
            log_process("UNDERSTAND", "ERROR", f"âŒ {err_msg}", logging.ERROR, exc)
            await self.send_queue.put({'type': 'error', 'message': err_msg})
            raise ProcessAnalyzeCodeError(err_msg)

    async def _wait_for_dependencies(self, batch: AnalysisBatch):
        """ë¶€ëª¨ ë°°ì¹˜ê°€ ì‹¤í–‰ë˜ê¸° ì „ì— ìì‹ ë…¸ë“œ ìš”ì•½ì´ ëª¨ë‘ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        # ë¶€ëª¨ ë…¸ë“œê°€ LLMì— ì „ë‹¬ë˜ê¸° ì „ ìì‹ ìš”ì•½ì´ ëª¨ë‘ ëë‚¬ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        waiters = []
        for node in batch.nodes:
            for child in node.children:
                if child.analyzable:
                    # ìì‹ ë…¸ë“œì˜ completion_eventë¥¼ ëª¨ì•„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ëŒ€ê¸°í•©ë‹ˆë‹¤.
                    waiters.append(child.completion_event.wait())
        if waiters:
            log_process("UNDERSTAND", "WAIT", f"â³ [parent] ë°°ì¹˜ #{batch.batch_id}: ìì‹ {len(waiters)}ê°œ ìš”ì•½ ì™„ë£Œ ëŒ€ê¸°")
            await asyncio.gather(*waiters)

