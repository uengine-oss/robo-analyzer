import asyncio
import logging
import os
import re
import textwrap
from collections import defaultdict, deque
from dataclasses import dataclass, field
from typing import Callable
from understand.neo4j_connection import Neo4jConnection
from util.exception import ConvertingError
from util.utility_tool import (
    build_rule_based_path, save_file, log_process
)
from util.rule_loader import RuleLoader
from convert.dbms.create_dbms_skeleton import start_dbms_skeleton


# ----- ÏÉÅÏàò Ï†ïÏùò -----
TOKEN_THRESHOLD = int(os.getenv('DBMS_TOKEN_THRESHOLD', '1000'))
CODE_PLACEHOLDER = "...code..."
DML_PLACEHOLDER_PATTERN = re.compile(
    r'(?P<indent>^[ \t]*)(?P<label>(?P<start>\d+)):\s*\.\.\.\s*code\s*\.\.\.',
    re.IGNORECASE | re.MULTILINE
)
DML_TYPES = frozenset(["SELECT", "INSERT", "UPDATE", "DELETE", "FETCH", "MERGE", "JOIN", "ALL_UNION", "UNION", "FOR"])
MAX_CONVERSION_CONCURRENCY = int(os.getenv('DBMS_MAX_CONCURRENCY', '5'))


@dataclass(slots=True)
class ChildFragment:
    sequence: int
    code: str
    start: int | None
    end: int | None


@dataclass(slots=True)
class ParentEntry:
    start: int
    end: int
    code: str
    is_dml: bool
    parent: "ParentEntry | None"
    sequence: int
    is_root: bool = False
    children: list[ChildFragment] = field(default_factory=list)
    pending_children: int = 0
    closed: bool = False
    finalized: bool = False


@dataclass(slots=True)
class ConversionWorkItem:
    work_id: int
    sequence: int
    code: str
    start: int
    end: int
    parent: ParentEntry | None
    parent_code: str
    token_count: int


class SpAccumulator:
    """Î¶¨ÌîÑ/ÏÜåÌòï ÎÖ∏Îìú ÏΩîÎìúÎ•º ÏûÑÍ≥ÑÍ∞íÍπåÏßÄ ÎàÑÏ†ÅÌïòÎäî Î≤ÑÌçº."""

    __slots__ = ('parts', 'token_total', 'start_line', 'end_line')

    def __init__(self) -> None:
        self.clear()

    def clear(self) -> None:
        self.parts: list[str] = []
        self.token_total = 0
        self.start_line: int | None = None
        self.end_line: int | None = None

    def append(self, code: str, token: int, start_line: int, end_line: int) -> bool:
        snippet = (code or '').strip()
        if not snippet:
            return False
        self.parts.append(snippet)
        self.token_total += token
        if self.start_line is None or start_line < self.start_line:
            self.start_line = start_line
        if self.end_line is None or end_line > self.end_line:
            self.end_line = end_line
        return True

    def has_data(self) -> bool:
        return bool(self.parts)

    def should_flush_with(self, incoming_token: int | None, token_limit: int) -> bool:
        if not self.parts or incoming_token is None:
            return False
        return (self.token_total + incoming_token) >= token_limit

    def part_count(self) -> int:
        return len(self.parts)

    def consume(self) -> tuple[str, int | None, int | None, int, int]:
        if not self.parts:
            return "", None, None, 0, 0
        code = '\n'.join(self.parts)
        start = self.start_line
        end = self.end_line
        tokens = self.token_total
        part_count = len(self.parts)
        self.clear()
        return code, start, end, tokens, part_count


class ConversionWorkQueue:
    """LLM Ìò∏Ï∂úÏùÑ Î≥ëÎ†¨Î°ú Ïã§ÌñâÌïòÍ∏∞ ÏúÑÌïú ÏûëÏóÖ ÌÅê."""

    __slots__ = ('rule_loader', 'api_key', 'locale', 'max_workers', 'items')

    def __init__(self, rule_loader: RuleLoader, api_key: str, locale: str, max_workers: int) -> None:
        self.rule_loader = rule_loader
        self.api_key = api_key
        self.locale = locale
        self.max_workers = max(1, max_workers)
        self.items: list[ConversionWorkItem] = []

    def reset(self) -> None:
        self.items.clear()

    def enqueue(self, item: ConversionWorkItem) -> None:
        self.items.append(item)

    def queued_count(self) -> int:
        return len(self.items)

    async def drain(self, completion_handler: Callable[[ConversionWorkItem, str], None]) -> None:
        if not self.items:
            return

        semaphore = asyncio.Semaphore(self.max_workers)

        async def worker(item: ConversionWorkItem) -> None:
            async with semaphore:
                result = await asyncio.to_thread(
                    self.rule_loader.execute,
                    role_name='dbms_conversion',
                    inputs={
                        'code': item.code,
                        'locale': self.locale,
                        'parent_code': item.parent_code
                    },
                    api_key=self.api_key
                )
            generated_code = (result.get('code') or '').strip()
            completion_handler(item, generated_code)

        await asyncio.gather(*(worker(item) for item in self.items))
        self.items.clear()


# ----- DBMS Î≥ÄÌôò ÌÅ¥ÎûòÏä§ -----
class DbmsConversionGenerator:
    """
    DBMS Î≥ÄÌôò Ï†ÑÏ≤¥ ÎùºÏù¥ÌîÑÏÇ¨Ïù¥ÌÅ¥ Í¥ÄÎ¶¨
    - Îã®Ïùº Ïª®ÌÖçÏä§Ìä∏ ÎàÑÏ†Å Î∞©ÏãùÏúºÎ°ú ÌÉÄÍ≤ü DBMS ÏΩîÎìú ÏÉùÏÑ±
    - ÎåÄÏö©Îüâ Î∂ÄÎ™®(ÌÜ†ÌÅ∞‚â•1000, ÏûêÏãù Î≥¥Ïú†) Ïä§ÏºàÎ†àÌÜ§ Í¥ÄÎ¶¨
    - ÌÜ†ÌÅ∞ ÏûÑÍ≥Ñ ÎèÑÎã¨ Ïãú LLM Î∂ÑÏÑù ÏàòÌñâ
    """
    __slots__ = (
        'traverse_nodes', 'system_name', 'file_name', 'procedure_name',
        'user_id', 'api_key', 'locale', 'project_name', 'target', 'skeleton_code',
        'merged_chunks', 'parent_stack',
        'rule_loader', 'sequence_counter',
        'work_id_counter', 'max_workers', 'root_entry',
        'sp_accumulator', 'work_queue'
    )

    def __init__(self, traverse_nodes: list, system_name: str, file_name: str,
                 procedure_name: str, user_id: str, api_key: str, locale: str, 
                 project_name: str = "demo", target: str = "oracle",
                 skeleton_code: str | None = None):
        self.traverse_nodes = traverse_nodes
        self.system_name = system_name
        self.file_name = file_name
        self.procedure_name = procedure_name
        self.user_id = user_id
        self.api_key = api_key
        self.locale = locale
        self.project_name = project_name or "demo"
        self.target = target
        self.skeleton_code = (skeleton_code or "").strip()

        # ÏÉÅÌÉú Ï¥àÍ∏∞Ìôî
        self.merged_chunks = []
        self.parent_stack = []
        
        # Rule ÌååÏùº Î°úÎçî (targetÏúºÎ°ú ÎîîÎ†âÌÜ†Î¶¨ Ï∞æÏùå)
        self.rule_loader = RuleLoader(target_lang=target)
        self.sequence_counter = 0
        self.work_id_counter = 0
        self.max_workers = MAX_CONVERSION_CONCURRENCY
        self.root_entry: ParentEntry | None = None
        self.sp_accumulator = SpAccumulator()
        self.work_queue = ConversionWorkQueue(self.rule_loader, self.api_key, self.locale, self.max_workers)

    # ----- Í≥µÍ∞ú Î©îÏÑúÎìú -----

    @staticmethod
    def _resolve_node_type(node_labels: list | None, node: dict) -> str:
        raw_type = node_labels[0] if node_labels else node.get('name', 'UNKNOWN')
        raw_type = str(raw_type)
        return raw_type.split('[')[0] if '[' in raw_type else raw_type

    @staticmethod
    def _safe_int(value) -> int:
        try:
            return int(value)
        except (TypeError, ValueError):
            return 0

    def _ensure_root_entry(self) -> None:
        if self.root_entry is not None:
            return
        self.root_entry = ParentEntry(
            start=0,
            end=float('inf'),
            code="",
            is_dml=False,
            parent=None,
            sequence=0,
            is_root=True
        )

    def _reset_state(self) -> None:
        self._ensure_root_entry()
        if self.root_entry:
            self.root_entry.children.clear()
            self.root_entry.pending_children = 0
            self.root_entry.closed = False
            self.root_entry.finalized = False
        self.parent_stack.clear()
        self.sp_accumulator.clear()
        self.merged_chunks = []
        self.sequence_counter = 0
        self.work_id_counter = 0
        self.work_queue.reset()

    def _next_sequence(self) -> int:
        self.sequence_counter += 1
        return self.sequence_counter

    def _next_work_id(self) -> int:
        self.work_id_counter += 1
        return self.work_id_counter

    @staticmethod
    def _register_pending_child(parent_entry: ParentEntry | None) -> None:
        if parent_entry is None:
            return
        parent_entry.pending_children += 1

    def _record_child_fragment(
        self,
        parent_entry: ParentEntry | None,
        code: str,
        start_line: int | None,
        end_line: int | None,
        sequence: int
    ) -> None:
        if parent_entry is None:
            if code and code.strip():
                self.merged_chunks.append(code)
            else:
                log_process("DBMS", "CONVERT", f"‚ö†Ô∏è Î£®Ìä∏ Íµ¨Í∞Ñ {start_line}~{end_line}Ïóê Îπà Î≥ÄÌôò Í≤∞Í≥º Î∞òÌôò - ÏµúÏ¢Ö ÏΩîÎìúÏóêÏÑú Ï†úÏô∏", logging.WARNING)
            return

        if code and code.strip():
            fragment = ChildFragment(
                sequence=sequence,
                code=code,
                start=start_line,
                end=end_line
            )
            parent_entry.children.append(fragment)
        else:
            log_process("DBMS", "CONVERT", f"‚ö†Ô∏è ÏûêÏãù Íµ¨Í∞Ñ {start_line}~{end_line}Ïóê Îπà Î≥ÄÌôò Í≤∞Í≥º Î∞òÌôò - Î∂ÄÎ™® {parent_entry.start}~{parent_entry.end}Ïóê Ï†ÅÏö©Ìï† ÏΩîÎìú ÏóÜÏùå", logging.WARNING)

        if parent_entry.pending_children > 0:
            parent_entry.pending_children -= 1
        else:
            log_process("DBMS", "CONVERT", f"‚ö†Ô∏è Î∂ÄÎ™® {parent_entry.start}~{parent_entry.end}Ïùò ÎØ∏Ï≤òÎ¶¨ ÏûêÏãù ÏàòÍ∞Ä ÏùåÏàò - Neo4j Îç∞Ïù¥ÌÑ∞ ÌôïÏù∏ ÌïÑÏöî", logging.WARNING)

        self._try_finalize_parent(parent_entry)

    def _try_finalize_parent(self, entry: ParentEntry | None) -> None:
        if entry is None or entry.finalized or not entry.closed or entry.pending_children > 0:
            return

        if entry.is_root:
            ordered = [
                fragment.code
                for fragment in sorted(entry.children, key=lambda frag: frag.sequence)
                if fragment.code and fragment.code.strip()
            ]
            self.merged_chunks = ordered
            entry.children.clear()
            entry.finalized = True
            log_process("DBMS", "CONVERT", f"üéâ Î™®Îì† Î≥ÄÌôò ÏôÑÎ£å: Î£®Ìä∏Ïóê {len(ordered)}Í∞ú ÏΩîÎìú Î∏îÎ°ù Î≥ëÌï©ÌïòÏó¨ ÏµúÏ¢Ö Î≥∏Î¨∏ Íµ¨ÏÑ±")
            return

        children = sorted(entry.children, key=lambda frag: frag.sequence)
        merged_code = entry.code
        if entry.is_dml:
            merged_code = self._merge_dml_children(merged_code, children)
        else:
            merged_code = self._merge_regular_children(merged_code, children)

        merged_code = merged_code.strip()
        entry.finalized = True
        entry.children.clear()
        self._record_child_fragment(entry.parent, merged_code, entry.start, entry.end, entry.sequence)

    async def _process_work_queue(self) -> None:
        await self.work_queue.drain(self._handle_work_completion)

    def _handle_work_completion(self, item: ConversionWorkItem, generated_code: str) -> None:
        parent_entry = item.parent
        if parent_entry is None:
            return
        code_len = len(generated_code) if generated_code else 0
        log_process(
            "DBMS",
            "CONVERT",
            f"‚úÖ Î≥ÄÌôò ÏôÑÎ£å (work #{item.work_id}): ÏûêÏãù {item.start}~{item.end} Í≤∞Í≥º({code_len}Ïûê)Î•º Î∂ÄÎ™® {parent_entry.start}~{parent_entry.end}Ïóê Î∞òÏòÅ"
        )
        self._record_child_fragment(parent_entry, generated_code, item.start, item.end, item.sequence)

    async def generate(self) -> str:
        """
        Ï†ÑÏ≤¥ ÎÖ∏ÎìúÎ•º ÏàúÌöåÌïòÎ©∞ ÌÉÄÍ≤ü DBMS ÏΩîÎìú ÏÉùÏÑ±
        
        Returns:
            str: ÏµúÏ¢Ö Î≥ëÌï©Îêú ÏΩîÎìú
        """
        log_process("DBMS", "START", f"üöÄ DBMS Î≥ÄÌôò ÏãúÏûë: {self.system_name}/{self.file_name} ‚Üí {self.target.upper()}")
        self._reset_state()

        # Ï§ëÎ≥µ Ï†úÍ±∞: Í∞ôÏùÄ ÎùºÏù∏ Î≤îÏúÑÎäî Ìïú Î≤àÎßå Ï≤òÎ¶¨
        seen_nodes = set()
        node_count = 0
        for record in self.traverse_nodes:
            node = record['n']
            start_line = self._safe_int(node.get('startLine'))
            end_line = self._safe_int(node.get('endLine'))

            node_key = (start_line, end_line)
            if node_key in seen_nodes:
                continue
            seen_nodes.add(node_key)
            node_count += 1
            await self._process_node(record)

        await self._finalize_remaining()

        if self.root_entry:
            self.root_entry.closed = True
            self._try_finalize_parent(self.root_entry)

        await self._process_work_queue()

        if self.root_entry and not self.root_entry.finalized:
            self._try_finalize_parent(self.root_entry)
        if self.root_entry and self.root_entry.pending_children:
            log_process("DBMS", "CONVERT", f"‚ö†Ô∏è Î£®Ìä∏Ïóê ÏïÑÏßÅ {self.root_entry.pending_children}Í∞ú ÏûêÏãù ÏΩîÎìú ÎØ∏Ï≤òÎ¶¨ - Neo4j Îç∞Ïù¥ÌÑ∞ ÎàÑÎùΩ Ïó¨Î∂Ä ÌôïÏù∏ ÌïÑÏöî", logging.WARNING)

        log_process("DBMS", "DONE", f"‚úÖ Î≥ÄÌôò ÏôÑÎ£å: Ï¥ù {node_count}Í∞ú ÎÖ∏Îìú Ï≤òÎ¶¨")
        return self._final_output()

    # ----- ÎÖ∏Îìú Ï≤òÎ¶¨ -----

    async def _process_node(self, record: dict) -> None:
        """Îã®Ïùº ÎÖ∏Îìú Ï≤òÎ¶¨"""
        node = record['n']
        node_labels = record.get('nodeLabels', [])
        node_type = self._resolve_node_type(node_labels, node)
        has_children = bool(node.get('has_children', False))
        token = self._safe_int(node.get('token'))
        start_line = self._safe_int(node.get('startLine'))
        end_line = self._safe_int(node.get('endLine'))
        relationship = record['r'][1] if record.get('r') else 'NEXT'

        # ÎÖ∏Îìú Ï≤òÎ¶¨ Î°úÍ∑∏
        node_kind = "Î∂ÄÎ™®" if has_children else "Î¶¨ÌîÑ"
        stack_info = f", Î∂ÄÎ™® Ïä§ÌÉù ÍπäÏù¥ {len(self.parent_stack)}" if self.parent_stack else ""
        log_process("DBMS", "LEAF" if not has_children else "PARENT", f"üîç {node_type} ({start_line}~{end_line}) {node_kind} ÎÖ∏Îìú Î∂ÑÏÑù Ï§ë - ÌÜ†ÌÅ∞ {token}{stack_info}")

        # Î∂ÄÎ™® Í≤ΩÍ≥Ñ Ï≤¥ÌÅ¨
        while self.parent_stack and start_line > self.parent_stack[-1].end:
            if self.sp_accumulator.has_data():
                await self._analyze_and_merge()
            await self._finalize_parent()

        # ÎÖ∏Îìú ÌÉÄÏûÖÎ≥Ñ Ï≤òÎ¶¨
        is_large_parent = token >= TOKEN_THRESHOLD and has_children
        is_large_leaf = token >= TOKEN_THRESHOLD and not has_children

        if is_large_parent:
            # ÌÅ∞ ÎÖ∏Îìú Ï≤òÎ¶¨ Ï†ÑÏóê ÏåìÏù∏ ÏûëÏùÄ ÎÖ∏ÎìúÎì§ Î®ºÏ†Ä Î≥ÄÌôò
            if self.sp_accumulator.has_data():
                await self._analyze_and_merge()
            
            log_process("DBMS", "PARENT", f"üèóÔ∏è ÎåÄÏö©Îüâ Î∂ÄÎ™® ÎÖ∏Îìú Î∞úÍ≤¨: {node_type} ({start_line}~{end_line}, ÌÜ†ÌÅ∞ {token}) - Ïä§ÏºàÎ†àÌÜ§ ÏÉùÏÑ± ÌõÑ Î∂ÄÎ™® Ïä§ÌÉùÏóê Ï∂îÍ∞Ä (ÌòÑÏû¨ ÍπäÏù¥ {len(self.parent_stack)})")
            await self._handle_large_node(node, node_labels, start_line, end_line, token)
        else:
            appended = False
            if is_large_leaf:
                if self.sp_accumulator.has_data():
                    await self._analyze_and_merge()
            else:
                await self._flush_pending_accumulation(token)

            appended = self._handle_small_node(node, node_type, start_line, end_line, token)

            if appended and self._is_within_dml_parent():
                await self._analyze_and_merge()

        # ÏûÑÍ≥ÑÍ∞í Ï≤¥ÌÅ¨
        if is_large_leaf:
            log_process("DBMS", "CONVERT", f"‚ö° Îã®ÎèÖ ÎåÄÏö©Îüâ Î¶¨ÌîÑ ÎÖ∏Îìú Ï¶âÏãú Î≥ÄÌôò: {node_type} ({start_line}~{end_line}, ÌÜ†ÌÅ∞ {token})")
            await self._analyze_and_merge()
        elif self.sp_accumulator.token_total >= TOKEN_THRESHOLD:
            log_process("DBMS", "CONVERT", f"üìä ÌÜ†ÌÅ∞ ÏûÑÍ≥ÑÍ∞í ÎèÑÎã¨: ÎàÑÏ†Å ÌÜ†ÌÅ∞ {self.sp_accumulator.token_total} ‚â• {TOKEN_THRESHOLD} - ÏßÄÍ∏àÍπåÏßÄ Î™®ÏùÄ Íµ¨Í∞ÑÏùÑ Î≥ÄÌôòÌï©ÎãàÎã§")
            await self._analyze_and_merge()

    # ----- ÎåÄÏö©Îüâ ÎÖ∏Îìú Ï≤òÎ¶¨ -----

    async def _handle_large_node(
        self,
        node: dict,
        node_labels: list,
        start_line: int,
        end_line: int,
        token: int
    ) -> None:
        """ÎåÄÏö©Îüâ ÎÖ∏Îìú(ÏûêÏãù ÏûàÏùå, ÌÜ†ÌÅ∞‚â•1000) Ï≤òÎ¶¨"""
        summarized = (node.get('summarized_code') or '').strip()
        if not summarized:
            log_process("DBMS", "PARENT", f"‚ö†Ô∏è {start_line}~{end_line} Íµ¨Í∞ÑÏóê ÏöîÏïΩ ÏΩîÎìúÍ∞Ä ÏóÜÏñ¥ Ïä§ÏºàÎ†àÌÜ§ ÏÉùÏÑ±ÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§", logging.WARNING)
            return

        node_type = self._resolve_node_type(node_labels, node)
        is_dml_node = str(node_type).upper() in DML_TYPES

        # LLMÏúºÎ°ú Ïä§ÏºàÎ†àÌÜ§ ÏÉùÏÑ± (Rule ÌååÏùº ÏÇ¨Ïö©)
        result = self.rule_loader.execute(
            role_name='dbms_summarized_dml' if is_dml_node else 'dbms_summarized',
            inputs={
                'summarized_code': summarized,
                'locale': self.locale
            },
            api_key=self.api_key
        )
        skeleton = result['code']

        parent_entry = self.parent_stack[-1] if self.parent_stack else self.root_entry
        entry = ParentEntry(
            start=start_line,
            end=end_line,
            code=skeleton,
            is_dml=is_dml_node,
            parent=parent_entry,
            sequence=self._next_sequence()
        )
        if parent_entry is not None:
            self._register_pending_child(parent_entry)
        self.parent_stack.append(entry)
        dml_info = " (DML)" if is_dml_node else ""
        log_process("DBMS", "PARENT", f"‚úÖ {node_type} ({entry.start}~{entry.end}) Ïä§ÏºàÎ†àÌÜ§ ÏÉùÏÑ± ÏôÑÎ£å{dml_info} - Î∂ÄÎ™® Ïä§ÌÉù ÍπäÏù¥ {len(self.parent_stack)}")

    # ----- ÏÜåÌòï ÎÖ∏Îìú Ï≤òÎ¶¨ -----

    def _handle_small_node(self, node: dict, node_type: str, start_line: int, end_line: int, token: int) -> bool:
        """ÏÜåÌòï ÎÖ∏Îìú ÎòêÎäî Î¶¨ÌîÑ ÎÖ∏Îìú Ï≤òÎ¶¨"""
        node_code = (node.get('node_code') or '').strip()
        if not node_code:
            return False

        appended = self.sp_accumulator.append(node_code, token, start_line, end_line)
        if not appended:
            return False

        # Î°úÍ∑∏Îäî ÏµúÏÜåÌôî - ÎàÑÏ†Å Ï†ïÎ≥¥Îßå Í∞ÑÎã®Ìûà ÌëúÏãú
        return True

    async def _flush_pending_accumulation(self, incoming_token: int) -> None:
        """Îã§Ïùå ÎÖ∏Îìú Ï∂îÍ∞Ä Ï†ÑÏóê ÏûÑÍ≥ÑÍ∞í Ï¥àÍ≥º Ïó¨Î∂Ä ÌôïÏù∏"""
        if self.sp_accumulator.should_flush_with(incoming_token, TOKEN_THRESHOLD):
            current_tokens = self.sp_accumulator.token_total
            next_total = current_tokens + (incoming_token or 0)
            log_process("DBMS", "CONVERT", f"üìä Îã§Ïùå ÎÖ∏Îìú Ï∂îÍ∞Ä Ïãú ÌÜ†ÌÅ∞ Ï¥àÍ≥º ÏòàÏÉÅ: ÌòÑÏû¨ {current_tokens} + Îã§Ïùå {incoming_token} = {next_total} ‚â• {TOKEN_THRESHOLD} - Í∏∞Ï°¥ ÎàÑÏ†ÅÎ∂Ñ Î®ºÏ†Ä Î≥ÄÌôò")
            await self._analyze_and_merge()

    # ----- Î∂ÄÎ™® Í¥ÄÎ¶¨ -----

    async def _finalize_parent(self) -> None:
        """ÌòÑÏû¨ Î∂ÄÎ™® ÎßàÎ¨¥Î¶¨"""
        if not self.parent_stack:
            return

        entry = self.parent_stack.pop()
        log_process("DBMS", "PARENT", f"üîö Î∂ÄÎ™® ÎÖ∏Îìú Ï≤òÎ¶¨ ÏôÑÎ£å: {entry.start}~{entry.end} (ÏûêÏãù {len(entry.children)}Í∞ú Î≥ëÌï© ÏòàÏ†ï, Ïä§ÌÉù ÍπäÏù¥ {len(self.parent_stack)})")
        entry.closed = True
        self._try_finalize_parent(entry)

    # ----- Î∂ÑÏÑù Î∞è Î≥ëÌï© -----

    async def _analyze_and_merge(self) -> None:
        """LLM Î∂ÑÏÑù Î∞è ÌÉÄÍ≤ü DBMS ÏΩîÎìú Î≥ëÌï©"""
        if not self.sp_accumulator.has_data():
            return

        sp_code, child_start, child_end, token_total, part_count = self.sp_accumulator.consume()
        parent_entry = self.parent_stack[-1] if self.parent_stack else self.root_entry
        
        # Î∂ÄÎ™® Ï†ïÎ≥¥ Íµ¨ÏÑ±
        if parent_entry and not parent_entry.is_root:
            parent_info = f"Î∂ÄÎ™® {parent_entry.start}~{parent_entry.end}"
            target = "Î∂ÄÎ™® children"
        else:
            parent_info = "Î£®Ìä∏"
            target = "ÏµúÏ¢Ö ÏΩîÎìú"
        
        log_process("DBMS", "CONVERT", f"üöÄ Î≥ÄÌôò ÏãúÏûë: ÎùºÏù∏ {child_start}~{child_end} ({part_count}Í∞ú Ï°∞Í∞Å, {token_total} ÌÜ†ÌÅ∞) ‚Üí {target} ({parent_info})")

        parent_code = parent_entry.code if parent_entry else ""
        if parent_entry:
            self._register_pending_child(parent_entry)
        child_start = child_start if child_start is not None else 0
        child_end = child_end if child_end is not None else child_start
        work_item = ConversionWorkItem(
            work_id=self._next_work_id(),
            sequence=self._next_sequence(),
            code=sp_code,
            start=child_start,
            end=child_end,
            parent=parent_entry,
            parent_code=parent_code,
            token_count=token_total
        )
        self.work_queue.enqueue(work_item)

    def _final_output(self) -> str:
        """ÎàÑÏ†ÅÎêú ÏµúÏÉÅÏúÑ ÏΩîÎìúÎ•º Îã®Ïùº Î¨∏ÏûêÏó¥Î°ú Î∞òÌôò"""
        return "\n".join(self.merged_chunks).strip()

    def _merge_regular_children(self, code: str, children: list[ChildFragment]) -> str:
        """ÎπÑ-DML Î∂ÄÎ™® placeholder Ï≤òÎ¶¨"""
        ordered_children = [
            fragment.code for fragment in children or []
            if fragment.code and fragment.code.strip()
        ]
        child_block = "\n".join(ordered_children).strip()

        if CODE_PLACEHOLDER in code:
            if child_block:
                indented = textwrap.indent(child_block, '    ')
                return code.replace(CODE_PLACEHOLDER, f"\n{indented}\n", 1)
            return code.replace(CODE_PLACEHOLDER, "", 1)

        if not child_block:
            return code

        indented = textwrap.indent(child_block, '    ')
        return f"{code}\n{indented}"

    def _merge_dml_children(self, code: str, children: list[ChildFragment]) -> str:
        """DML Ïä§ÏºàÎ†àÌÜ§ placeholderÏóê ÏûêÏãù ÏΩîÎìúÎ•º Ï£ºÏûÖ"""
        children_by_start: dict[int, deque] = defaultdict(deque)
        fallback_children: deque = deque()

        for fragment in children or []:
            payload = {
                'code': fragment.code,
                'start': fragment.start,
                'end': fragment.end
            }

            raw_start = payload.get('start')
            try:
                start_line = int(raw_start) if raw_start is not None else None
            except (TypeError, ValueError):
                start_line = None
            if start_line is None:
                fallback_children.append(payload)
            else:
                children_by_start[start_line].append(payload)

        placeholders = list(DML_PLACEHOLDER_PATTERN.finditer(code))
        placeholder_starts = [int(match.group('start')) for match in placeholders]
        total_children = sum(len(queue) for queue in children_by_start.values()) + len(fallback_children)
        log_process("DBMS", "PARENT", f"üîó DML Î≥ëÌï© ÏãúÏûë: {len(placeholders)}Í∞ú placeholder({placeholder_starts})Ïóê {total_children}Í∞ú ÏûêÏãù ÏΩîÎìú Îß§Ìïë")

        def _replacement(match: re.Match) -> str:
            indent = match.group('indent') or ''
            start = int(match.group('start'))
            label = match.group('label')

            queue = children_by_start.get(start)
            child_queue = children_by_start.get(start)
            child = child_queue.popleft() if child_queue else None
            if child_queue is not None and not child_queue:
                children_by_start.pop(start, None)

            if not child:
                remaining_starts = sorted(children_by_start.keys()) or ['ÏóÜÏùå']
                remaining_count = sum(len(queue) for queue in children_by_start.values()) + len(fallback_children)
                log_process("DBMS", "PARENT", f"‚ö†Ô∏è DML placeholder {label} (ÎùºÏù∏ {start})ÏôÄ Îß§Ïπ≠ÎêòÎäî ÏûêÏãù ÏóÜÏùå - ÎÇ®ÏùÄ ÌõÑÎ≥¥: {remaining_starts} (Ï¥ù {remaining_count}Í∞ú)", logging.WARNING)
                return match.group(0)

            child_code = (child.get('code') or '').strip()
            if not child_code:
                log_process("DBMS", "PARENT", f"‚ö†Ô∏è DML placeholder {label}Ïóê Ïó∞Í≤∞Îêú {child.get('start')}~{child.get('end')} ÏΩîÎìúÍ∞Ä ÎπÑÏñ¥ÏûàÏùå - placeholder Ïú†ÏßÄ", logging.WARNING)
                return match.group(0)

            return textwrap.indent(child_code, indent)

        merged_code = DML_PLACEHOLDER_PATTERN.sub(_replacement, code)

        residual_entries: list[dict] = []
        for start_line in sorted(children_by_start.keys()):
            residual_entries.extend(children_by_start[start_line])
        residual_entries.extend(fallback_children)

        if residual_entries:
            residual = "\n".join(
                (entry.get('code') or '').strip()
                for entry in residual_entries
                if entry.get('code')
            ).strip()
            if residual:
                merged_code = f"{merged_code.rstrip()}\n{residual}"
                log_process("DBMS", "PARENT", f"‚ö†Ô∏è DML placeholderÎ≥¥Îã§ ÏûêÏãù {len(residual_entries)}Í∞úÍ∞Ä ÎßéÏïÑ ÌïòÎã®Ïóê residual Î∏îÎ°ùÏúºÎ°ú Ï∂îÍ∞Ä", logging.WARNING)
            else:
                log_process("DBMS", "PARENT", f"‚ö†Ô∏è DML placeholderÎ≥¥Îã§ ÏûêÏãù {len(residual_entries)}Í∞úÍ∞Ä ÎÇ®ÏïòÏßÄÎßå Î™®Îëê Îπà Î¨∏ÏûêÏó¥Ïù¥Îùº Ï†úÏô∏", logging.WARNING)

        return merged_code

    def _is_within_dml_parent(self) -> bool:
        """ÌòÑÏû¨ Ïä§ÌÉù ÏµúÏÉÅÎã®Ïù¥ DML Î∂ÄÎ™®Ïù∏ÏßÄ ÌôïÏù∏"""
        return bool(self.parent_stack and self.parent_stack[-1].is_dml)

    # ----- ÎßàÎ¨¥Î¶¨ -----

    async def _finalize_remaining(self) -> None:
        """ÎÇ®ÏùÄ Îç∞Ïù¥ÌÑ∞ Ï†ïÎ¶¨"""
        if self.parent_stack:
            if self.sp_accumulator.has_data():
                await self._analyze_and_merge()
            while self.parent_stack:
                await self._finalize_parent()
        elif self.sp_accumulator.has_data():
            await self._analyze_and_merge()

    async def _save_target_file(self, base_name: str) -> str:
        """ÌÉÄÍ≤ü DBMS ÌååÏùº ÏûêÎèô Ï†ÄÏû•"""
        try:
            # Ï†ÄÏû• Í≤ΩÎ°ú ÏÑ§Ï†ï
            base_path = build_rule_based_path(
                self.project_name,
                self.user_id,
                self.target,
                'dbms_conversion',
                system_name=self.system_name
            )
            
            body_code = self._final_output().strip()
            header_code = self.skeleton_code.strip()

            parts = [part for part in [header_code, body_code] if part]
            final_code = "\n\n".join(parts).rstrip() + "\n"

            # ÌååÏùº Ï†ÄÏû•
            await save_file(
                content=final_code,
                filename=f"{base_name}.sql",
                base_path=base_path
            )
            
            log_process("DBMS", "SAVE", f"üíæ {self.target.upper()} ÌååÏùº Ï†ÄÏû• ÏôÑÎ£å: {base_path}/{base_name}.sql")
            
            return final_code
            
        except Exception as e:
            log_process("DBMS", "ERROR", f"‚ùå {self.target.upper()} ÌååÏùº Ï†ÄÏû• Ïã§Ìå®: {e}", logging.ERROR, e)
            raise ConvertingError(f"{self.target.upper()} ÌååÏùº Ï†ÄÏû• Ï§ë Ïò§Î•ò: {str(e)}")


# ----- ÏßÑÏûÖÏ†ê Ìï®Ïàò -----
async def start_dbms_conversion(
    system_name: str,
    file_name: str,
    procedure_name: str,
    project_name: str,
    user_id: str,
    api_key: str,
    locale: str,
    target: str = "oracle"
) -> str:
    """
    DBMS Î≥ÄÌôò ÏãúÏûë (Îã®Ïùº Ìï®Ïàò Ìò∏Ï∂úÏö©)
    """
    result = await start_dbms_conversion_steps(
        system_name, file_name, procedure_name,
        project_name, user_id, api_key, locale, target
    )
    return result["converted_code"]


async def start_dbms_conversion_steps(
    system_name: str,
    file_name: str,
    procedure_name: str,
    project_name: str,
    user_id: str,
    api_key: str,
    locale: str,
    target: str = "oracle",
    on_step: callable = None
) -> dict:
    """
    DBMS Î≥ÄÌôò (Îã®Í≥ÑÎ≥Ñ ÏΩúÎ∞± ÏßÄÏõê)
    
    Args:
        system_name: ÏãúÏä§ÌÖúÎ™Ö
        file_name: ÌååÏùºÎ™Ö
        procedure_name: ÌîÑÎ°úÏãúÏ†Ä Ïù¥Î¶Ñ
        project_name: ÌîÑÎ°úÏ†ùÌä∏ Ïù¥Î¶Ñ
        user_id: ÏÇ¨Ïö©Ïûê ID
        api_key: LLM API ÌÇ§
        locale: Î°úÏºÄÏùº
        target: ÌÉÄÍ≤ü DBMS (oracle, postgresql)
        on_step: Îã®Í≥Ñ ÏΩúÎ∞± Ìï®Ïàò (step: int, name: str, done: bool) -> None
    
    Returns:
        dict: {
            "skeleton_code": str,
            "converted_code": str,
            "procedure_name": str
        }
    
    Raises:
        ConvertingError: Î≥ÄÌôò Ï§ë Ïò§Î•ò Î∞úÏÉù Ïãú
    """
    connection = Neo4jConnection()
    
    log_process("DBMS", "START", f"üöÄ DBMS Î≥ÄÌôò Ï§ÄÎπÑ: {system_name}/{file_name} ‚Üí {target.upper()}")

    try:
        # Step 1: Ïä§ÏºàÎ†àÌÜ§ ÏÉùÏÑ±
        if on_step:
            on_step(1, "skeleton", False)
        
        skeleton_code = await start_dbms_skeleton(
            system_name=system_name,
            file_name=file_name,
            procedure_name=procedure_name,
            project_name=project_name,
            user_id=user_id,
            api_key=api_key,
            locale=locale,
            target=target
        )
        
        if on_step:
            on_step(1, "skeleton", True)

        # Step 2: Neo4j ÎÖ∏Îìú Ï°∞Ìöå Î∞è Î≥∏Î¨∏ Î≥ÄÌôò
        if on_step:
            on_step(2, "body", False)
        
        # Neo4j ÏøºÎ¶¨
        query_results = await connection.execute_queries([
            f"""
            MATCH (p:PROCEDURE {{
              system_name: '{system_name}',
              file_name: '{file_name}',
              procedure_name: '{procedure_name}',
              user_id: '{user_id}'
            }})
            
            CALL {{
              WITH p
              MATCH (p)-[:PARENT_OF]->(c)
              WHERE NOT c:DECLARE AND NOT c:Table AND NOT c:SPEC
                AND c.token < 1000
              WITH c, labels(c) AS cLabels, coalesce(toInteger(c.startLine), 0) AS sortKey
              RETURN c AS n, cLabels AS nodeLabels, NULL AS r, NULL AS m, sortKey
              
              UNION ALL
              
              WITH p
              MATCH (p)-[:PARENT_OF]->(c)
              WHERE NOT c:DECLARE AND NOT c:Table AND NOT c:SPEC
                AND coalesce(toInteger(c.token), 0) >= 1000
              WITH c
              MATCH path = (c)-[:PARENT_OF*0..]->(n)
              WHERE NOT n:DECLARE AND NOT n:Table AND NOT n:SPEC
              WITH n, path, nodes(path) AS pathNodes
              WHERE ALL(i IN range(0, size(pathNodes)-2) 
                        WHERE coalesce(toInteger(pathNodes[i].token), 0) >= 1000)
              OPTIONAL MATCH (n)-[r]->(m {{
                system_name: '{system_name}', file_name: '{file_name}', user_id: '{user_id}'
              }})
              WHERE r IS NULL
                 OR ( NOT (m:DECLARE OR m:Table OR m:SPEC)
                      AND none(x IN ['CALL','WRITES','FROM'] WHERE type(r) CONTAINS x) )
              WITH n, labels(n) AS nLabels, r, m, coalesce(toInteger(n.startLine), 0) AS sortKey
              RETURN DISTINCT n, nLabels AS nodeLabels, r, m, sortKey
            }}
            
            RETURN n, nodeLabels, r, m
            ORDER BY sortKey, coalesce(toInteger(n.token), 0), id(n)
            """
        ])
        dbms_nodes = query_results[0] if query_results else []

        # Î≥ÄÌôò ÏàòÌñâ
        generator = DbmsConversionGenerator(
            dbms_nodes,
            system_name,
            file_name,
            procedure_name,
            user_id,
            api_key,
            locale,
            project_name,
            target,
            skeleton_code
        )

        await generator.generate()
        
        # ÌååÏùº Ï†ÄÏû•
        base_name = file_name.rsplit(".", 1)[0]
        converted_code = await generator._save_target_file(base_name)
        
        if on_step:
            on_step(2, "body", True)

        log_process("DBMS", "DONE", f"‚úÖ {base_name} Î≥ÄÌôò ÏôÑÎ£å")
        
        return {
            "skeleton_code": skeleton_code,
            "converted_code": converted_code,
            "procedure_name": procedure_name
        }

    except ConvertingError:
        raise
    except Exception as e:
        err_msg = f"DBMS Î≥ÄÌôò Ï§ë Ïò§Î•ò: {str(e)}"
        log_process("DBMS", "ERROR", f"‚ùå {err_msg}", logging.ERROR, e)
        raise ConvertingError(err_msg)
    finally:
        await connection.close()

