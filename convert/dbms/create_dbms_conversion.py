import logging
import textwrap
from understand.neo4j_connection import Neo4jConnection
from util.exception import ConvertingError
from util.utility_tool import (
    build_rule_based_path, save_file,
    build_converting_root_query, build_conversion_block_query
)
from util.rule_loader import RuleLoader
from convert.dbms.create_dbms_skeleton import start_dbms_skeleton


# ----- ìƒìˆ˜ ì •ì˜ -----
TOKEN_THRESHOLD = 1000
CODE_PLACEHOLDER = "...code..."
DML_TYPES = frozenset(["SELECT", "INSERT", "UPDATE", "DELETE", "FETCH", "MERGE", "JOIN", "ALL_UNION", "UNION"])


# ----- DBMS ë³€í™˜ í´ë˜ìŠ¤ -----
class DbmsConversionGenerator:
    """
    DBMS ë³€í™˜ ì „ì²´ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬
    - ë‹¨ì¼ ì»¨í…ìŠ¤íŠ¸ ëˆ„ì  ë°©ì‹ìœ¼ë¡œ íƒ€ê²Ÿ DBMS ì½”ë“œ ìƒì„±
    - ëŒ€ìš©ëŸ‰ ë¶€ëª¨(í† í°â‰¥1000, ìì‹ ë³´ìœ ) ìŠ¤ì¼ˆë ˆí†¤ ê´€ë¦¬
    - í† í° ì„ê³„ ë„ë‹¬ ì‹œ LLM ë¶„ì„ ìˆ˜í–‰
    """
    __slots__ = (
        'traverse_nodes', 'folder_name', 'file_name', 'procedure_name',
        'user_id', 'api_key', 'locale', 'project_name', 'target_dbms', 'skeleton_code',
        'merged_code', 'total_tokens', 'parent_stack', 'top_level_begin_skipped',
        'sp_code_parts', 'sp_start', 'sp_end',
        'rule_loader', 'conversion_queries', 'last_block_range'
    )

    def __init__(self, traverse_nodes: list, folder_name: str, file_name: str,
                 procedure_name: str, user_id: str, api_key: str, locale: str, 
                 project_name: str = "demo", target_dbms: str = "oracle",
                 skeleton_code: str | None = None):
        self.traverse_nodes = traverse_nodes
        self.folder_name = folder_name
        self.file_name = file_name
        self.procedure_name = procedure_name
        self.user_id = user_id
        self.api_key = api_key
        self.locale = locale
        self.project_name = project_name or "demo"
        self.target_dbms = target_dbms
        self.skeleton_code = (skeleton_code or "").strip()

        # ìƒíƒœ ì´ˆê¸°í™”
        self.merged_code = ""
        self.total_tokens = int(0)
        self.parent_stack = []
        self.top_level_begin_skipped = False
        self.sp_code_parts = []
        self.sp_start = None
        self.sp_end = None
        self.conversion_queries = []
        self.last_block_range = None  # (start_line, end_line) - NEXT ê´€ê³„ìš©
        
        # Rule íŒŒì¼ ë¡œë” (target_dbmsë¡œ ë””ë ‰í† ë¦¬ ì°¾ìŒ)
        self.rule_loader = RuleLoader(target_lang=target_dbms)

    # ----- ê³µê°œ ë©”ì„œë“œ -----

    async def generate(self) -> str:
        """
        ì „ì²´ ë…¸ë“œë¥¼ ìˆœíšŒí•˜ë©° íƒ€ê²Ÿ DBMS ì½”ë“œ ìƒì„±
        
        Returns:
            str: ìµœì¢… ë³‘í•©ëœ ì½”ë“œ
        """
        logging.info(f"ğŸ“‹ DBMS ë³€í™˜ ë…¸ë“œ ìˆœíšŒ ì‹œì‘: postgres â†’ {self.target_dbms}")

        # CONVERTING ë£¨íŠ¸ ë…¸ë“œ ìƒì„± (ë³€í™˜ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ)
        root_query = build_converting_root_query(
            folder_name=self.folder_name,
            file_name=self.file_name,
            procedure_name=self.procedure_name,
            user_id=self.user_id,
            project_name=self.project_name,
            conversion_type="dbms",
            target=self.target_dbms
        )
        self.conversion_queries.append(root_query)

        # ì¤‘ë³µ ì œê±°: ê°™ì€ ë¼ì¸ ë²”ìœ„ëŠ” í•œ ë²ˆë§Œ ì²˜ë¦¬
        seen_nodes = set()
        node_count = 0
        for record in self.traverse_nodes:
            node = record['n']
            node_key = (node.get('startLine'), node.get('endLine'))
            if node_key in seen_nodes:
                continue
            seen_nodes.add(node_key)
            node_count += 1
            await self._process_node(record)

        await self._finalize_remaining()

        logging.info(f"âœ… ì´ {node_count}ê°œ ë…¸ë“œ ì²˜ë¦¬ ì™„ë£Œ\n")
        return self.merged_code.strip()

    # ----- ë…¸ë“œ ì²˜ë¦¬ -----

    async def _process_node(self, record: dict) -> None:
        """ë‹¨ì¼ ë…¸ë“œ ì²˜ë¦¬"""
        node = record['n']
        node_labels = record.get('nodeLabels', [])
        node_type = node_labels[0] if node_labels else node.get('name', 'UNKNOWN')
        has_children = bool(node.get('has_children', False))
        token = int(node.get('token', 0) or 0)
        start_line = int(node.get('startLine', 0) or 0)
        end_line = int(node.get('endLine', 0) or 0)
        relationship = record['r'][1] if record.get('r') else 'NEXT'

        # ë…¸ë“œ ì²˜ë¦¬ ë¡œê·¸
        readable_type = node_type.split('[')[0] if '[' in str(node_type) else str(node_type)
        logging.info(
            "â¡ï¸  ë…¸ë“œ ê°ì§€ | íƒ€ì…=%s | ë¼ì¸=%s~%s | í† í°=%s | ê´€ê³„=%s | ìì‹=%s | stack_depth=%s",
            readable_type,
            start_line,
            end_line,
            token,
            relationship,
            "ìˆìŒ" if has_children else "ì—†ìŒ",
            len(self.parent_stack)
        )

        # ë¶€ëª¨ ê²½ê³„ ì²´í¬
        while self.parent_stack and start_line > self.parent_stack[-1]['end']:
            if self.sp_code_parts:
                await self._analyze_and_merge()
            await self._finalize_parent()

        # ìµœìƒìœ„ BEGIN ë¸”ë¡ì€ ìŠ¤ì¼ˆë ˆí†¤ì´ ì²˜ë¦¬í•˜ë¯€ë¡œ ìŠ¤í‚µ
        if (readable_type == "BEGIN"
                and not self.top_level_begin_skipped
                and not self.parent_stack
                and not self.merged_code):
            self.top_level_begin_skipped = True
            logging.info("    â›” ìµœìƒìœ„ BEGIN ë¸”ë¡ ìŠ¤í‚µ (ìŠ¤ì¼ˆë ˆí†¤ì—ì„œ ì²˜ë¦¬)")
            return

        # ë…¸ë“œ íƒ€ì…ë³„ ì²˜ë¦¬
        is_large_parent = token >= TOKEN_THRESHOLD and has_children and node_type not in DML_TYPES
        is_large_leaf = token >= TOKEN_THRESHOLD and not has_children

        if is_large_parent:
            # í° ë…¸ë“œ ì²˜ë¦¬ ì „ì— ìŒ“ì¸ ì‘ì€ ë…¸ë“œë“¤ ë¨¼ì € ë³€í™˜
            if self.sp_code_parts:
                await self._analyze_and_merge()
            
            logging.info(
                "    ğŸ§± ëŒ€ìš©ëŸ‰ ë…¸ë“œ ì²˜ë¦¬ ì¤€ë¹„ | ë¼ì¸=%s~%s | í† í°=%s | í˜„ì¬ stack=%s",
                start_line,
                end_line,
                token,
                len(self.parent_stack)
            )
            await self._handle_large_node(node, start_line, end_line, token)
        else:
            if is_large_leaf:
                if self.sp_code_parts:
                    await self._analyze_and_merge()
            else:
                await self._flush_pending_accumulation(token)

            logging.info(
                "    âœï¸ ì¼ë°˜ ë…¸ë“œ ëˆ„ì  | ë¼ì¸=%s~%s | í† í°=%s | í˜„ì¬ stack=%s",
                start_line,
                end_line,
                token,
                len(self.parent_stack)
            )
            self._handle_small_node(node, start_line, end_line, token)

        # ì„ê³„ê°’ ì²´í¬
        if is_large_leaf:
            logging.info("    âš ï¸  ë‹¨ë… ëŒ€ìš©ëŸ‰ ë¦¬í”„ ë…¸ë“œ ë³€í™˜ ì‹¤í–‰")
            await self._analyze_and_merge()
        elif int(self.total_tokens) >= TOKEN_THRESHOLD:
            logging.info("    âš ï¸  í† í° ëˆ„ì  %s â‰¥ %s â†’ LLM ë¶„ì„ ì‹¤í–‰", int(self.total_tokens), TOKEN_THRESHOLD)
            await self._analyze_and_merge()

    # ----- ëŒ€ìš©ëŸ‰ ë…¸ë“œ ì²˜ë¦¬ -----

    async def _handle_large_node(self, node: dict, start_line: int, end_line: int, token: int) -> None:
        """ëŒ€ìš©ëŸ‰ ë…¸ë“œ(ìì‹ ìˆìŒ, í† í°â‰¥1000) ì²˜ë¦¬"""
        summarized = (node.get('summarized_code') or '').strip()
        if not summarized:
            logging.info("      â›” ìš”ì•½ ì½”ë“œ ì—†ìŒ â†’ ìŠ¤í‚µ")
            return

        # LLMìœ¼ë¡œ ìŠ¤ì¼ˆë ˆí†¤ ìƒì„± (Rule íŒŒì¼ ì‚¬ìš©)
        result = self.rule_loader.execute(
            role_name='dbms_summarized',
            inputs={
                'summarized_code': summarized,
                'locale': self.locale
            },
            api_key=self.api_key
        )
        skeleton = result['code']

        # í° ë…¸ë“œë„ CONVERSION_BLOCKìœ¼ë¡œ ì €ì¥
        original_code = (node.get('node_code') or summarized).strip()
        self._create_and_add_block_query(
            start_line=start_line,
            end_line=end_line,
            original_code=original_code,
            converted_code=skeleton
        )

        entry = {
            'start': start_line,
            'end': end_line,
            'code': skeleton,
            'children': []
        }
        self.parent_stack.append(entry)
        logging.info(
            "      ğŸ“¦ ë¶€ëª¨ ìŠ¤ì¼ˆë ˆí†¤ push | ë¼ì¸=%s~%s | stack=%s",
            start_line,
            end_line,
            len(self.parent_stack)
        )

    # ----- ì†Œí˜• ë…¸ë“œ ì²˜ë¦¬ -----

    def _handle_small_node(self, node: dict, start_line: int, end_line: int, token: int) -> None:
        """ì†Œí˜• ë…¸ë“œ ë˜ëŠ” ë¦¬í”„ ë…¸ë“œ ì²˜ë¦¬"""
        node_code = (node.get('node_code') or '').strip()
        if not node_code:
            logging.info("    â›” ë…¸ë“œ ì½”ë“œ ì—†ìŒ â†’ ìŠ¤í‚µ")
            return

        # SP ì½”ë“œ ëˆ„ì 
        self.sp_code_parts.append(node_code)
        self.total_tokens = int(self.total_tokens) + int(token)
        logging.info(
            "    âœï¸  ë¦¬í”„/ì†Œí˜• ë…¸ë“œ ëˆ„ì  | í˜„ì¬ íŒŒíŠ¸ %sê°œ | ëˆ„ì  í† í°: %s",
            len(self.sp_code_parts),
            self.total_tokens
        )

        # ë²”ìœ„ ì—…ë°ì´íŠ¸
        if self.sp_start is None or start_line < self.sp_start:
            self.sp_start = start_line
        if self.sp_end is None or end_line > self.sp_end:
            self.sp_end = end_line

    async def _flush_pending_accumulation(self, incoming_token: int) -> None:
        """ë‹¤ìŒ ë…¸ë“œ ì¶”ê°€ ì „ì— ì„ê³„ê°’ ì´ˆê³¼ ì—¬ë¶€ í™•ì¸"""
        if (self.sp_code_parts
                and incoming_token is not None
                and (int(self.total_tokens) + int(incoming_token)) >= TOKEN_THRESHOLD):
            logging.info("    âš ï¸  ë‹¤ìŒ ë…¸ë“œ ì¶”ê°€ ì‹œ í† í° ì´ˆê³¼ ì˜ˆìƒ â†’ ê¸°ì¡´ ëˆ„ì  ë³€í™˜")
            await self._analyze_and_merge()

    # ----- ë¶€ëª¨ ê´€ë¦¬ -----

    async def _finalize_parent(self) -> None:
        """í˜„ì¬ ë¶€ëª¨ ë§ˆë¬´ë¦¬"""
        if not self.parent_stack:
            return

        entry = self.parent_stack.pop()
        logging.info(
            "    âœ… ë¶€ëª¨ ìŠ¤ì¼ˆë ˆí†¤ pop | ë¼ì¸=%s~%s | ì”ì—¬ children=%s | stackâ†’%s",
            entry['start'],
            entry['end'],
            len(entry['children']),
            len(self.parent_stack)
        )

        code = entry['code']
        child_block = "\n".join(entry['children']).strip()

        if CODE_PLACEHOLDER in code:
            if child_block:
                indented = textwrap.indent(child_block, '    ')
                code = code.replace(CODE_PLACEHOLDER, f"\n{indented}\n", 1)
            else:
                code = code.replace(CODE_PLACEHOLDER, "", 1)
        elif child_block:
            indented = textwrap.indent(child_block, '    ')
            code = f"{code}\n{indented}"

        code = code.strip()

        if self.parent_stack:
            self.parent_stack[-1]['children'].append(code)
            logging.info(
                "      ğŸ” ìƒìœ„ ë¶€ëª¨ childrenì— merge | ìƒìœ„ ë¼ì¸=%s~%s | stack=%s",
                self.parent_stack[-1]['start'],
                self.parent_stack[-1]['end'],
                len(self.parent_stack)
            )
        else:
            self.merged_code += f"\n{code}"
            logging.info("      ğŸ§© ìµœìƒìœ„ ì½”ë“œì— ë³‘í•© ì™„ë£Œ")

    # ----- ë¶„ì„ ë° ë³‘í•© -----

    async def _analyze_and_merge(self) -> None:
        """LLM ë¶„ì„ ë° íƒ€ê²Ÿ DBMS ì½”ë“œ ë³‘í•©"""
        if not self.sp_code_parts or self.sp_start is None:
            return

        # ë¬¸ìì—´ ì¡°ì¸
        sp_code = '\n'.join(self.sp_code_parts)
        target = "ë¶€ëª¨ children" if self.parent_stack else "ìµœì¢…ì½”ë“œ"
        logging.info(
            "    ğŸ¤– LLM ë³€í™˜ ìš”ì²­ | ë¼ì¸: %s~%s | íŒŒíŠ¸ ìˆ˜: %s | í† í°: %s | ëŒ€ìƒ: %s",
            self.sp_start,
            self.sp_end,
            len(self.sp_code_parts),
            self.total_tokens,
            target
        )

        parent_code = self._build_parent_context()
        logging.debug(
            "      â†³ parent_code ê¸¸ì´=%s | stack=%s",
            len(parent_code),
            len(self.parent_stack)
        )
        result = self.rule_loader.execute(
            role_name='dbms_conversion',
            inputs={
                'code': sp_code,
                'locale': self.locale,
                'parent_code': parent_code
            },
            api_key=self.api_key
        )

        # ìƒì„±ëœ ì½”ë“œ ë³‘í•©
        generated_code = (result.get('code') or '').strip()
        if generated_code:
            # CONVERSION_BLOCK ë…¸ë“œ ì¿¼ë¦¬ ìƒì„±
            self._create_and_add_block_query(
                start_line=self.sp_start,
                end_line=self.sp_end,
                original_code=sp_code,
                converted_code=generated_code
            )
            
            if self.parent_stack:
                self.parent_stack[-1]['children'].append(generated_code)
                logging.info(
                    "      â• í˜„ì¬ ë¶€ëª¨(children) ì¶”ê°€ | ë¶€ëª¨ ë¼ì¸=%s~%s | child_len=%s",
                    self.parent_stack[-1]['start'],
                    self.parent_stack[-1]['end'],
                    len(self.parent_stack[-1]['children'])
                )
            else:
                self.merged_code += f"\n{generated_code}"
                logging.info("      â• ìµœì¢… ì½”ë“œì— ë³€í™˜ ê²°ê³¼ ì¶”ê°€")

        # ìƒíƒœ ì´ˆê¸°í™”
        self.total_tokens = int(0)
        self.sp_code_parts.clear()
        self.sp_start = None
        self.sp_end = None

    def _build_parent_context(self) -> str:
        """í˜„ì¬ ë¶€ëª¨ ìŠ¤ì¼ˆë ˆí†¤ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        if not self.parent_stack:
            return ""

        entry = self.parent_stack[-1]
        return entry['code']

    def _get_current_parent_range(self) -> tuple[int | None, int | None]:
        """í˜„ì¬ ë¶€ëª¨ ë²”ìœ„ ë°˜í™˜ (ìŠ¤íƒì˜ ë§ˆì§€ë§‰ í•­ëª©)"""
        if not self.parent_stack:
            return None, None
        entry = self.parent_stack[-1]
        return entry['start'], entry['end']

    def _calculate_next_relation(self, parent_start: int | None, parent_end: int | None) -> tuple[int | None, int | None]:
        """NEXT ê´€ê³„ ê³„ì‚°
        
        Args:
            parent_start: ë¶€ëª¨ ì‹œì‘ ë¼ì¸
            parent_end: ë¶€ëª¨ ì¢…ë£Œ ë¼ì¸
        
        Returns:
            (prev_start, prev_end): ì´ì „ ë¸”ë¡ ë²”ìœ„ ë˜ëŠ” (None, None)
        """
        if not self.last_block_range:
            return None, None
        
        if parent_start is None and parent_end is None:
            # ë¶€ëª¨ê°€ ì—†ìœ¼ë©´ ê°™ì€ ë ˆë²¨ í˜•ì œ â†’ NEXT ìƒì„±
            return self.last_block_range[0], self.last_block_range[1]
        elif (parent_start is not None and parent_end is not None and
              parent_start < self.last_block_range[0] and 
              self.last_block_range[1] < parent_end):
            # ê°™ì€ ë¶€ëª¨ì˜ í˜•ì œ ë…¸ë“œ â†’ NEXT ìƒì„±
            # (last_block_rangeê°€ ë¶€ëª¨ ë²”ìœ„ ë‚´ì— ìˆê³ , ë¶€ëª¨ ìì²´ê°€ ì•„ë‹˜)
            return self.last_block_range[0], self.last_block_range[1]
        
        return None, None

    def _create_and_add_block_query(
        self,
        start_line: int,
        end_line: int,
        original_code: str,
        converted_code: str
    ) -> None:
        """CONVERSION_BLOCK ì¿¼ë¦¬ ìƒì„± ë° ì¶”ê°€"""
        parent_start, parent_end = self._get_current_parent_range()
        prev_start, prev_end = self._calculate_next_relation(parent_start, parent_end)
        
        block_query = build_conversion_block_query(
            folder_name=self.folder_name,
            file_name=self.file_name,
            procedure_name=self.procedure_name,
            user_id=self.user_id,
            start_line=start_line,
            end_line=end_line,
            original_code=original_code,
            converted_code=converted_code,
            conversion_type="dbms",
            target=self.target_dbms,
            parent_start_line=parent_start,
            parent_end_line=parent_end,
            prev_start_line=prev_start,
            prev_end_line=prev_end
        )
        self.conversion_queries.append(block_query)
        self.last_block_range = (start_line, end_line)

    # ----- ë§ˆë¬´ë¦¬ -----

    async def _finalize_remaining(self) -> None:
        """ë‚¨ì€ ë°ì´í„° ì •ë¦¬"""
        if self.parent_stack:
            if self.sp_code_parts:
                await self._analyze_and_merge()
            while self.parent_stack:
                await self._finalize_parent()
        elif self.sp_code_parts:
            await self._analyze_and_merge()

    async def _save_target_file(self, base_name: str) -> str:
        """íƒ€ê²Ÿ DBMS íŒŒì¼ ìë™ ì €ì¥"""
        try:
            # ì €ì¥ ê²½ë¡œ ì„¤ì •
            base_path = build_rule_based_path(
                self.project_name,
                self.user_id,
                self.target_dbms,
                'dbms_conversion',
                folder_name=self.folder_name
            )
            
            # ìŠ¤ì¼ˆë ˆí†¤ê³¼ ë³‘í•©
            final_code = self.skeleton_code.replace("CodePlaceHolder", self.merged_code.strip())

            # íŒŒì¼ ì €ì¥
            await save_file(
                content=final_code,
                filename=f"{base_name}.sql",
                base_path=base_path
            )
            
            logging.info(f"âœ… [{base_name}] {self.target_dbms.capitalize()} íŒŒì¼ ìë™ ì €ì¥ ì™„ë£Œ")
            logging.info(f"ğŸ“ ì €ì¥ ê²½ë¡œ: {base_path}/{base_name}.sql")
            
            return final_code
            
        except Exception as e:
            logging.error(f"âŒ {self.target_dbms.capitalize()} íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            raise ConvertingError(f"{self.target_dbms.capitalize()} íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")


# ----- ì§„ì…ì  í•¨ìˆ˜ -----
async def start_dbms_conversion(
    folder_name: str,
    file_name: str,
    procedure_name: str,
    project_name: str,
    user_id: str,
    api_key: str,
    locale: str,
    target_dbms: str = "oracle"
) -> str:
    """
    DBMS ë³€í™˜ ì‹œì‘
    
    Args:
        folder_name: í´ë”ëª…
        file_name: íŒŒì¼ëª…
        procedure_name: í”„ë¡œì‹œì € ì´ë¦„
        project_name: í”„ë¡œì íŠ¸ ì´ë¦„
        user_id: ì‚¬ìš©ì ID
        api_key: LLM API í‚¤
        locale: ë¡œì¼€ì¼
        target_dbms: íƒ€ê²Ÿ DBMS (oracle ë“±)
    
    Returns:
        str: ë³€í™˜ëœ ì½”ë“œ
    
    Raises:
        ConvertingError: ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ
    """
    connection = Neo4jConnection()
    
    logging.info(f"DBMS ë³€í™˜ ì‹œì‘: {folder_name}/{file_name} (POSTGRES â†’ {target_dbms.upper()})")

    try:
        # Neo4j ì¿¼ë¦¬
        query_results = await connection.execute_queries([
            f"""
            MATCH (p:PROCEDURE {{
              folder_name: '{folder_name}',
              file_name: '{file_name}',
              procedure_name: '{procedure_name}',
              user_id: '{user_id}'
            }})
            
            CALL {{
              WITH p
              MATCH (p)-[:PARENT_OF]->(c)
              WHERE NOT c:DECLARE AND NOT c:Table AND NOT c:SPEC
                AND c.token < 1000
              WITH c, labels(c) AS cLabels, coalesce(toInteger(c.startLine), 0) AS sortKey
              RETURN c AS n, cLabels AS nodeLabels, NULL AS r, NULL AS m, sortKey
              
              UNION ALL
              
              WITH p
              MATCH (p)-[:PARENT_OF]->(c)
              WHERE NOT c:DECLARE AND NOT c:Table AND NOT c:SPEC
                AND coalesce(toInteger(c.token), 0) >= 1000
              WITH c
              MATCH path = (c)-[:PARENT_OF*0..]->(n)
              WHERE NOT n:DECLARE AND NOT n:Table AND NOT n:SPEC
              WITH n, path, nodes(path) AS pathNodes
              WHERE ALL(i IN range(0, size(pathNodes)-2) 
                        WHERE coalesce(toInteger(pathNodes[i].token), 0) >= 1000)
              OPTIONAL MATCH (n)-[r]->(m {{
                folder_name: '{folder_name}', file_name: '{file_name}', user_id: '{user_id}'
              }})
              WHERE r IS NULL
                 OR ( NOT (m:DECLARE OR m:Table OR m:SPEC)
                      AND none(x IN ['CALL','WRITES','FROM'] WHERE type(r) CONTAINS x) )
              WITH n, labels(n) AS nLabels, r, m, coalesce(toInteger(n.startLine), 0) AS sortKey
              RETURN DISTINCT n, nLabels AS nodeLabels, r, m, sortKey
            }}
            
            RETURN n, nodeLabels, r, m
            ORDER BY sortKey, coalesce(toInteger(n.token), 0), id(n)
            """
        ])
        dbms_nodes = query_results[0] if query_results else []

        # ìŠ¤ì¼ˆë ˆí†¤ ìƒì„±
        skeleton_code = await start_dbms_skeleton(
            folder_name=folder_name,
            file_name=file_name,
            procedure_name=procedure_name,
            project_name=project_name,
            user_id=user_id,
            api_key=api_key,
            locale=locale,
            target_dbms=target_dbms
        )

        # ë³€í™˜ ìˆ˜í–‰
        generator = DbmsConversionGenerator(
            dbms_nodes,
            folder_name,
            file_name,
            procedure_name,
            user_id,
            api_key,
            locale,
            project_name,
            target_dbms,
            skeleton_code
        )

        await generator.generate()
        
        # ë³€í™˜ ë…¸ë“œ ì¿¼ë¦¬ë“¤ì„ Neo4jì— í•œë²ˆì— ì €ì¥
        if generator.conversion_queries:
            await connection.execute_queries(generator.conversion_queries)
            logging.info(f"âœ… ë³€í™˜ ë…¸ë“œ ì €ì¥ ì™„ë£Œ: CONVERTING 1ê°œ, BLOCK {len(generator.conversion_queries)-1}ê°œ")
        
        # íŒŒì¼ ì €ì¥
        base_name = file_name.rsplit(".", 1)[0]
        converted_code = await generator._save_target_file(base_name)

        logging.info("\n" + "-"*80)
        logging.info(f"âœ… DBMS ë³€í™˜ ì™„ë£Œ: {base_name}")
        logging.info("-"*80 + "\n")
        
        return converted_code

    except ConvertingError:
        raise
    except Exception as e:
        err_msg = f"DBMS ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        logging.error(err_msg)
        raise ConvertingError(err_msg)
    finally:
        await connection.close()

