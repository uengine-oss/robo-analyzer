import asyncio
import logging
import os
import re
import textwrap
from collections import defaultdict, deque
from dataclasses import dataclass, field
from typing import Callable
from understand.neo4j_connection import Neo4jConnection
from util.exception import ConvertingError
from util.utility_tool import (
    build_rule_based_path, save_file, log_process
)
from util.rule_loader import RuleLoader
from convert.dbms.create_dbms_skeleton import start_dbms_skeleton
from convert.dbms.parentheses_repair import (
    validate_and_repair_sql,
    RepairContext,
    has_parentheses_mismatch,
    count_parentheses
)


# ----- ìƒìˆ˜ ì •ì˜ -----
TOKEN_THRESHOLD = int(os.getenv('DBMS_TOKEN_THRESHOLD', '1000'))
CODE_PLACEHOLDER = "...code..."

ENABLE_PARENTHESES_VALIDATION = False # ê´„í˜¸ ê²€ì¦ í™œì„±í™” ì—¬ë¶€ (True: í™œì„±í™”, False: ë¹„í™œì„±í™”)

DML_PLACEHOLDER_PATTERN = re.compile(
    r'(?P<indent>^[ \t]*)(?P<label>(?P<start>\d+)):\s*\.\.\.\s*code\s*\.\.\.',
    re.IGNORECASE | re.MULTILINE
)
DML_TYPES = frozenset(["SELECT", "INSERT", "UPDATE", "DELETE", "FETCH", "MERGE", "JOIN", "ALL_UNION", "UNION", "FOR"])
MAX_CONVERSION_CONCURRENCY = int(os.getenv('DBMS_MAX_CONCURRENCY', '5'))


@dataclass(slots=True)
class ChildFragment:
    sequence: int
    code: str
    start: int | None
    end: int | None


@dataclass(slots=True)
class ParentEntry:
    start: int
    end: int
    code: str
    is_dml: bool
    parent: "ParentEntry | None"
    sequence: int
    is_root: bool = False
    children: list[ChildFragment] = field(default_factory=list)
    pending_children: int = 0
    closed: bool = False
    finalized: bool = False


@dataclass(slots=True)
class ConversionWorkItem:
    work_id: int
    sequence: int
    code: str
    start: int
    end: int
    parent: ParentEntry | None
    parent_code: str
    token_count: int


class SpAccumulator:
    """ë¦¬í”„/ì†Œí˜• ë…¸ë“œ ì½”ë“œë¥¼ ì„ê³„ê°’ê¹Œì§€ ëˆ„ì í•˜ëŠ” ë²„í¼."""

    __slots__ = ('parts', 'token_total', 'start_line', 'end_line')

    def __init__(self) -> None:
        self.clear()

    def clear(self) -> None:
        self.parts: list[str] = []
        self.token_total = 0
        self.start_line: int | None = None
        self.end_line: int | None = None

    def append(self, code: str, token: int, start_line: int, end_line: int) -> bool:
        snippet = (code or '').strip()
        if not snippet:
            return False
        self.parts.append(snippet)
        self.token_total += token
        if self.start_line is None or start_line < self.start_line:
            self.start_line = start_line
        if self.end_line is None or end_line > self.end_line:
            self.end_line = end_line
        return True

    def has_data(self) -> bool:
        return bool(self.parts)

    def should_flush_with(self, incoming_token: int | None, token_limit: int) -> bool:
        if not self.parts or incoming_token is None:
            return False
        return (self.token_total + incoming_token) >= token_limit

    def part_count(self) -> int:
        return len(self.parts)

    def consume(self) -> tuple[str, int | None, int | None, int, int]:
        if not self.parts:
            return "", None, None, 0, 0
        code = '\n'.join(self.parts)
        start = self.start_line
        end = self.end_line
        tokens = self.token_total
        part_count = len(self.parts)
        self.clear()
        return code, start, end, tokens, part_count


class ConversionWorkQueue:
    """LLM í˜¸ì¶œì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì‘ì—… í."""

    __slots__ = ('rule_loader', 'api_key', 'locale', 'max_workers', 'items', 'enable_parentheses_validation')

    def __init__(self, rule_loader: RuleLoader, api_key: str, locale: str, max_workers: int,
                 enable_parentheses_validation: bool = True) -> None:
        self.rule_loader = rule_loader
        self.api_key = api_key
        self.locale = locale
        self.max_workers = max(1, max_workers)
        self.items: list[ConversionWorkItem] = []
        self.enable_parentheses_validation = enable_parentheses_validation

    def reset(self) -> None:
        self.items.clear()

    def enqueue(self, item: ConversionWorkItem) -> None:
        self.items.append(item)

    def queued_count(self) -> int:
        return len(self.items)

    async def drain(self, completion_handler: Callable[[ConversionWorkItem, str], None]) -> None:
        if not self.items:
            return

        semaphore = asyncio.Semaphore(self.max_workers)

        async def worker(item: ConversionWorkItem) -> None:
            async with semaphore:
                result = await asyncio.to_thread(
                    self.rule_loader.execute,
                    role_name='dbms_conversion',
                    inputs={
                        'code': item.code,
                        'locale': self.locale,
                        'parent_code': item.parent_code
                    },
                    api_key=self.api_key
                )
            generated_code = (result.get('code') or '').strip()
            
            # ê´„í˜¸ ê²€ì¦ ë° ë³µêµ¬ (DBMS ë³€í™˜ì—ë§Œ ì ìš©)
            if self.enable_parentheses_validation and generated_code:
                generated_code = await self._validate_parentheses(item, generated_code)
            
            completion_handler(item, generated_code)

        await asyncio.gather(*(worker(item) for item in self.items))
        self.items.clear()
    
    async def _validate_parentheses(self, item: ConversionWorkItem, generated_code: str) -> str:
        """ë³€í™˜ëœ ì½”ë“œì˜ ê´„í˜¸ ê²€ì¦ ë° í•„ìš”ì‹œ ë³µêµ¬"""
        # ê´„í˜¸ ë¶ˆì¼ì¹˜ê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if not has_parentheses_mismatch(generated_code):
            return generated_code
        
        # ë³µêµ¬ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = RepairContext(
            work_id=item.work_id,
            start_line=item.start,
            end_line=item.end,
            node_type="CONVERSION",
            parent_context=f"Parent: {item.parent.start if item.parent else 'ROOT'}~{item.parent.end if item.parent else 'ROOT'}"
        )
        
        open_count, close_count = count_parentheses(generated_code)
        log_process(
            "DBMS",
            "VALIDATE",
            f"âš ï¸ ì‘ì—… #{item.work_id} ({item.start}~{item.end}) ê´„í˜¸ ë¶ˆì¼ì¹˜ ê°ì§€: "
            f"ì—¬ëŠ” ê´„í˜¸ {open_count}, ë‹«ëŠ” ê´„í˜¸ {close_count} - ë³µêµ¬ ì‹œë„",
            logging.WARNING
        )
        
        # ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
        repaired = await asyncio.to_thread(
            validate_and_repair_sql,
            self.rule_loader,
            self.api_key,
            self.locale,
            generated_code,
            context
        )
        
        # ë³µêµ¬ ê²°ê³¼ ë¡œê¹…
        if not has_parentheses_mismatch(repaired):
            log_process(
                "DBMS",
                "VALIDATE",
                f"âœ… ì‘ì—… #{item.work_id} ê´„í˜¸ ë³µêµ¬ ì„±ê³µ"
            )
        else:
            open_r, close_r = count_parentheses(repaired)
            log_process(
                "DBMS",
                "VALIDATE",
                f"âš ï¸ ì‘ì—… #{item.work_id} ê´„í˜¸ ë³µêµ¬ ì‹¤íŒ¨ (ìµœì„ ì˜ ê²°ê³¼ ì‚¬ìš©): "
                f"ì—¬ëŠ” ê´„í˜¸ {open_r}, ë‹«ëŠ” ê´„í˜¸ {close_r}",
                logging.WARNING
            )
        
        return repaired


# ----- DBMS ë³€í™˜ í´ë˜ìŠ¤ -----
class DbmsConversionGenerator:
    """
    DBMS ë³€í™˜ ì „ì²´ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬
    - ë‹¨ì¼ ì»¨í…ìŠ¤íŠ¸ ëˆ„ì  ë°©ì‹ìœ¼ë¡œ íƒ€ê²Ÿ DBMS ì½”ë“œ ìƒì„±
    - ëŒ€ìš©ëŸ‰ ë¶€ëª¨(í† í°â‰¥1000, ìì‹ ë³´ìœ ) ìŠ¤ì¼ˆë ˆí†¤ ê´€ë¦¬
    - í† í° ì„ê³„ ë„ë‹¬ ì‹œ LLM ë¶„ì„ ìˆ˜í–‰
    """
    __slots__ = (
        'traverse_nodes', 'folder_name', 'file_name', 'procedure_name',
        'user_id', 'api_key', 'locale', 'project_name', 'target_dbms', 'skeleton_code',
        'merged_chunks', 'parent_stack',
        'rule_loader', 'sequence_counter',
        'work_id_counter', 'max_workers', 'root_entry',
        'sp_accumulator', 'work_queue'
    )

    def __init__(self, traverse_nodes: list, folder_name: str, file_name: str,
                 procedure_name: str, user_id: str, api_key: str, locale: str, 
                 project_name: str = "demo", target_dbms: str = "oracle",
                 skeleton_code: str | None = None):
        self.traverse_nodes = traverse_nodes
        self.folder_name = folder_name
        self.file_name = file_name
        self.procedure_name = procedure_name
        self.user_id = user_id
        self.api_key = api_key
        self.locale = locale
        self.project_name = project_name or "demo"
        self.target_dbms = target_dbms
        self.skeleton_code = (skeleton_code or "").strip()

        # ìƒíƒœ ì´ˆê¸°í™”
        self.merged_chunks = []
        self.parent_stack = []
        
        # Rule íŒŒì¼ ë¡œë” (target_dbmsë¡œ ë””ë ‰í† ë¦¬ ì°¾ìŒ)
        self.rule_loader = RuleLoader(target_lang=target_dbms)
        self.sequence_counter = 0
        self.work_id_counter = 0
        self.max_workers = MAX_CONVERSION_CONCURRENCY
        self.root_entry: ParentEntry | None = None
        self.sp_accumulator = SpAccumulator()
        self.work_queue = ConversionWorkQueue(
            self.rule_loader, self.api_key, self.locale, self.max_workers,
            enable_parentheses_validation=ENABLE_PARENTHESES_VALIDATION
        )

    # ----- ê³µê°œ ë©”ì„œë“œ -----

    @staticmethod
    def _resolve_node_type(node_labels: list | None, node: dict) -> str:
        raw_type = node_labels[0] if node_labels else node.get('name', 'UNKNOWN')
        raw_type = str(raw_type)
        return raw_type.split('[')[0] if '[' in raw_type else raw_type

    @staticmethod
    def _safe_int(value) -> int:
        try:
            return int(value)
        except (TypeError, ValueError):
            return 0

    def _ensure_root_entry(self) -> None:
        if self.root_entry is not None:
            return
        self.root_entry = ParentEntry(
            start=0,
            end=float('inf'),
            code="",
            is_dml=False,
            parent=None,
            sequence=0,
            is_root=True
        )

    def _reset_state(self) -> None:
        self._ensure_root_entry()
        if self.root_entry:
            self.root_entry.children.clear()
            self.root_entry.pending_children = 0
            self.root_entry.closed = False
            self.root_entry.finalized = False
        self.parent_stack.clear()
        self.sp_accumulator.clear()
        self.merged_chunks = []
        self.sequence_counter = 0
        self.work_id_counter = 0
        self.work_queue.reset()

    def _next_sequence(self) -> int:
        self.sequence_counter += 1
        return self.sequence_counter

    def _next_work_id(self) -> int:
        self.work_id_counter += 1
        return self.work_id_counter

    @staticmethod
    def _register_pending_child(parent_entry: ParentEntry | None) -> None:
        if parent_entry is None:
            return
        parent_entry.pending_children += 1

    def _record_child_fragment(
        self,
        parent_entry: ParentEntry | None,
        code: str,
        start_line: int | None,
        end_line: int | None,
        sequence: int
    ) -> None:
        if parent_entry is None:
            if code and code.strip():
                self.merged_chunks.append(code)
            else:
                log_process("DBMS", "CONVERT", f"âš ï¸ ë£¨íŠ¸ êµ¬ê°„ {start_line}~{end_line}ì— ë¹ˆ ë³€í™˜ ê²°ê³¼ ë°˜í™˜ - ìµœì¢… ì½”ë“œì—ì„œ ì œì™¸", logging.WARNING)
            return

        if code and code.strip():
            fragment = ChildFragment(
                sequence=sequence,
                code=code,
                start=start_line,
                end=end_line
            )
            parent_entry.children.append(fragment)
        else:
            log_process("DBMS", "CONVERT", f"âš ï¸ ìì‹ êµ¬ê°„ {start_line}~{end_line}ì— ë¹ˆ ë³€í™˜ ê²°ê³¼ ë°˜í™˜ - ë¶€ëª¨ {parent_entry.start}~{parent_entry.end}ì— ì ìš©í•  ì½”ë“œ ì—†ìŒ", logging.WARNING)

        if parent_entry.pending_children > 0:
            parent_entry.pending_children -= 1
        else:
            log_process("DBMS", "CONVERT", f"âš ï¸ ë¶€ëª¨ {parent_entry.start}~{parent_entry.end}ì˜ ë¯¸ì²˜ë¦¬ ìì‹ ìˆ˜ê°€ ìŒìˆ˜ - Neo4j ë°ì´í„° í™•ì¸ í•„ìš”", logging.WARNING)

        self._try_finalize_parent(parent_entry)

    def _try_finalize_parent(self, entry: ParentEntry | None) -> None:
        if entry is None or entry.finalized or not entry.closed or entry.pending_children > 0:
            return

        if entry.is_root:
            ordered = [
                fragment.code
                for fragment in sorted(entry.children, key=lambda frag: frag.sequence)
                if fragment.code and fragment.code.strip()
            ]
            self.merged_chunks = ordered
            entry.children.clear()
            entry.finalized = True
            log_process("DBMS", "CONVERT", f"ğŸ‰ ëª¨ë“  ë³€í™˜ ì™„ë£Œ: ë£¨íŠ¸ì— {len(ordered)}ê°œ ì½”ë“œ ë¸”ë¡ ë³‘í•©í•˜ì—¬ ìµœì¢… ë³¸ë¬¸ êµ¬ì„±")
            return

        children = sorted(entry.children, key=lambda frag: frag.sequence)
        merged_code = entry.code
        if entry.is_dml:
            merged_code = self._merge_dml_children(merged_code, children)
        else:
            merged_code = self._merge_regular_children(merged_code, children)

        merged_code = merged_code.strip()
        entry.finalized = True
        entry.children.clear()
        self._record_child_fragment(entry.parent, merged_code, entry.start, entry.end, entry.sequence)

    async def _process_work_queue(self) -> None:
        await self.work_queue.drain(self._handle_work_completion)

    def _handle_work_completion(self, item: ConversionWorkItem, generated_code: str) -> None:
        parent_entry = item.parent
        if parent_entry is None:
            return
        code_len = len(generated_code) if generated_code else 0
        log_process(
            "DBMS",
            "CONVERT",
            f"âœ… ë³€í™˜ ì™„ë£Œ (work #{item.work_id}): ìì‹ {item.start}~{item.end} ê²°ê³¼({code_len}ì)ë¥¼ ë¶€ëª¨ {parent_entry.start}~{parent_entry.end}ì— ë°˜ì˜"
        )
        self._record_child_fragment(parent_entry, generated_code, item.start, item.end, item.sequence)

    async def generate(self) -> str:
        """
        ì „ì²´ ë…¸ë“œë¥¼ ìˆœíšŒí•˜ë©° íƒ€ê²Ÿ DBMS ì½”ë“œ ìƒì„±
        
        Returns:
            str: ìµœì¢… ë³‘í•©ëœ ì½”ë“œ
        """
        log_process("DBMS", "START", f"ğŸš€ DBMS ë³€í™˜ ì‹œì‘: {self.folder_name}/{self.file_name} (Postgres â†’ {self.target_dbms.upper()})")
        self._reset_state()

        # ì¤‘ë³µ ì œê±°: ê°™ì€ ë¼ì¸ ë²”ìœ„ëŠ” í•œ ë²ˆë§Œ ì²˜ë¦¬
        seen_nodes = set()
        node_count = 0
        for record in self.traverse_nodes:
            node = record['n']
            start_line = self._safe_int(node.get('startLine'))
            end_line = self._safe_int(node.get('endLine'))

            node_key = (start_line, end_line)
            if node_key in seen_nodes:
                continue
            seen_nodes.add(node_key)
            node_count += 1
            await self._process_node(record)

        await self._finalize_remaining()

        if self.root_entry:
            self.root_entry.closed = True
            self._try_finalize_parent(self.root_entry)

        await self._process_work_queue()

        if self.root_entry and not self.root_entry.finalized:
            self._try_finalize_parent(self.root_entry)
        if self.root_entry and self.root_entry.pending_children:
            log_process("DBMS", "CONVERT", f"âš ï¸ ë£¨íŠ¸ì— ì•„ì§ {self.root_entry.pending_children}ê°œ ìì‹ ì½”ë“œ ë¯¸ì²˜ë¦¬ - Neo4j ë°ì´í„° ëˆ„ë½ ì—¬ë¶€ í™•ì¸ í•„ìš”", logging.WARNING)

        log_process("DBMS", "DONE", f"âœ… ë³€í™˜ ì™„ë£Œ: ì´ {node_count}ê°œ ë…¸ë“œ ì²˜ë¦¬")
        return self._final_output()

    # ----- ë…¸ë“œ ì²˜ë¦¬ -----

    async def _process_node(self, record: dict) -> None:
        """ë‹¨ì¼ ë…¸ë“œ ì²˜ë¦¬"""
        node = record['n']
        node_labels = record.get('nodeLabels', [])
        node_type = self._resolve_node_type(node_labels, node)
        has_children = bool(node.get('has_children', False))
        token = self._safe_int(node.get('token'))
        start_line = self._safe_int(node.get('startLine'))
        end_line = self._safe_int(node.get('endLine'))
        relationship = record['r'][1] if record.get('r') else 'NEXT'

        # ë…¸ë“œ ì²˜ë¦¬ ë¡œê·¸
        node_kind = "ë¶€ëª¨" if has_children else "ë¦¬í”„"
        stack_info = f", ë¶€ëª¨ ìŠ¤íƒ ê¹Šì´ {len(self.parent_stack)}" if self.parent_stack else ""
        log_process("DBMS", "LEAF" if not has_children else "PARENT", f"ğŸ” {node_type} ({start_line}~{end_line}) {node_kind} ë…¸ë“œ ë¶„ì„ ì¤‘ - í† í° {token}{stack_info}")

        # ë¶€ëª¨ ê²½ê³„ ì²´í¬
        while self.parent_stack and start_line > self.parent_stack[-1].end:
            if self.sp_accumulator.has_data():
                await self._analyze_and_merge()
            await self._finalize_parent()

        # ë…¸ë“œ íƒ€ì…ë³„ ì²˜ë¦¬
        is_large_parent = token >= TOKEN_THRESHOLD and has_children
        is_large_leaf = token >= TOKEN_THRESHOLD and not has_children

        if is_large_parent:
            # í° ë…¸ë“œ ì²˜ë¦¬ ì „ì— ìŒ“ì¸ ì‘ì€ ë…¸ë“œë“¤ ë¨¼ì € ë³€í™˜
            if self.sp_accumulator.has_data():
                await self._analyze_and_merge()
            
            log_process("DBMS", "PARENT", f"ğŸ—ï¸ ëŒ€ìš©ëŸ‰ ë¶€ëª¨ ë…¸ë“œ ë°œê²¬: {node_type} ({start_line}~{end_line}, í† í° {token}) - ìŠ¤ì¼ˆë ˆí†¤ ìƒì„± í›„ ë¶€ëª¨ ìŠ¤íƒì— ì¶”ê°€ (í˜„ì¬ ê¹Šì´ {len(self.parent_stack)})")
            await self._handle_large_node(node, node_labels, start_line, end_line, token)
        else:
            appended = False
            if is_large_leaf:
                if self.sp_accumulator.has_data():
                    await self._analyze_and_merge()
            else:
                await self._flush_pending_accumulation(token)

            appended = self._handle_small_node(node, node_type, start_line, end_line, token)

            if appended and self._is_within_dml_parent():
                await self._analyze_and_merge()

        # ì„ê³„ê°’ ì²´í¬
        if is_large_leaf:
            log_process("DBMS", "CONVERT", f"âš¡ ë‹¨ë… ëŒ€ìš©ëŸ‰ ë¦¬í”„ ë…¸ë“œ ì¦‰ì‹œ ë³€í™˜: {node_type} ({start_line}~{end_line}, í† í° {token})")
            await self._analyze_and_merge()
        elif self.sp_accumulator.token_total >= TOKEN_THRESHOLD:
            log_process("DBMS", "CONVERT", f"ğŸ“Š í† í° ì„ê³„ê°’ ë„ë‹¬: ëˆ„ì  í† í° {self.sp_accumulator.token_total} â‰¥ {TOKEN_THRESHOLD} - ì§€ê¸ˆê¹Œì§€ ëª¨ì€ êµ¬ê°„ì„ ë³€í™˜í•©ë‹ˆë‹¤")
            await self._analyze_and_merge()

    # ----- ëŒ€ìš©ëŸ‰ ë…¸ë“œ ì²˜ë¦¬ -----

    async def _handle_large_node(
        self,
        node: dict,
        node_labels: list,
        start_line: int,
        end_line: int,
        token: int
    ) -> None:
        """ëŒ€ìš©ëŸ‰ ë…¸ë“œ(ìì‹ ìˆìŒ, í† í°â‰¥1000) ì²˜ë¦¬"""
        summarized = (node.get('summarized_code') or '').strip()
        if not summarized:
            log_process("DBMS", "PARENT", f"âš ï¸ {start_line}~{end_line} êµ¬ê°„ì— ìš”ì•½ ì½”ë“œê°€ ì—†ì–´ ìŠ¤ì¼ˆë ˆí†¤ ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤", logging.WARNING)
            return

        node_type = self._resolve_node_type(node_labels, node)
        is_dml_node = str(node_type).upper() in DML_TYPES

        # LLMìœ¼ë¡œ ìŠ¤ì¼ˆë ˆí†¤ ìƒì„± (Rule íŒŒì¼ ì‚¬ìš©)
        result = self.rule_loader.execute(
            role_name='dbms_summarized_dml' if is_dml_node else 'dbms_summarized',
            inputs={
                'summarized_code': summarized,
                'locale': self.locale
            },
            api_key=self.api_key
        )
        skeleton = result['code']

        parent_entry = self.parent_stack[-1] if self.parent_stack else self.root_entry
        entry = ParentEntry(
            start=start_line,
            end=end_line,
            code=skeleton,
            is_dml=is_dml_node,
            parent=parent_entry,
            sequence=self._next_sequence()
        )
        if parent_entry is not None:
            self._register_pending_child(parent_entry)
        self.parent_stack.append(entry)
        dml_info = " (DML)" if is_dml_node else ""
        log_process("DBMS", "PARENT", f"âœ… {node_type} ({entry.start}~{entry.end}) ìŠ¤ì¼ˆë ˆí†¤ ìƒì„± ì™„ë£Œ{dml_info} - ë¶€ëª¨ ìŠ¤íƒ ê¹Šì´ {len(self.parent_stack)}")

    # ----- ì†Œí˜• ë…¸ë“œ ì²˜ë¦¬ -----

    def _handle_small_node(self, node: dict, node_type: str, start_line: int, end_line: int, token: int) -> bool:
        """ì†Œí˜• ë…¸ë“œ ë˜ëŠ” ë¦¬í”„ ë…¸ë“œ ì²˜ë¦¬"""
        node_code = (node.get('node_code') or '').strip()
        if not node_code:
            return False

        appended = self.sp_accumulator.append(node_code, token, start_line, end_line)
        if not appended:
            return False

        # ë¡œê·¸ëŠ” ìµœì†Œí™” - ëˆ„ì  ì •ë³´ë§Œ ê°„ë‹¨íˆ í‘œì‹œ
        return True

    async def _flush_pending_accumulation(self, incoming_token: int) -> None:
        """ë‹¤ìŒ ë…¸ë“œ ì¶”ê°€ ì „ì— ì„ê³„ê°’ ì´ˆê³¼ ì—¬ë¶€ í™•ì¸"""
        if self.sp_accumulator.should_flush_with(incoming_token, TOKEN_THRESHOLD):
            current_tokens = self.sp_accumulator.token_total
            next_total = current_tokens + (incoming_token or 0)
            log_process("DBMS", "CONVERT", f"ğŸ“Š ë‹¤ìŒ ë…¸ë“œ ì¶”ê°€ ì‹œ í† í° ì´ˆê³¼ ì˜ˆìƒ: í˜„ì¬ {current_tokens} + ë‹¤ìŒ {incoming_token} = {next_total} â‰¥ {TOKEN_THRESHOLD} - ê¸°ì¡´ ëˆ„ì ë¶„ ë¨¼ì € ë³€í™˜")
            await self._analyze_and_merge()

    # ----- ë¶€ëª¨ ê´€ë¦¬ -----

    async def _finalize_parent(self) -> None:
        """í˜„ì¬ ë¶€ëª¨ ë§ˆë¬´ë¦¬"""
        if not self.parent_stack:
            return

        entry = self.parent_stack.pop()
        log_process("DBMS", "PARENT", f"ğŸ”š ë¶€ëª¨ ë…¸ë“œ ì²˜ë¦¬ ì™„ë£Œ: {entry.start}~{entry.end} (ìì‹ {len(entry.children)}ê°œ ë³‘í•© ì˜ˆì •, ìŠ¤íƒ ê¹Šì´ {len(self.parent_stack)})")
        entry.closed = True
        self._try_finalize_parent(entry)

    # ----- ë¶„ì„ ë° ë³‘í•© -----

    async def _analyze_and_merge(self) -> None:
        """LLM ë¶„ì„ ë° íƒ€ê²Ÿ DBMS ì½”ë“œ ë³‘í•©"""
        if not self.sp_accumulator.has_data():
            return

        sp_code, child_start, child_end, token_total, part_count = self.sp_accumulator.consume()
        parent_entry = self.parent_stack[-1] if self.parent_stack else self.root_entry
        
        # ë¶€ëª¨ ì •ë³´ êµ¬ì„±
        if parent_entry and not parent_entry.is_root:
            parent_info = f"ë¶€ëª¨ {parent_entry.start}~{parent_entry.end}"
            target = "ë¶€ëª¨ children"
        else:
            parent_info = "ë£¨íŠ¸"
            target = "ìµœì¢… ì½”ë“œ"
        
        log_process("DBMS", "CONVERT", f"ğŸš€ ë³€í™˜ ì‹œì‘: ë¼ì¸ {child_start}~{child_end} ({part_count}ê°œ ì¡°ê°, {token_total} í† í°) â†’ {target} ({parent_info})")

        parent_code = parent_entry.code if parent_entry else ""
        if parent_entry:
            self._register_pending_child(parent_entry)
        child_start = child_start if child_start is not None else 0
        child_end = child_end if child_end is not None else child_start
        work_item = ConversionWorkItem(
            work_id=self._next_work_id(),
            sequence=self._next_sequence(),
            code=sp_code,
            start=child_start,
            end=child_end,
            parent=parent_entry,
            parent_code=parent_code,
            token_count=token_total
        )
        self.work_queue.enqueue(work_item)

    def _final_output(self) -> str:
        """ëˆ„ì ëœ ìµœìƒìœ„ ì½”ë“œë¥¼ ë‹¨ì¼ ë¬¸ìì—´ë¡œ ë°˜í™˜"""
        return "\n".join(self.merged_chunks).strip()

    def _merge_regular_children(self, code: str, children: list[ChildFragment]) -> str:
        """ë¹„-DML ë¶€ëª¨ placeholder ì²˜ë¦¬"""
        ordered_children = [
            fragment.code for fragment in children or []
            if fragment.code and fragment.code.strip()
        ]
        child_block = "\n".join(ordered_children).strip()

        if CODE_PLACEHOLDER in code:
            if child_block:
                indented = textwrap.indent(child_block, '    ')
                return code.replace(CODE_PLACEHOLDER, f"\n{indented}\n", 1)
            return code.replace(CODE_PLACEHOLDER, "", 1)

        if not child_block:
            return code

        indented = textwrap.indent(child_block, '    ')
        return f"{code}\n{indented}"

    def _merge_dml_children(self, code: str, children: list[ChildFragment]) -> str:
        """DML ìŠ¤ì¼ˆë ˆí†¤ placeholderì— ìì‹ ì½”ë“œë¥¼ ì£¼ì…"""
        children_by_start: dict[int, deque] = defaultdict(deque)
        fallback_children: deque = deque()

        for fragment in children or []:
            payload = {
                'code': fragment.code,
                'start': fragment.start,
                'end': fragment.end
            }

            raw_start = payload.get('start')
            try:
                start_line = int(raw_start) if raw_start is not None else None
            except (TypeError, ValueError):
                start_line = None
            if start_line is None:
                fallback_children.append(payload)
            else:
                children_by_start[start_line].append(payload)

        placeholders = list(DML_PLACEHOLDER_PATTERN.finditer(code))
        placeholder_starts = [int(match.group('start')) for match in placeholders]
        total_children = sum(len(queue) for queue in children_by_start.values()) + len(fallback_children)
        log_process("DBMS", "PARENT", f"ğŸ”— DML ë³‘í•© ì‹œì‘: {len(placeholders)}ê°œ placeholder({placeholder_starts})ì— {total_children}ê°œ ìì‹ ì½”ë“œ ë§¤í•‘")

        def _replacement(match: re.Match) -> str:
            indent = match.group('indent') or ''
            start = int(match.group('start'))
            label = match.group('label')

            queue = children_by_start.get(start)
            child_queue = children_by_start.get(start)
            child = child_queue.popleft() if child_queue else None
            if child_queue is not None and not child_queue:
                children_by_start.pop(start, None)

            if not child:
                remaining_starts = sorted(children_by_start.keys()) or ['ì—†ìŒ']
                remaining_count = sum(len(queue) for queue in children_by_start.values()) + len(fallback_children)
                log_process("DBMS", "PARENT", f"âš ï¸ DML placeholder {label} (ë¼ì¸ {start})ì™€ ë§¤ì¹­ë˜ëŠ” ìì‹ ì—†ìŒ - ë‚¨ì€ í›„ë³´: {remaining_starts} (ì´ {remaining_count}ê°œ)", logging.WARNING)
                return match.group(0)

            child_code = (child.get('code') or '').strip()
            if not child_code:
                log_process("DBMS", "PARENT", f"âš ï¸ DML placeholder {label}ì— ì—°ê²°ëœ {child.get('start')}~{child.get('end')} ì½”ë“œê°€ ë¹„ì–´ìˆìŒ - placeholder ìœ ì§€", logging.WARNING)
                return match.group(0)

            return textwrap.indent(child_code, indent)

        merged_code = DML_PLACEHOLDER_PATTERN.sub(_replacement, code)

        residual_entries: list[dict] = []
        for start_line in sorted(children_by_start.keys()):
            residual_entries.extend(children_by_start[start_line])
        residual_entries.extend(fallback_children)

        if residual_entries:
            residual = "\n".join(
                (entry.get('code') or '').strip()
                for entry in residual_entries
                if entry.get('code')
            ).strip()
            if residual:
                merged_code = f"{merged_code.rstrip()}\n{residual}"
                log_process("DBMS", "PARENT", f"âš ï¸ DML placeholderë³´ë‹¤ ìì‹ {len(residual_entries)}ê°œê°€ ë§ì•„ í•˜ë‹¨ì— residual ë¸”ë¡ìœ¼ë¡œ ì¶”ê°€", logging.WARNING)
            else:
                log_process("DBMS", "PARENT", f"âš ï¸ DML placeholderë³´ë‹¤ ìì‹ {len(residual_entries)}ê°œê°€ ë‚¨ì•˜ì§€ë§Œ ëª¨ë‘ ë¹ˆ ë¬¸ìì—´ì´ë¼ ì œì™¸", logging.WARNING)

        return merged_code

    def _is_within_dml_parent(self) -> bool:
        """í˜„ì¬ ìŠ¤íƒ ìµœìƒë‹¨ì´ DML ë¶€ëª¨ì¸ì§€ í™•ì¸"""
        return bool(self.parent_stack and self.parent_stack[-1].is_dml)

    # ----- ë§ˆë¬´ë¦¬ -----

    async def _finalize_remaining(self) -> None:
        """ë‚¨ì€ ë°ì´í„° ì •ë¦¬"""
        if self.parent_stack:
            if self.sp_accumulator.has_data():
                await self._analyze_and_merge()
            while self.parent_stack:
                await self._finalize_parent()
        elif self.sp_accumulator.has_data():
            await self._analyze_and_merge()

    async def _save_target_file(self, base_name: str) -> str:
        """íƒ€ê²Ÿ DBMS íŒŒì¼ ìë™ ì €ì¥"""
        try:
            # ì €ì¥ ê²½ë¡œ ì„¤ì •
            base_path = build_rule_based_path(
                self.project_name,
                self.user_id,
                self.target_dbms,
                'dbms_conversion',
                folder_name=self.folder_name
            )
            
            body_code = self._final_output().strip()
            header_code = self.skeleton_code.strip()

            parts = [part for part in [header_code, body_code] if part]
            final_code = "\n\n".join(parts).rstrip() + "\n"

            # íŒŒì¼ ì €ì¥
            await save_file(
                content=final_code,
                filename=f"{base_name}.sql",
                base_path=base_path
            )
            
            log_process("DBMS", "SAVE", f"ğŸ’¾ {self.target_dbms.capitalize()} íŒŒì¼ ì €ì¥ ì™„ë£Œ: {base_path}/{base_name}.sql")
            
            return final_code
            
        except Exception as e:
            log_process("DBMS", "ERROR", f"âŒ {self.target_dbms.capitalize()} íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}", logging.ERROR, e)
            raise ConvertingError(f"{self.target_dbms.capitalize()} íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")


# ----- ì§„ì…ì  í•¨ìˆ˜ -----
async def start_dbms_conversion(
    folder_name: str,
    file_name: str,
    procedure_name: str,
    project_name: str,
    user_id: str,
    api_key: str,
    locale: str,
    target_dbms: str = "oracle"
) -> str:
    """
    DBMS ë³€í™˜ ì‹œì‘
    
    Args:
        folder_name: í´ë”ëª…
        file_name: íŒŒì¼ëª…
        procedure_name: í”„ë¡œì‹œì € ì´ë¦„
        project_name: í”„ë¡œì íŠ¸ ì´ë¦„
        user_id: ì‚¬ìš©ì ID
        api_key: LLM API í‚¤
        locale: ë¡œì¼€ì¼
        target_dbms: íƒ€ê²Ÿ DBMS (oracle ë“±)
    
    Returns:
        str: ë³€í™˜ëœ ì½”ë“œ
    
    Raises:
        ConvertingError: ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ
    """
    connection = Neo4jConnection()
    
    log_process("DBMS", "START", f"ğŸš€ DBMS ë³€í™˜ ì¤€ë¹„: {folder_name}/{file_name} (Postgres â†’ {target_dbms.upper()})")

    try:
        # Neo4j ì¿¼ë¦¬
        query_results = await connection.execute_queries([
            f"""
            MATCH (p:PROCEDURE {{
              folder_name: '{folder_name}',
              file_name: '{file_name}',
              procedure_name: '{procedure_name}',
              user_id: '{user_id}'
            }})
            
            CALL {{
              WITH p
              MATCH (p)-[:PARENT_OF]->(c)
              WHERE NOT c:DECLARE AND NOT c:Table AND NOT c:SPEC
                AND c.token < 1000
              WITH c, labels(c) AS cLabels, coalesce(toInteger(c.startLine), 0) AS sortKey
              RETURN c AS n, cLabels AS nodeLabels, NULL AS r, NULL AS m, sortKey
              
              UNION ALL
              
              WITH p
              MATCH (p)-[:PARENT_OF]->(c)
              WHERE NOT c:DECLARE AND NOT c:Table AND NOT c:SPEC
                AND coalesce(toInteger(c.token), 0) >= 1000
              WITH c
              MATCH path = (c)-[:PARENT_OF*0..]->(n)
              WHERE NOT n:DECLARE AND NOT n:Table AND NOT n:SPEC
              WITH n, path, nodes(path) AS pathNodes
              WHERE ALL(i IN range(0, size(pathNodes)-2) 
                        WHERE coalesce(toInteger(pathNodes[i].token), 0) >= 1000)
              OPTIONAL MATCH (n)-[r]->(m {{
                folder_name: '{folder_name}', file_name: '{file_name}', user_id: '{user_id}'
              }})
              WHERE r IS NULL
                 OR ( NOT (m:DECLARE OR m:Table OR m:SPEC)
                      AND none(x IN ['CALL','WRITES','FROM'] WHERE type(r) CONTAINS x) )
              WITH n, labels(n) AS nLabels, r, m, coalesce(toInteger(n.startLine), 0) AS sortKey
              RETURN DISTINCT n, nLabels AS nodeLabels, r, m, sortKey
            }}
            
            RETURN n, nodeLabels, r, m
            ORDER BY sortKey, coalesce(toInteger(n.token), 0), id(n)
            """
        ])
        dbms_nodes = query_results[0] if query_results else []

        # ìŠ¤ì¼ˆë ˆí†¤ ìƒì„±
        skeleton_code = await start_dbms_skeleton(
            folder_name=folder_name,
            file_name=file_name,
            procedure_name=procedure_name,
            project_name=project_name,
            user_id=user_id,
            api_key=api_key,
            locale=locale,
            target_dbms=target_dbms
        )

        # ë³€í™˜ ìˆ˜í–‰
        generator = DbmsConversionGenerator(
            dbms_nodes,
            folder_name,
            file_name,
            procedure_name,
            user_id,
            api_key,
            locale,
            project_name,
            target_dbms,
            skeleton_code
        )

        await generator.generate()
        
        # íŒŒì¼ ì €ì¥
        base_name = file_name.rsplit(".", 1)[0]
        converted_code = await generator._save_target_file(base_name)

        log_process("DBMS", "DONE", f"âœ… {base_name} ë³€í™˜ ì™„ë£Œ")
        
        return converted_code

    except ConvertingError:
        raise
    except Exception as e:
        err_msg = f"DBMS ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        log_process("DBMS", "ERROR", f"âŒ {err_msg}", logging.ERROR, e)
        raise ConvertingError(err_msg)
    finally:
        await connection.close()

