import logging
import textwrap
import json
from understand.neo4j_connection import Neo4jConnection
from util.exception import ConvertingError
from util.utility_tool import extract_used_query_methods, collect_variables_in_range, build_rule_based_path, save_file, convert_to_pascal_case
from util.rule_loader import RuleLoader


# ----- ìƒìˆ˜ ì •ì˜ -----
TOKEN_THRESHOLD = 1000
CODE_PLACEHOLDER = "...code..."


# ----- ì„œë¹„ìŠ¤ ì „ì²˜ë¦¬ í´ë˜ìŠ¤ -----
class ServicePreprocessingGenerator:
    """
    ì„œë¹„ìŠ¤ ì „ì²˜ë¦¬ ì „ì²´ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬
    - ë‹¨ì¼ ì»¨í…ìŠ¤íŠ¸ ëˆ„ì  ë°©ì‹ìœ¼ë¡œ ìë°” ì½”ë“œ ìƒì„±
    - ëŒ€ìš©ëŸ‰ ë¶€ëª¨(í† í°â‰¥1000, ìì‹ ë³´ìœ ) ìŠ¤ì¼ˆë ˆí†¤ ê´€ë¦¬
    - í† í° ì„ê³„ ë„ë‹¬ ì‹œ LLM ë¶„ì„ ìˆ˜í–‰
    """
    __slots__ = (
        'traverse_nodes', 'variable_nodes', 'command_class_variable', 'service_skeleton',
        'query_method_list', 'directory', 'file_name', 'procedure_name', 'sequence_methods',
        'user_id', 'api_key', 'locale', 'project_name', 'target_lang',
        'merged_chunks', 'total_tokens', 'tracking_variables', 'parent_stack',
        'sp_code_parts', 'sp_start', 'sp_end', 'pending_try_mode', 'try_buffer',
        'rule_loader'
    )

    def __init__(self, traverse_nodes: list, variable_nodes: list, command_class_variable: dict,
                 service_skeleton: str, query_method_list: dict, directory: str, file_name: str,
                 procedure_name: str, sequence_methods: list, user_id: str, api_key: str, locale: str, 
                 project_name: str = "demo", target_lang: str = 'java'):
        self.traverse_nodes = traverse_nodes
        self.variable_nodes = variable_nodes
        self.command_class_variable = command_class_variable
        self.service_skeleton = service_skeleton
        self.query_method_list = query_method_list
        self.directory = directory
        self.file_name = file_name
        self.procedure_name = procedure_name
        self.sequence_methods = sequence_methods
        self.user_id = user_id
        self.api_key = api_key
        self.locale = locale
        self.project_name = project_name or "demo"
        self.target_lang = target_lang

        # ìƒíƒœ ì´ˆê¸°í™”
        self.merged_chunks: list[str] = []
        self.total_tokens = 0
        self.tracking_variables: dict = {}
        self.parent_stack: list[dict] = []
        self.sp_code_parts: list[str] = []
        self.sp_start: int | None = None
        self.sp_end: int | None = None

        # TRY-EXCEPTION ì²˜ë¦¬
        self.pending_try_mode = False
        self.try_buffer: list[str] = []

        # Rule íŒŒì¼ ë¡œë”
        self.rule_loader = RuleLoader(target_lang=target_lang)

    # ----- ê³µê°œ ë©”ì„œë“œ -----

    @staticmethod
    def _resolve_node_type(node_labels: list | None, node: dict) -> str:
        raw_type = node_labels[0] if node_labels else node.get('name', 'UNKNOWN')
        raw_type = str(raw_type)
        return raw_type.split('[')[0] if '[' in raw_type else raw_type

    @staticmethod
    def _safe_int(value) -> int:
        try:
            return int(value)
        except (TypeError, ValueError):
            return 0

    async def generate(self) -> str:
        """
        ì „ì²´ ë…¸ë“œë¥¼ ìˆœíšŒí•˜ë©° ìë°” ì½”ë“œ ìƒì„±
        
        Returns:
            str: ìµœì¢… ë³‘í•©ëœ ìë°” ì½”ë“œ
        """
        logging.info("ğŸ“‹ ë…¸ë“œ ìˆœíšŒ ì‹œì‘")

        seen_nodes = set()
        node_count = 0
        for record in self.traverse_nodes:
            node = record['n']
            start_line = self._safe_int(node.get('startLine'))
            end_line = self._safe_int(node.get('endLine'))
            node_key = (start_line, end_line)
            if node_key in seen_nodes:
                continue
            seen_nodes.add(node_key)
            node_count += 1
            await self._process_node(record)

        await self._finalize_remaining()

        logging.info(f"âœ… ì´ {node_count}ê°œ ë…¸ë“œ ì²˜ë¦¬ ì™„ë£Œ\n")
        return self._final_output()

    # ----- ë…¸ë“œ ì²˜ë¦¬ -----

    async def _process_node(self, record: dict) -> None:
        """ë‹¨ì¼ ë…¸ë“œ ì²˜ë¦¬"""
        node = record['n']
        node_labels = record.get('nodeLabels', [])
        node_type = self._resolve_node_type(node_labels, node)
        has_children = bool(node.get('has_children', False))
        token = self._safe_int(node.get('token'))
        start_line = self._safe_int(node.get('startLine'))
        end_line = self._safe_int(node.get('endLine'))
        logging.debug(
            "â†’ %s[%s~%s] í† í°=%s | ìì‹=%s",
            node_type,
            start_line,
            end_line,
            token,
            "ìˆìŒ" if has_children else "ì—†ìŒ"
        )

        if node_type == 'TRY':
            self.pending_try_mode = True
            logging.info("  ğŸ”’ TRY ë…¸ë“œ ê°ì§€ â†’ EXCEPTIONê¹Œì§€ merge ë³´ë¥˜")

        if node_type == 'EXCEPTION':
            await self._handle_exception_node(node, start_line, end_line)
            return

        while self.parent_stack and start_line > self.parent_stack[-1]['end']:
            if self.sp_code_parts:
                await self._analyze_and_merge()
            await self._finalize_parent()

        is_large_parent = token >= TOKEN_THRESHOLD and has_children
        is_large_leaf = token >= TOKEN_THRESHOLD and not has_children

        if is_large_parent:
            if self.sp_code_parts:
                await self._analyze_and_merge()
            logging.info("  â”Œâ”€ í° ë…¸ë“œ ì§„ì… [%s~%s] (í† í°: %s)", start_line, end_line, token)
            await self._handle_large_node(node, start_line, end_line, token)
        else:
            if is_large_leaf:
                if self.sp_code_parts:
                    await self._analyze_and_merge()
            else:
                await self._flush_pending_accumulation(token)
            self._handle_small_node(node, start_line, end_line, token)

        if is_large_leaf:
            logging.info("  âš ï¸  ë‹¨ë… ëŒ€ìš©ëŸ‰ ë¦¬í”„ ë…¸ë“œ ë³€í™˜ ì‹¤í–‰")
            await self._analyze_and_merge()
        elif self.total_tokens >= TOKEN_THRESHOLD:
            logging.info("  âš ï¸  í† í° ì„ê³„ê°’ ë„ë‹¬ (%s) â†’ LLM ë¶„ì„ ì‹¤í–‰", self.total_tokens)
            await self._analyze_and_merge()

    # ----- ëŒ€ìš©ëŸ‰ ë…¸ë“œ ì²˜ë¦¬ -----

    async def _handle_large_node(self, node: dict, start_line: int, end_line: int, token: int) -> None:
        """ëŒ€ìš©ëŸ‰ ë…¸ë“œ(ìì‹ ìˆìŒ, í† í°â‰¥1000) ì²˜ë¦¬"""
        summarized = (node.get('summarized_code') or '').strip()
        if not summarized:
            return
        
        # í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        used_vars, used_queries = await self._collect_current_context()

        # LLMìœ¼ë¡œ ìŠ¤ì¼ˆë ˆí†¤ ìƒì„± (Rule íŒŒì¼ ì‚¬ìš©)
        result = self.rule_loader.execute(
            role_name='service_summarized',
            inputs={
                'summarized_code': summarized,
                'service_skeleton': json.dumps(self.service_skeleton, ensure_ascii=False),
                'variable': json.dumps(used_vars, ensure_ascii=False, indent=2),
                'command_variables': json.dumps(self.command_class_variable, ensure_ascii=False, indent=2),
                'query_method_list': json.dumps(used_queries, ensure_ascii=False, indent=2),
                'sequence_methods': json.dumps(self.sequence_methods, ensure_ascii=False, indent=2),
                'locale': self.locale
            },
            api_key=self.api_key
        )
        skeleton = result['code']

        entry = {
            'start': start_line,
            'end': end_line,
            'code': skeleton,
            'children': []
        }
        self.parent_stack.append(entry)
        logging.info("  â”‚  ë¶€ëª¨ push ì™„ë£Œ (stack=%s)", len(self.parent_stack))


    # ----- ì†Œí˜• ë…¸ë“œ ì²˜ë¦¬ -----

    def _handle_small_node(self, node: dict, start_line: int, end_line: int, token: int) -> None:
        """ì†Œí˜• ë…¸ë“œ ë˜ëŠ” ë¦¬í”„ ë…¸ë“œ ì²˜ë¦¬"""
        node_code = (node.get('node_code') or '').strip()
        if not node_code:
            return

        # SP ì½”ë“œ ëˆ„ì 
        self.sp_code_parts.append(node_code)
        self.total_tokens += int(token or 0)

        # ë²”ìœ„ ì—…ë°ì´íŠ¸
        if self.sp_start is None or start_line < self.sp_start:
            self.sp_start = start_line
        if self.sp_end is None or end_line > self.sp_end:
            self.sp_end = end_line

    async def _flush_pending_accumulation(self, incoming_token: int | None) -> None:
        """ë‹¤ìŒ ë…¸ë“œ ì¶”ê°€ ì „ì— ì„ê³„ê°’ ì´ˆê³¼ ì—¬ë¶€ í™•ì¸"""
        if (
            self.sp_code_parts
            and incoming_token is not None
            and (self.total_tokens + int(incoming_token or 0)) >= TOKEN_THRESHOLD
        ):
            logging.info("  âš ï¸  ë‹¤ìŒ ë…¸ë“œ ì¶”ê°€ ì‹œ í† í° ì´ˆê³¼ ì˜ˆìƒ â†’ ê¸°ì¡´ ëˆ„ì  ë³€í™˜")
            await self._analyze_and_merge()

    # ----- ë³€ìˆ˜/JPA ìˆ˜ì§‘ -----

    async def _collect_current_context(self) -> tuple:
        """í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë³€ìˆ˜ì™€ ì¿¼ë¦¬ ë©”ì„œë“œ ìˆ˜ì§‘"""
        if not self.sp_start:
            return [], {}

        used_vars = []
        used_queries = {}

        # ë³€ìˆ˜ ìˆ˜ì§‘
        if self.variable_nodes:
            try:
                collected = await collect_variables_in_range(
                    self.variable_nodes, self.sp_start, self.sp_end or self.sp_start
                )
                used_vars = [{**v, 'role': self.tracking_variables.get(v['name'], '')} for v in collected]
            except Exception as e:
                logging.debug(f"ë³€ìˆ˜ ìˆ˜ì§‘ ìŠ¤í‚µ: {e}")

        # JPA ë©”ì„œë“œ ìˆ˜ì§‘
        if self.query_method_list:
            try:
                used_queries = await extract_used_query_methods(
                    self.sp_start, self.sp_end or self.sp_start, self.query_method_list, {}
                )
            except Exception as e:
                logging.debug(f"JPA ìˆ˜ì§‘ ìŠ¤í‚µ: {e}")

        return used_vars, used_queries

    # ----- ë¶€ëª¨ ê´€ë¦¬ -----

    async def _finalize_parent(self) -> None:
        """í˜„ì¬ ë¶€ëª¨ ë§ˆë¬´ë¦¬"""
        if not self.parent_stack:
            return

        entry = self.parent_stack.pop()
        logging.info(
            "  â””â”€ í° ë…¸ë“œ ì™„ë£Œ [%s~%s] (stackâ†’%s)",
            entry['start'],
            entry['end'],
            len(self.parent_stack)
        )

        code = self._merge_regular_children(entry['code'], entry.get('children', []))
        code = code.strip()
        self._add_child_code(code, entry.get('start'), entry.get('end'))

    def _merge_regular_children(self, code: str, children: list) -> str:
        """ë¶€ëª¨ placeholderì— ìì‹ ì½”ë“œ ì‚½ì…"""
        child_block = "\n".join(
            child for child in children or [] if isinstance(child, str) and child.strip()
        ).strip()

        if CODE_PLACEHOLDER in code:
            if child_block:
                indented = textwrap.indent(child_block, '    ')
                return code.replace(CODE_PLACEHOLDER, f"\n{indented}\n", 1)
            return code.replace(CODE_PLACEHOLDER, "", 1)

        if not child_block:
            return code

        indented = textwrap.indent(child_block, '    ')
        return f"{code}\n{indented}"

    def _add_child_code(self, code: str, start_line: int | None = None, end_line: int | None = None) -> None:
        """ìƒì„±ëœ ì½”ë“œë¥¼ ë¶€ëª¨ ë˜ëŠ” ìµœì¢… ì½”ë“œì— ì¶”ê°€"""
        if not code or not code.strip():
            return

        if self.parent_stack:
            parent_entry = self.parent_stack[-1]
            parent_entry.setdefault('children', []).append(code.strip())
            logging.info(
                "      â• ë¶€ëª¨ children ì¶”ê°€ | ë¶€ëª¨ë¼ì¸=%s~%s | child_count=%s",
                parent_entry.get('start'),
                parent_entry.get('end'),
                len(parent_entry['children'])
            )
            return

        target = self.try_buffer if self.pending_try_mode else self.merged_chunks
        target.append(code.strip())
        logging.info("      â• %sì— ë³€í™˜ ê²°ê³¼ ì¶”ê°€", "TRY ë²„í¼" if self.pending_try_mode else "ìµœì¢… ì½”ë“œ")

    # ----- EXCEPTION ë…¸ë“œ ì „ìš© ì²˜ë¦¬ -----

    async def _handle_exception_node(self, node: dict, start_line: int, end_line: int) -> None:
        """EXCEPTION ë…¸ë“œ ì „ìš© ì²˜ë¦¬: ì „ì²´ ì½”ë“œë¥¼ try-catchë¡œ ê°ì‹¸ëŠ” ì˜ˆì™¸ì²˜ë¦¬ êµ¬ì¡° ìƒì„±
        
        ì²˜ë¦¬ íë¦„:
        1. TRY ë…¸ë“œ ì¡´ì¬: TRY ë¸”ë¡ ì½”ë“œë§Œ ì˜ˆì™¸ì²˜ë¦¬ë¡œ ê°ì‹¸ê¸°
        2. TRY ë…¸ë“œ ë¯¸ì¡´ì¬: ì „ì²´ ë©”ì„œë“œ ì½”ë“œë¥¼ ì˜ˆì™¸ì²˜ë¦¬ë¡œ ê°ì‹¸ê¸°
        
        Args:
            node: EXCEPTION ë…¸ë“œ ë°ì´í„°
            start_line: ì‹œì‘ ë¼ì¸
            end_line: ì¢…ë£Œ ë¼ì¸
        """
        logging.info("  âš¡ EXCEPTION ë…¸ë“œ ê°ì§€ â†’ ì˜ˆì™¸ì²˜ë¦¬ êµ¬ì¡° ìƒì„± ì‹œì‘")

        if self.sp_code_parts:
            await self._analyze_and_merge()

        node_code = (node.get('node_code') or '').strip()
        if not node_code:
            logging.warning("     âš ï¸  EXCEPTION ë…¸ë“œ ì½”ë“œê°€ ë¹„ì–´ìˆìŒ")
            return

        result = self.rule_loader.execute(
            role_name='service_exception',
            inputs={
                'node_code': node_code,
                'locale': self.locale
            },
            api_key=self.api_key
        )
        exception_java_code = result.get('code', '').strip()

        if 'CodePlaceHolder' not in exception_java_code:
            logging.warning("     âš ï¸  try-catch í…œí”Œë¦¿ì— CodePlaceHolderê°€ ì—†ìŒ")
            return

        if self.pending_try_mode:
            try_block_code = "\n".join(self.try_buffer).strip()
            wrapped_code = exception_java_code.replace('CodePlaceHolder', try_block_code)
            if wrapped_code.strip():
                self.merged_chunks.append(wrapped_code)
            logging.info("     âœ“ TRY ë¸”ë¡ ì½”ë“œë¥¼ ì˜ˆì™¸ì²˜ë¦¬ë¡œ ê°ìŒˆ")
        else:
            entire_code = self._final_output()
            wrapped_code = exception_java_code.replace('CodePlaceHolder', entire_code)
            self.merged_chunks = [wrapped_code]
            logging.info("     âœ“ ì „ì²´ ë©”ì„œë“œ ì½”ë“œë¥¼ ì˜ˆì™¸ì²˜ë¦¬ë¡œ ê°ìŒˆ")

        self.try_buffer.clear()
        self.pending_try_mode = False
        logging.info("     âœ“ ì˜ˆì™¸ì²˜ë¦¬ ì™„ë£Œ ë° ìƒíƒœ ì´ˆê¸°í™”")

    # ----- ë¶„ì„ ë° ë³‘í•© -----

    async def _analyze_and_merge(self) -> None:
        """LLM ë¶„ì„ ë° ìë°” ì½”ë“œ ë³‘í•©"""
        if not self.sp_code_parts or self.sp_start is None:
            return

        # ë¬¸ìì—´ ì¡°ì¸
        sp_code = '\n'.join(self.sp_code_parts)
        if self.parent_stack:
            target = "ë¶€ëª¨ children"
        elif self.pending_try_mode:
            target = "TRY ë²„í¼"
        else:
            target = "ìµœì¢…ì½”ë“œ"
        logging.info(
            "  ğŸ¤– LLM ë¶„ì„ ì‹œì‘: [%s~%s] %sê°œ íŒŒíŠ¸ (í† í°: %s) â†’ %s",
            self.sp_start,
            self.sp_end,
            len(self.sp_code_parts),
            self.total_tokens,
            target
        )

        # ë³€ìˆ˜ ìˆ˜ì§‘
        used_variables = []
        try:
            collected = await collect_variables_in_range(self.variable_nodes, self.sp_start, self.sp_end)
            used_variables = [{**v, 'role': self.tracking_variables.get(v['name'], '')} for v in collected]
        except Exception as e:
            logging.debug(f"ë³€ìˆ˜ ìˆ˜ì§‘ ìŠ¤í‚µ: {e}")

        # JPA ë©”ì„œë“œ ìˆ˜ì§‘
        used_query_methods = {}
        try:
            used_query_methods = await extract_used_query_methods(
                self.sp_start, self.sp_end, self.query_method_list, {}
            )
        except Exception as e:
            logging.debug(f"JPA ìˆ˜ì§‘ ìŠ¤í‚µ: {e}")

        # LLM ë¶„ì„ (Role íŒŒì¼ ì‚¬ìš©)
        result = self.rule_loader.execute(
            role_name='service',
            inputs={
                'code': sp_code,
                'service_skeleton': json.dumps(self.service_skeleton, ensure_ascii=False),
                'variable': json.dumps(used_variables, ensure_ascii=False, indent=2),
                'query_method_list': json.dumps(used_query_methods, ensure_ascii=False, indent=2),
                'sequence_methods': json.dumps(self.sequence_methods, ensure_ascii=False, indent=2),
                'locale': self.locale,
                'parent_code': self.parent_stack[-1]['code'] if self.parent_stack else ""
            },
            api_key=self.api_key
        )

        analysis = result.get('analysis', {}) or {}
        self.tracking_variables.update(analysis.get('variables', {}))

        java_code = (analysis.get('code') or '').strip()
        if java_code:
            self._add_child_code(java_code, self.sp_start, self.sp_end)

        # ìƒíƒœ ì´ˆê¸°í™”
        self.total_tokens = 0
        self.sp_code_parts.clear()
        self.sp_start = None
        self.sp_end = None

    # ----- ë§ˆë¬´ë¦¬ -----

    async def _finalize_remaining(self) -> None:
        """ë‚¨ì€ ë°ì´í„° ì •ë¦¬"""
        if self.parent_stack:
            if self.sp_code_parts:
                await self._analyze_and_merge()
            while self.parent_stack:
                await self._finalize_parent()
        elif self.sp_code_parts:
            await self._analyze_and_merge()

    def _final_output(self) -> str:
        """ëˆ„ì ëœ ìë°” ì½”ë“œë¥¼ ë‹¨ì¼ ë¬¸ìì—´ë¡œ ë°˜í™˜"""
        chunks = list(self.merged_chunks)
        if self.pending_try_mode and self.try_buffer:
            chunks.extend(self.try_buffer)
        return "\n".join(chunk for chunk in chunks if chunk and chunk.strip()).strip()

    async def _save_service_file(self, service_class_name: str) -> str:
        """ì„±ëŠ¥ ìµœì í™”ëœ ì„œë¹„ìŠ¤ íŒŒì¼ ìë™ ì €ì¥"""
        try:
            # ë³‘í•©ëœ Java ì½”ë“œë¥¼ ì„œë¹„ìŠ¤ ìŠ¤ì¼ˆë ˆí†¤ì— ì‚½ì…
            completed_service_code = self.service_skeleton.replace("CodePlaceHolder", self._final_output())
            
            # ì €ì¥ ê²½ë¡œ ì„¤ì • (Rule íŒŒì¼ ê¸°ë°˜)
            base_path = build_rule_based_path(self.project_name, self.user_id, self.rule_loader.target_lang, 'service')
            
            # íŒŒì¼ ì €ì¥ (ë¹„ë™ê¸° ìµœì í™”)
            await save_file(
                content=completed_service_code,
                filename=f"{service_class_name}.java",
                base_path=base_path
            )
            
            logging.info(f"âœ… [{service_class_name}] ì„œë¹„ìŠ¤ íŒŒì¼ ìë™ ì €ì¥ ì™„ë£Œ")
            logging.info(f"ğŸ“ ì €ì¥ ê²½ë¡œ: {base_path}/{service_class_name}.java")
            
            return completed_service_code
            
        except Exception as e:
            logging.error(f"âŒ ì„œë¹„ìŠ¤ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            raise ConvertingError(f"ì„œë¹„ìŠ¤ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")


# ----- ì§„ì…ì  í•¨ìˆ˜ -----
async def start_service_preprocessing(
    service_skeleton: str,
    command_class_variable: dict,
    procedure_name: str,
    query_method_list: dict,
    directory: str,
    file_name: str,
    sequence_methods: list,
    project_name: str,
    user_id: str,
    api_key: str,
    locale: str,
    target_lang: str = 'java'
) -> tuple:
    """
    ì„œë¹„ìŠ¤ ì „ì²˜ë¦¬ ì‹œì‘
    
    Args:
        service_skeleton: ì„œë¹„ìŠ¤ ë©”ì„œë“œ ìŠ¤ì¼ˆë ˆí†¤ í…œí”Œë¦¿
        command_class_variable: ì»¤ë§¨ë“œ í´ë˜ìŠ¤ í•„ë“œ ì •ì˜
        procedure_name: í”„ë¡œì‹œì € ì´ë¦„
        query_method_list: JPA ì¿¼ë¦¬ ë©”ì„œë“œ ëª©ë¡
        directory: ë””ë ‰í† ë¦¬ ê²½ë¡œ
        file_name: íŒŒì¼ëª…
        sequence_methods: ì‹œí€€ìŠ¤ ë©”ì„œë“œ ëª©ë¡
        user_id: ì‚¬ìš©ì ID
        api_key: LLM API í‚¤
        locale: ë¡œì¼€ì¼
    
    Returns:
        None (íŒŒì¼ ë‚´ë¶€ì—ì„œ ìë™ ì €ì¥)
    
    Raises:
        ConvertingError: ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ
    """
    connection = Neo4jConnection()
    
    logging.info("\n" + "="*80)
    logging.info(f"âš™ï¸  STEP 4: Service ì½”ë“œ ìƒì„± - {procedure_name}")
    logging.info("="*80)
    logging.info(f"ğŸ“ íŒŒì¼: {directory}/{file_name}")

    # Neo4j ì¿¼ë¦¬ìš© ì •ê·œí™”ëœ directory (Windows ê²½ë¡œ êµ¬ë¶„ì í†µì¼)
    directory_normalized = directory.replace('\\', '/') if directory else ''

    try:
        # Neo4j ì¿¼ë¦¬
        service_nodes, variable_nodes = await connection.execute_queries([
            f"""
            MATCH (p:PROCEDURE {{
              directory: '{directory_normalized}',
              file_name: '{file_name}',
              procedure_name: '{procedure_name}',
              user_id: '{user_id}'
            }})
            
            CALL {{
              WITH p
              MATCH (p)-[:PARENT_OF]->(c)
              WHERE NOT c:DECLARE AND NOT c:Table AND NOT c:SPEC
                AND c.token < 1000
              WITH c, labels(c) AS cLabels, coalesce(toInteger(c.startLine), 0) AS sortKey
              RETURN c AS n, cLabels AS nodeLabels, NULL AS r, NULL AS m, sortKey
              
              UNION ALL
              
              // token >= 1000ì¸ í° ë…¸ë“œ â†’ ì‘ì€ ë…¸ë“œë¥¼ ë§Œë‚  ë•Œê¹Œì§€ ì¬ê·€ íƒìƒ‰
              WITH p
              MATCH (p)-[:PARENT_OF]->(c)
              WHERE NOT c:DECLARE AND NOT c:Table AND NOT c:SPEC
                AND coalesce(toInteger(c.token), 0) >= 1000
              // í° ë…¸ë“œë¶€í„° ìì† íƒìƒ‰
              WITH c
              MATCH path = (c)-[:PARENT_OF*0..]->(n)
              WHERE NOT n:DECLARE AND NOT n:Table AND NOT n:SPEC
              // ê²½ë¡œìƒ ëª¨ë“  ë…¸ë“œì˜ token ì²´í¬
              WITH n, path, nodes(path) AS pathNodes
              // í•µì‹¬: ê²½ë¡œì˜ ëª¨ë“  ë¶€ëª¨ê°€ í° ë…¸ë“œ(token >= 1000)ì´ê±°ë‚˜, 
              //       nì´ ì²« ë²ˆì§¸ ì‘ì€ ë…¸ë“œ(token < 1000)ì¸ ê²½ìš°ë§Œ ë°˜í™˜
              WHERE ALL(i IN range(0, size(pathNodes)-2) 
                        WHERE coalesce(toInteger(pathNodes[i].token), 0) >= 1000)
              OPTIONAL MATCH (n)-[r]->(m {{
                directory: '{directory_normalized}', file_name: '{file_name}', user_id: '{user_id}'
              }})
              WHERE r IS NULL
                 OR ( NOT (m:DECLARE OR m:Table OR m:SPEC)
                      AND none(x IN ['CALL','WRITES','FROM'] WHERE type(r) CONTAINS x) )
              WITH n, labels(n) AS nLabels, r, m, coalesce(toInteger(n.startLine), 0) AS sortKey
              RETURN DISTINCT n, nLabels AS nodeLabels, r, m, sortKey
            }}
            
            RETURN n, nodeLabels, r, m
            ORDER BY sortKey, coalesce(toInteger(n.token), 0), id(n)
            """,
            f"""
            MATCH (n {{directory: '{directory_normalized}', file_name: '{file_name}', 
                     procedure_name: '{procedure_name}', user_id: '{user_id}'}})
            WHERE n:DECLARE
            MATCH (n)-[:SCOPE]->(v:Variable)
            RETURN v
            """
        ])

        # ì „ì²˜ë¦¬ ìˆ˜í–‰
        generator = ServicePreprocessingGenerator(
            service_nodes,
            variable_nodes,
            command_class_variable,
            service_skeleton,
            query_method_list,
            directory,
            file_name,
            procedure_name,
            sequence_methods,
            user_id,
            api_key,
            locale,
            project_name,
            target_lang
        )

        await generator.generate()
        
        # ğŸš€ ì„±ëŠ¥ ìµœì í™”ëœ ìë™ íŒŒì¼ ì €ì¥
        service_class_name = convert_to_pascal_case(procedure_name) + "Service"
        service_code = await generator._save_service_file(service_class_name)

        logging.info("\n" + "-"*80)
        logging.info(f"âœ… STEP 4 ì™„ë£Œ: {service_class_name} ìƒì„± ë° ì €ì¥ ì™„ë£Œ")
        logging.info("-"*80 + "\n")
        
        return service_code

    except ConvertingError:
        raise
    except Exception as e:
        err_msg = f"ì„œë¹„ìŠ¤ ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        logging.error(err_msg)
        raise ConvertingError(err_msg)
    finally:
        await connection.close()
