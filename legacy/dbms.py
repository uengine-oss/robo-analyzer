"""DBMS ì½”ë“œ ë¶„ì„ ì „ëµ - PL/SQL, í”„ë¡œì‹œì €, í•¨ìˆ˜ ë“±

AST ê¸°ë°˜ PL/SQL ì½”ë“œ ë¶„ì„ â†’ Neo4j ê·¸ë˜í”„ ìƒì„±.

ë¶„ì„ íë¦„ (Frameworkì™€ ë™ì¼í•œ 2ë‹¨ê³„ + DDL):
1. [Phase 1] DDL ì²˜ë¦¬ + ëª¨ë“  íŒŒì¼ AST ê·¸ë˜í”„ ìƒì„± (ë³‘ë ¬)
2. [Phase 2] ëª¨ë“  íŒŒì¼ LLM ë¶„ì„ (ë³‘ë ¬)
3. [Phase 3] User Story ë¬¸ì„œ ìƒì„± (BaseStreamingAnalyzer ê³µí†µ)
"""

import asyncio
import json
import logging
import os
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, AsyncGenerator, Optional, List, Dict, Tuple

import aiofiles

from analyzer.neo4j_client import Neo4jClient
from analyzer.strategy.base_analyzer import BaseStreamingAnalyzer, AnalysisStats
from analyzer.strategy.dbms.ast_processor import DbmsAstProcessor
from config.settings import settings
from util.exception import AnalysisError
from util.rule_loader import RuleLoader
from util.stream_utils import (
    emit_data,
    emit_message,
    format_graph_result,
)
from util.utility_tool import (
    escape_for_cypher,
    log_process,
    parse_table_identifier,
    generate_user_story_document,
)


class FileStatus(Enum):
    """íŒŒì¼ ë¶„ì„ ìƒíƒœ"""
    PENDING = "PENDING"
    PH1_OK = "PH1_OK"
    PH1_FAIL = "PH1_FAIL"
    PH2_OK = "PH2_OK"
    PH2_FAIL = "PH2_FAIL"
    SKIPPED = "SKIPPED"


@dataclass
class FileAnalysisContext:
    """íŒŒì¼ ë¶„ì„ ì»¨í…ìŠ¤íŠ¸"""
    directory: str
    file_name: str
    ast_data: dict
    source_lines: List[str]
    processor: Optional[DbmsAstProcessor] = None
    status: FileStatus = field(default=FileStatus.PENDING)
    error_message: str = ""


class DbmsAnalyzer(BaseStreamingAnalyzer):
    """DBMS ì½”ë“œ ë¶„ì„ ì „ëµ
    
    2ë‹¨ê³„ ë¶„ì„ + DDL ì²˜ë¦¬ (Frameworkì™€ ë™ì¼):
    - Phase 1: DDL ì²˜ë¦¬ + ëª¨ë“  íŒŒì¼ AST ê·¸ë˜í”„ ìƒì„± (ë³‘ë ¬)
    - Phase 2: ëª¨ë“  íŒŒì¼ LLM ë¶„ì„ (ë³‘ë ¬) - Phase1 ì‹¤íŒ¨ íŒŒì¼ ì œì™¸
    - Phase 3: User Story ë¬¸ì„œ ìƒì„± (ë¶€ëª¨ í´ë˜ìŠ¤ ê³µí†µ)
    """

    # =========================================================================
    # ì „ëµ ë©”íƒ€ë°ì´í„° (BaseStreamingAnalyzer êµ¬í˜„)
    # =========================================================================
    
    @property
    def strategy_name(self) -> str:
        return "DBMS"
    
    @property
    def strategy_emoji(self) -> str:
        return "ğŸ—„ï¸"
    
    @property
    def file_type_description(self) -> str:
        return "SQL íŒŒì¼"

    def __init__(self):
        self._cypher_lock = asyncio.Lock()
        self._file_semaphore: Optional[asyncio.Semaphore] = None
        self._ddl_schemas: set[str] = set()  # DDLì—ì„œ ìˆ˜ì§‘ëœ ìŠ¤í‚¤ë§ˆ Set
        # DDL ë©”íƒ€ë°ì´í„° ìºì‹œ: {(schema, table_name): {description, columns}}
        self._ddl_table_metadata: Dict[Tuple[str, str], Dict[str, Any]] = {}

    # =========================================================================
    # ë©”ì¸ íŒŒì´í”„ë¼ì¸ (BaseStreamingAnalyzer êµ¬í˜„)
    # =========================================================================

    async def run_pipeline(
        self,
        file_names: list[tuple[str, str]],
        client: Neo4jClient,
        orchestrator: Any,
        stats: AnalysisStats,
    ) -> AsyncGenerator[bytes, None]:
        """DBMS ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        íë¦„ (Frameworkì™€ ë™ì¼):
        1. DDL ì²˜ë¦¬ + íŒŒì¼ ë¡œë“œ (ë³‘ë ¬)
        2. Phase 1: ëª¨ë“  íŒŒì¼ AST ê·¸ë˜í”„ ìƒì„± (ë³‘ë ¬)
        3. Phase 2: ëª¨ë“  íŒŒì¼ LLM ë¶„ì„ (ë³‘ë ¬) - Phase1 ì‹¤íŒ¨ íŒŒì¼ ì œì™¸
        
        Note: User Story PhaseëŠ” ë¶€ëª¨ í´ë˜ìŠ¤ì—ì„œ ì²˜ë¦¬
        """
        total_files = len(file_names)
        self._file_semaphore = asyncio.Semaphore(settings.concurrency.file_concurrency)

        yield emit_message(f"âš¡ ë³‘ë ¬ ì²˜ë¦¬: íŒŒì¼ {settings.concurrency.file_concurrency}ê°œ ë™ì‹œ")

        # ========== DDL ì²˜ë¦¬ ==========
        async for chunk in self._run_ddl_phase(client, orchestrator, stats):
            yield chunk

        # ========== íŒŒì¼ ë¡œë“œ ==========
        yield emit_message("")
        yield self.emit_separator()
        yield self.emit_phase_header(1, "ğŸ—ï¸ AST êµ¬ì¡° ê·¸ë˜í”„ ìƒì„±", f"{total_files}ê°œ íŒŒì¼ ë³‘ë ¬")
        yield self.emit_separator()

        contexts = await self._load_all_files(file_names, orchestrator)
        yield emit_message(f"   âœ“ {len(contexts)}ê°œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ")

        # ========== Phase 1: AST ê·¸ë˜í”„ ìƒì„± (ë³‘ë ¬) ==========
        async for chunk in self._run_phase1(contexts, client, orchestrator, stats):
            yield chunk

        # Phase 1 ê²°ê³¼ ìš”ì•½
        ph1_ok_count = sum(1 for c in contexts if c.status == FileStatus.PH1_OK)
        ph1_fail_count = sum(1 for c in contexts if c.status == FileStatus.PH1_FAIL)
        
        yield emit_message("")
        yield self.emit_phase_complete(1, f"{stats.static_nodes_created}ê°œ ë…¸ë“œ ìƒì„±")
        if ph1_fail_count > 0:
            yield self.emit_warning(f"Phase 1 ì‹¤íŒ¨: {ph1_fail_count}ê°œ íŒŒì¼ â†’ Phase 2 ìŠ¤í‚µ (í† í° ì ˆê°)")

        # ========== Phase 2: LLM ë¶„ì„ (ë³‘ë ¬) - Phase1 ì„±ê³µ íŒŒì¼ë§Œ ==========
        ph2_targets = [c for c in contexts if c.status == FileStatus.PH1_OK]
        
        yield emit_message("")
        yield self.emit_separator()
        yield self.emit_phase_header(2, "ğŸ¤– AI ë¶„ì„", f"{len(ph2_targets)}ê°œ íŒŒì¼ ë³‘ë ¬")
        yield self.emit_separator()
        
        if ph1_fail_count > 0:
            yield emit_message(f"   â„¹ï¸ {ph1_fail_count}ê°œ íŒŒì¼ì€ Phase 1 ì‹¤íŒ¨ë¡œ ìŠ¤í‚µë¨ (í† í° ì ˆê°)")

        async for chunk in self._run_phase2(ph2_targets, client, orchestrator, stats):
            yield chunk

        yield emit_message("")
        yield self.emit_phase_complete(2, f"{stats.llm_batches_executed}ê°œ ë¶„ì„ ì™„ë£Œ")

    # =========================================================================
    # User Story ë¬¸ì„œ ìƒì„± (BaseStreamingAnalyzer êµ¬í˜„)
    # =========================================================================

    async def build_user_story_doc(
        self,
        client: Neo4jClient,
        orchestrator: Any,
    ) -> Optional[str]:
        """ë¶„ì„ëœ í”„ë¡œì‹œì €ì—ì„œ User Story ë¬¸ì„œ ìƒì„±"""
        query = f"""
            MATCH (n)
            WHERE (n:PROCEDURE OR n:FUNCTION OR n:TRIGGER)
              AND n.user_id = '{escape_for_cypher(orchestrator.user_id)}'
              AND n.project_name = '{escape_for_cypher(orchestrator.project_name)}'
              AND n.summary IS NOT NULL
            OPTIONAL MATCH (n)-[:HAS_USER_STORY]->(us:UserStory)
            OPTIONAL MATCH (us)-[:HAS_AC]->(ac:AcceptanceCriteria)
            WITH n, us, collect(DISTINCT {{
                id: ac.id,
                title: ac.title,
                given: ac.given,
                when: ac.when,
                then: ac.then
            }}) AS acceptance_criteria
            WITH n, collect(DISTINCT {{
                id: us.id,
                role: us.role,
                goal: us.goal,
                benefit: us.benefit,
                acceptance_criteria: acceptance_criteria
            }}) AS user_stories
            RETURN n.procedure_name AS name, 
                   n.summary AS summary,
                   user_stories AS user_stories, 
                   labels(n)[0] AS type
            ORDER BY n.file_name, n.startLine
        """
        
        async with self._cypher_lock:
            results = await client.execute_queries([query])
        
        if not results or not results[0]:
            raise AnalysisError("User Story ìƒì„±ì„ ìœ„í•œ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        filtered = [
            r for r in results[0]
            if r.get("summary") or (r.get("user_stories") and len(r["user_stories"]) > 0)
        ]
        
        if not filtered:
            return None
        
        log_process("ANALYZE", "USER_STORY", f"User Story ìƒì„± | ëŒ€ìƒ={len(filtered)}ê°œ í”„ë¡œì‹œì €")
        return generate_user_story_document(
            results=filtered,
            source_name=orchestrator.project_name,
            source_type="DBMS í”„ë¡œì‹œì €/í•¨ìˆ˜",
        )

    # =========================================================================
    # DDL ì²˜ë¦¬
    # =========================================================================

    async def _run_ddl_phase(
        self,
        client: Neo4jClient,
        orchestrator: Any,
        stats: AnalysisStats,
    ) -> AsyncGenerator[bytes, None]:
        """DDL íŒŒì¼ ì²˜ë¦¬ - í…Œì´ë¸”/ì»¬ëŸ¼ ìŠ¤í‚¤ë§ˆ ìƒì„±"""
        ddl_files = self._list_ddl_files(orchestrator)
        
        if not ddl_files:
            yield self.emit_skip("DDL íŒŒì¼ ì—†ìŒ â†’ ìŠ¤í‚¤ë§ˆ ì²˜ë¦¬ ê±´ë„ˆëœ€")
            return
        
        ddl_count = len(ddl_files)
        yield emit_message("")
        yield self.emit_separator()
        yield self.emit_phase_header(0, "ğŸ“‹ DDL ìŠ¤í‚¤ë§ˆ ìˆ˜ì§‘", f"{ddl_count}ê°œ DDL")
        yield self.emit_separator()
        
        ddl_dir = orchestrator.dirs["ddl"]
        
        for idx, ddl_file in enumerate(ddl_files, 1):
            yield emit_message("")
            yield self.emit_file_start(idx, ddl_count, ddl_file)
            
            ddl_graph, ddl_stats = await self._process_ddl(
                ddl_path=os.path.join(ddl_dir, ddl_file),
                client=client,
                file_name=ddl_file,
                orchestrator=orchestrator,
            )
            
            if ddl_stats["tables"]:
                yield emit_message(f"   âœ“ Table ë…¸ë“œ: {ddl_stats['tables']}ê°œ")
            if ddl_stats["columns"]:
                yield emit_message(f"   âœ“ Column ë…¸ë“œ: {ddl_stats['columns']}ê°œ")
            if ddl_stats["fks"]:
                yield emit_message(f"   âœ“ FK ê´€ê³„: {ddl_stats['fks']}ê°œ")
            
            stats.add_ddl_result(ddl_stats["tables"], ddl_stats["columns"], ddl_stats["fks"])
            
            if ddl_graph and (ddl_graph.get("Nodes") or ddl_graph.get("Relationships")):
                yield emit_data(
                    graph=ddl_graph,
                    line_number=0,
                    analysis_progress=0,
                    current_file=f"DDL-{ddl_file}",
                )
        
        yield emit_message("")
        yield emit_message("ğŸ“Š DDL ì²˜ë¦¬ ì™„ë£Œ:")
        yield emit_message(f"   â€¢ í…Œì´ë¸”: {stats.ddl_tables}ê°œ")
        yield emit_message(f"   â€¢ ì»¬ëŸ¼: {stats.ddl_columns}ê°œ")
        yield emit_message(f"   â€¢ FK: {stats.ddl_fks}ê°œ")

    def _list_ddl_files(self, orchestrator: Any) -> list[str]:
        """DDL íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        ddl_dir = orchestrator.dirs.get("ddl", "")
        if not ddl_dir:
            log_process("ANALYZE", "DDL", "DDL ë””ë ‰í† ë¦¬ ì„¤ì • ì—†ìŒ - DDL ì²˜ë¦¬ ìƒëµ")
            return []
        if not os.path.isdir(ddl_dir):
            raise AnalysisError(f"DDL ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {ddl_dir}")
        try:
            files = sorted(
                f for f in os.listdir(ddl_dir)
                if os.path.isfile(os.path.join(ddl_dir, f))
            )
            if not files:
                raise AnalysisError(f"DDL ë””ë ‰í† ë¦¬ì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {ddl_dir}")
            log_process("ANALYZE", "DDL", f"DDL íŒŒì¼ ë°œê²¬: {len(files)}ê°œ")
            return files
        except OSError as e:
            raise AnalysisError(f"DDL ë””ë ‰í† ë¦¬ ì½ê¸° ì‹¤íŒ¨: {ddl_dir}") from e

    async def _process_ddl(
        self,
        ddl_path: str,
        client: Neo4jClient,
        file_name: str,
        orchestrator: Any,
    ) -> tuple[dict, dict]:
        """DDL íŒŒì¼ ì²˜ë¦¬ ë° í…Œì´ë¸”/ì»¬ëŸ¼ ë…¸ë“œ ìƒì„±"""
        ddl_stats = {"tables": 0, "columns": 0, "fks": 0}
        
        async with aiofiles.open(ddl_path, "r", encoding="utf-8") as f:
            ddl_content = await f.read()
        
        loader = RuleLoader(target_lang="dbms")
        parsed = loader.execute(
            "ddl",
            {"ddl_content": ddl_content, "locale": orchestrator.locale},
            orchestrator.api_key,
        )
        
        queries = []
        common = {
            "user_id": orchestrator.user_id,
            "db": orchestrator.target,
            "project_name": orchestrator.project_name,
        }

        for table_info in parsed.get("analysis", []):
            table = table_info.get("table", {})
            columns = table_info.get("columns", [])
            foreign_keys = table_info.get("foreignKeys", [])
            primary_keys = [
                str(pk).strip().upper()
                for pk in (table_info.get("primaryKeys") or [])
                if pk
            ]

            schema_raw = (table.get("schema") or "").strip()
            table_name = (table.get("name") or "").strip()
            comment = (table.get("comment") or "").strip()
            table_type = (table.get("table_type") or "BASE TABLE").strip().upper()
            
            qualified = f"{schema_raw}.{table_name}" if schema_raw else table_name
            parsed_schema, parsed_name, _ = parse_table_identifier(qualified)
            schema = parsed_schema or ""
            
            # DDLì—ì„œ ë°œê²¬ëœ ìŠ¤í‚¤ë§ˆ ìˆ˜ì§‘
            if schema:
                self._ddl_schemas.add(schema.lower())

            # Table ë…¸ë“œ ìƒì„±
            merge_key = {**common, "schema": schema, "name": parsed_name}
            merge_str = ", ".join(f"`{k}`: '{v}'" for k, v in merge_key.items())
            
            column_metadata = {}
            for col in columns:
                col_name = (col.get("name") or "").strip()
                if not col_name:
                    continue
                col_comment = (col.get("comment") or "").strip()
                column_metadata[col_name] = {
                    "description": col_comment,
                    "dtype": (col.get("dtype") or col.get("type") or "").strip(),
                    "nullable": col.get("nullable", True),
                }
            
            set_props = {
                **common,
                "description": escape_for_cypher(comment),
                "table_type": table_type,
            }
            set_str = ", ".join(f"t.`{k}` = '{v}'" for k, v in set_props.items())
            queries.append(f"MERGE (t:Table {{{merge_str}}}) SET {set_str} RETURN t")
            ddl_stats["tables"] += 1
            
            # DDL ë©”íƒ€ë°ì´í„° ìºì‹œ ì €ì¥ (ë©”ëª¨ë¦¬)
            table_key = (schema.lower(), parsed_name.lower())
            self._ddl_table_metadata[table_key] = {
                "description": comment,
                "columns": column_metadata,
            }

            # Column ë…¸ë“œ ìƒì„±
            for col in columns:
                col_name = (col.get("name") or "").strip()
                if not col_name:
                    continue
                
                col_type = (col.get("dtype") or col.get("type") or "").strip()
                col_nullable = col.get("nullable", True)
                col_comment = (col.get("comment") or "").strip()
                fqn = ".".join(filter(None, [schema, parsed_name, col_name])).lower()

                col_merge = {"user_id": orchestrator.user_id, "fqn": fqn, "project_name": orchestrator.project_name}
                col_merge_str = ", ".join(f"`{k}`: '{v}'" for k, v in col_merge.items())
                col_set = {
                    "name": escape_for_cypher(col_name),
                    "dtype": escape_for_cypher(col_type),
                    "description": escape_for_cypher(col_comment),
                    "nullable": "true" if col_nullable else "false",
                    "project_name": orchestrator.project_name,
                    "fqn": fqn,
                }
                if col_name.upper() in primary_keys:
                    col_set["pk_constraint"] = f"{parsed_name}_pkey"
                
                col_set_str = ", ".join(f"c.`{k}` = '{v}'" for k, v in col_set.items())
                queries.append(f"MERGE (c:Column {{{col_merge_str}}}) SET {col_set_str} RETURN c")
                queries.append(
                    f"MATCH (t:Table {{{merge_str}}})\n"
                    f"MATCH (c:Column {{{col_merge_str}}})\n"
                    f"MERGE (t)-[r:HAS_COLUMN]->(c) RETURN t, r, c"
                )
                ddl_stats["columns"] += 1

            # FK ê´€ê³„ ìƒì„±
            for fk in foreign_keys:
                src_col = (fk.get("column") or "").strip()
                ref = (fk.get("ref") or "").strip()
                if not src_col or not ref or "." not in ref:
                    continue

                ref_table_part, ref_col = ref.rsplit(".", 1)
                ref_schema, ref_table, _ = parse_table_identifier(ref_table_part)
                ref_schema = ref_schema or schema

                ref_table_merge = {**common, "schema": ref_schema or "", "name": ref_table or ""}
                ref_merge_str = ", ".join(f"`{k}`: '{v}'" for k, v in ref_table_merge.items())
                queries.append(f"MERGE (rt:Table {{{ref_merge_str}}}) RETURN rt")
                queries.append(
                    f"MATCH (t:Table {{{merge_str}}})\n"
                    f"MATCH (rt:Table {{{ref_merge_str}}})\n"
                    f"MERGE (t)-[r:FK_TO_TABLE]->(rt) RETURN t, r, rt"
                )
                ddl_stats["fks"] += 1

        async with self._cypher_lock:
            result = await client.run_graph_query(queries)
        
        log_process("ANALYZE", "DDL", f"DDL ì²˜ë¦¬ ì™„ë£Œ: {file_name} (T:{ddl_stats['tables']}, C:{ddl_stats['columns']}, FK:{ddl_stats['fks']})")
        return result, ddl_stats

    # =========================================================================
    # ìŠ¤í‚¤ë§ˆ ê²°ì •
    # =========================================================================

    def _resolve_default_schema(self, directory: str) -> str:
        """íŒŒì¼ ê²½ë¡œì—ì„œ ê¸°ë³¸ ìŠ¤í‚¤ë§ˆë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
        
        ìš°ì„ ìˆœìœ„:
        1. ê²½ë¡œì˜ í´ë”ëª… ì¤‘ DDL ìŠ¤í‚¤ë§ˆì™€ ì¼ì¹˜í•˜ëŠ” ê²ƒ (ê¹Šì€ í´ë” ìš°ì„ )
        2. ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ 'public'
        """
        if not directory or not self._ddl_schemas:
            return "public"
        
        # ê²½ë¡œë¥¼ í´ë” ëª©ë¡ìœ¼ë¡œ ë¶„ë¦¬ (ê¹Šì€ ìˆœì„œëŒ€ë¡œ)
        parts = directory.replace("\\", "/").split("/")
        parts = [p.lower() for p in parts if p]
        
        # ê¹Šì€ í´ë”ë¶€í„° ë§¤ì¹­ (ì—­ìˆœ ìˆœíšŒ)
        for folder in reversed(parts):
            if folder in self._ddl_schemas:
                return folder
        
        return "public"

    # =========================================================================
    # íŒŒì¼ ë¡œë“œ
    # =========================================================================

    async def _load_all_files(
        self,
        file_names: list[tuple[str, str]],
        orchestrator: Any,
    ) -> List[FileAnalysisContext]:
        """ëª¨ë“  íŒŒì¼ì˜ ASTì™€ ì†ŒìŠ¤ì½”ë“œë¥¼ ë³‘ë ¬ë¡œ ë¡œë“œí•©ë‹ˆë‹¤."""
        
        async def load_single(directory: str, file_name: str) -> FileAnalysisContext:
            src_path = os.path.join(orchestrator.dirs["src"], directory, file_name)
            base_name = os.path.splitext(file_name)[0]
            ast_path = os.path.join(orchestrator.dirs["analysis"], directory, f"{base_name}.json")

            async with aiofiles.open(ast_path, "r", encoding="utf-8") as ast_file, \
                       aiofiles.open(src_path, "r", encoding="utf-8") as src_file:
                ast_content, source_lines = await asyncio.gather(
                    ast_file.read(),
                    src_file.readlines(),
                )
                return FileAnalysisContext(
                    directory=directory,
                    file_name=file_name,
                    ast_data=json.loads(ast_content),
                    source_lines=source_lines,
                )

        tasks = [load_single(d, f) for d, f in file_names]
        return await asyncio.gather(*tasks)

    # =========================================================================
    # Phase 1: AST ê·¸ë˜í”„ ìƒì„±
    # =========================================================================

    async def _run_phase1(
        self,
        contexts: List[FileAnalysisContext],
        client: Neo4jClient,
        orchestrator: Any,
        stats: AnalysisStats,
    ) -> AsyncGenerator[bytes, None]:
        """Phase 1: ëª¨ë“  íŒŒì¼ì˜ AST ê·¸ë˜í”„ë¥¼ ë³‘ë ¬ë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
        
        completed = 0
        total = len(contexts)
        results_queue: asyncio.Queue = asyncio.Queue()

        async def process_file(ctx: FileAnalysisContext):
            async with self._file_semaphore:
                try:
                    # íŒŒì¼ ê²½ë¡œ ê¸°ë°˜ ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ ê²°ì •
                    default_schema = self._resolve_default_schema(ctx.directory)
                    
                    processor = DbmsAstProcessor(
                        antlr_data=ctx.ast_data,
                        file_content="".join(ctx.source_lines),
                        directory=ctx.directory,
                        file_name=ctx.file_name,
                        user_id=orchestrator.user_id,
                        api_key=orchestrator.api_key,
                        locale=orchestrator.locale,
                        dbms=orchestrator.target,
                        project_name=orchestrator.project_name,
                        last_line=len(ctx.source_lines),
                        default_schema=default_schema,
                        ddl_table_metadata=self._ddl_table_metadata,
                    )
                    ctx.processor = processor
                    
                    # ì •ì  ê·¸ë˜í”„ ìƒì„±
                    queries = processor.build_static_graph_queries()
                    
                    if queries:
                        async with self._cypher_lock:
                            graph = await client.run_graph_query(queries)
                        
                        node_count = len(graph.get("Nodes", []))
                        rel_count = len(graph.get("Relationships", []))
                        
                        ctx.status = FileStatus.PH1_OK
                        await results_queue.put({
                            "type": "success",
                            "file": ctx.file_name,
                            "graph": graph,
                            "node_count": node_count,
                            "rel_count": rel_count,
                        })
                    else:
                        ctx.status = FileStatus.PH1_OK
                        await results_queue.put({
                            "type": "success",
                            "file": ctx.file_name,
                            "graph": {"Nodes": [], "Relationships": []},
                            "node_count": 0,
                            "rel_count": 0,
                        })
                        
                except Exception as e:
                    log_process("ANALYZE", "ERROR", f"Phase 1 ì˜¤ë¥˜ ({ctx.file_name}): {e}", logging.ERROR, e)
                    ctx.status = FileStatus.PH1_FAIL
                    ctx.error_message = str(e)[:100]
                    await results_queue.put({
                        "type": "error",
                        "file": ctx.file_name,
                        "message": str(e),
                    })

        # ëª¨ë“  íŒŒì¼ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘
        tasks = [asyncio.create_task(process_file(ctx)) for ctx in contexts]

        # ê²°ê³¼ ìˆ˜ì‹  ë° ìŠ¤íŠ¸ë¦¬ë°
        while completed < total:
            result = await asyncio.wait_for(results_queue.get(), timeout=300.0)
            completed += 1
            stats.files_completed = completed
            
            if result["type"] == "error":
                yield emit_message(f"   âŒ [{completed}/{total}] {result['file']}: {result['message'][:50]}")
                stats.mark_file_failed(result['file'], "Phase1 ì‹¤íŒ¨")
            else:
                stats.add_graph_result(result["graph"], is_static=True)
                
                graph = result["graph"]
                graph_msg = format_graph_result(graph)
                
                yield emit_message(f"   âœ“ [{completed}/{total}] {result['file']}")
                if graph_msg:
                    for line in graph_msg.split("\n")[:3]:
                        yield emit_message(f"      {line}")
                
                yield emit_data(
                    graph=graph,
                    line_number=0,
                    analysis_progress=int(completed / total * 50),
                    current_file=result["file"],
                )

        # ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
        await asyncio.gather(*tasks, return_exceptions=True)

    # =========================================================================
    # Phase 2: LLM ë¶„ì„
    # =========================================================================

    async def _run_phase2(
        self,
        contexts: List[FileAnalysisContext],
        client: Neo4jClient,
        orchestrator: Any,
        stats: AnalysisStats,
    ) -> AsyncGenerator[bytes, None]:
        """Phase 2: Phase1 ì„±ê³µ íŒŒì¼ì˜ LLM ë¶„ì„ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        
        if not contexts:
            yield emit_message("   â„¹ï¸ ë¶„ì„ ëŒ€ìƒ íŒŒì¼ ì—†ìŒ")
            return
        
        completed = 0
        total = len(contexts)
        results_queue: asyncio.Queue = asyncio.Queue()

        async def analyze_file(ctx: FileAnalysisContext):
            async with self._file_semaphore:
                try:
                    if not ctx.processor:
                        raise AnalysisError(f"Phase 1ì—ì„œ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì‹¤íŒ¨: {ctx.file_name}")
                    
                    # LLM ë¶„ì„ ì‹¤í–‰ (íŠœí”Œ ë°˜í™˜: queries, failed_batch_count, failed_details)
                    analysis_queries, failed_batch_count, failed_details = await ctx.processor.run_llm_analysis()
                    
                    if analysis_queries:
                        async with self._cypher_lock:
                            graph = await client.run_graph_query(analysis_queries)
                        
                        ctx.status = FileStatus.PH2_OK
                        await results_queue.put({
                            "type": "success",
                            "file": ctx.file_name,
                            "graph": graph,
                            "query_count": len(analysis_queries),
                            "failed_batches": failed_batch_count,
                            "failed_details": failed_details,  # ìƒì„¸ ì •ë³´ ì¶”ê°€
                        })
                    else:
                        ctx.status = FileStatus.PH2_OK
                        await results_queue.put({
                            "type": "success",
                            "file": ctx.file_name,
                            "graph": {"Nodes": [], "Relationships": []},
                            "query_count": 0,
                            "failed_batches": failed_batch_count,
                        })
                    
                    # ë°°ì¹˜ ì‹¤íŒ¨ê°€ ìˆìœ¼ë©´ ê²½ê³  í‘œì‹œ
                    if failed_batch_count > 0:
                        await results_queue.put({
                            "type": "warning",
                            "file": ctx.file_name,
                            "message": f"{failed_batch_count}ê°œ ë°°ì¹˜ ì‹¤íŒ¨ (ë¶€ë¶„ ì„±ê³µ)",
                        })
                        
                except Exception as e:
                    log_process("ANALYZE", "ERROR", f"Phase 2 ì˜¤ë¥˜ ({ctx.file_name}): {e}", logging.ERROR, e)
                    ctx.status = FileStatus.PH2_FAIL
                    ctx.error_message = str(e)[:100]
                    await results_queue.put({
                        "type": "error",
                        "file": ctx.file_name,
                        "message": str(e),
                    })

        # ëª¨ë“  íŒŒì¼ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘
        tasks = [asyncio.create_task(analyze_file(ctx)) for ctx in contexts]

        # ê²°ê³¼ ìˆ˜ì‹  ë° ìŠ¤íŠ¸ë¦¬ë°
        while completed < total:
            result = await asyncio.wait_for(results_queue.get(), timeout=600.0)
            result_type = result.get("type", "")
            
            # warningì€ ì¹´ìš´íŠ¸í•˜ì§€ ì•ŠìŒ (ì¶”ê°€ ì •ë³´ì¼ ë¿)
            if result_type == "warning":
                yield emit_message(f"   âš ï¸ {result['file']}: {result['message']}")
                continue
            
            completed += 1
            
            if result_type == "error":
                yield emit_message(f"   âŒ [{completed}/{total}] {result['file']}: {result['message'][:50]}")
                stats.mark_file_failed(result['file'], "Phase2 ì‹¤íŒ¨")
            else:
                stats.llm_batches_executed += 1
                graph = result["graph"]
                stats.add_graph_result(graph, is_static=False)
                
                # ë°°ì¹˜ ì‹¤íŒ¨ ì •ë³´ í‘œì‹œ
                failed_batches = result.get("failed_batches", 0)
                failed_details = result.get("failed_details", [])
                fail_info = f" (ë°°ì¹˜ {failed_batches}ê°œ ì‹¤íŒ¨)" if failed_batches > 0 else ""
                
                graph_msg = format_graph_result(graph)
                yield emit_message(f"   âœ“ [{completed}/{total}] {result['file']} (ì¿¼ë¦¬ {result['query_count']}ê°œ){fail_info}")
                if graph_msg:
                    for line in graph_msg.split("\n")[:3]:
                        yield emit_message(f"      {line}")
                
                # ì‹¤íŒ¨ ìƒì„¸ ì •ë³´ ì¶œë ¥ (ìµœëŒ€ 3ê°œ)
                if failed_details:
                    stats.llm_batches_failed += len(failed_details)
                    for detail in failed_details[:3]:
                        yield emit_message(f"      âš ï¸ ë°°ì¹˜ #{detail['batch_id']} ({detail['node_ranges']}): {detail['error'][:50]}")
                
                yield emit_data(
                    graph=graph,
                    line_number=0,
                    analysis_progress=50 + int(completed / total * 50),
                    current_file=result["file"],
                )

        # ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
        await asyncio.gather(*tasks, return_exceptions=True)