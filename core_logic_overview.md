# Legacy Modernizer í•µì‹¬ ë¡œì§ (ì†ŒìŠ¤ ì½”ë“œ ì¤‘ì‹¬)

## Understanding ê³µìœ  ë¡œì§

### StatementCollector._visit: AST í‰íƒ„í™” + StatementNode ìƒì„±
AST ë…¸ë“œë¥¼ í›„ìœ„ ìˆœíšŒí•˜ë©° `StatementNode`ë¡œ ë³€í™˜í•˜ê³ , ë¶€ëª¨Â·ìì‹ ì—°ê²° ë° í”„ë¡œì‹œì € ë©”íƒ€ ì •ë³´ë¥¼ ë™ì‹œì— êµ¬ì¶•í•©ë‹ˆë‹¤.
í”„ë¡œì‹œì € ë£¨íŠ¸ì—ì„œëŠ” ì´ë¦„Â·ìŠ¤í‚¤ë§ˆë¥¼ ì¶”ì¶œí•´ `ProcedureInfo`ë¥¼ ì´ˆê¸°í™”í•˜ê³ , ë¶„ì„ ëŒ€ìƒ ë…¸ë“œëŠ” `pending_nodes`ë¥¼ ì¦ê°€ì‹œì¼œ ìš”ì•½ ì™„ë£Œ ì—¬ë¶€ë¥¼ ì¶”ì í•©ë‹ˆë‹¤.
```245:357:understand/analysis.py
class StatementCollector:
    def _visit(
        self,
        node: Dict[str, Any],
        current_proc: Optional[str],
        current_type: Optional[str],
        current_schema: Optional[str],
    ) -> Optional[StatementNode]:
        start_line = node['startLine']
        end_line = node['endLine']
        node_type = node['type']
        children = node.get('children', []) or []

        child_nodes: List[StatementNode] = []
        procedure_key = current_proc
        procedure_type = current_type
        schema_name = current_schema

        line_entries = [
            (line_no, self._file_lines[line_no - 1] if 0 <= line_no - 1 < len(self._file_lines) else '')
            for line_no in range(start_line, end_line + 1)
        ]
        code = '\n'.join(f"{line_no}: {text}" for line_no, text in line_entries)

        if node_type in PROCEDURE_TYPES:
            schema_candidate, name_candidate = get_procedure_name_from_code(code)
            procedure_key = self._make_proc_key(name_candidate, start_line)
            procedure_type = node_type
            schema_name = schema_candidate
            if procedure_key not in self.procedures:
                self.procedures[procedure_key] = ProcedureInfo(
                    key=procedure_key,
                    procedure_type=node_type,
                    procedure_name=name_candidate or procedure_key,
                    schema_name=schema_candidate,
                    start_line=start_line,
                    end_line=end_line,
                )

        for child in children:
            child_node = self._visit(child, procedure_key, procedure_type, schema_name)
            if child_node is not None:
                child_nodes.append(child_node)

        analyzable = node_type not in NON_ANALYSIS_TYPES
        token = calculate_code_token(code)
        dml = node_type in DML_STATEMENT_TYPES
        has_children = bool(child_nodes)

        self._node_id += 1
        statement_node = StatementNode(
            node_id=self._node_id,
            start_line=start_line,
            end_line=end_line,
            node_type=node_type,
            code=code,
            token=token,
            has_children=has_children,
            procedure_key=procedure_key,
            procedure_type=procedure_type,
            procedure_name=self.procedures.get(procedure_key).procedure_name if procedure_key in self.procedures else None,
            schema_name=schema_name,
            analyzable=analyzable,
            dml=dml,
            lines=line_entries,
        )
        for child_node in child_nodes:
            child_node.parent = statement_node
        statement_node.children.extend(child_nodes)

        if analyzable and procedure_key and procedure_key in self.procedures:
            self.procedures[procedure_key].pending_nodes += 1
        else:
            statement_node.completion_event.set()

        self.nodes.append(statement_node)
        return statement_node
```

### BatchPlanner.plan: í† í° ê¸°ë°˜ ë°°ì¹˜ ì„¤ê³„
ìˆ˜ì§‘ëœ `StatementNode`ë¥¼ í† í° í•©ê³„ì™€ ë¶€ëª¨ ì—¬ë¶€ì— ë”°ë¼ ë¶„ë¦¬í•´ LLM í˜¸ì¶œ ë‹¨ìœ„ë¥¼ í˜•ì„±í•©ë‹ˆë‹¤.
ë¶€ëª¨ ë…¸ë“œëŠ” ë‹¨ë… ë°°ì¹˜ë¡œ, ë¦¬í”„ ë…¸ë“œëŠ” í† í° í•œë„(`MAX_BATCH_TOKEN`)ë¥¼ ë„˜ì§€ ì•ŠëŠ” ë²”ìœ„ì—ì„œ ë¬¶ìŠµë‹ˆë‹¤.
```360:431:understand/analysis.py
class BatchPlanner:
    def plan(self, nodes: List[StatementNode], folder_file: str) -> List[AnalysisBatch]:
        batches: List[AnalysisBatch] = []
        current_nodes: List[StatementNode] = []
        current_tokens = 0
        batch_id = 1

        for node in nodes:
            if not node.analyzable:
                continue

            if node.has_children:
                if current_nodes:
                    batches.append(self._create_batch(batch_id, current_nodes))
                    batch_id += 1
                    current_nodes = []
                    current_tokens = 0

                batches.append(self._create_batch(batch_id, [node]))
                batch_id += 1
                continue

            if current_nodes and current_tokens + node.token > self.token_limit:
                batches.append(self._create_batch(batch_id, current_nodes))
                batch_id += 1
                current_nodes = []
                current_tokens = 0

            current_nodes.append(node)
            current_tokens += node.token

        if current_nodes:
            batches.append(self._create_batch(batch_id, current_nodes))

        return batches
```

### ApplyManager._apply_batch: LLM ê²°ê³¼ â†’ Neo4j ë°˜ì˜
LLM ì‘ë‹µì„ ë°›ì•„ ìš”ì•½Â·ë³€ìˆ˜ ì‚¬ìš©Â·CALL ê´€ê³„ë¥¼ Cypher ì¿¼ë¦¬ë¡œ ìƒì„±í•˜ê³ , ë°°ì¹˜ ìˆœì„œë¥¼ ë³´ì¥í•˜ë©° Neo4jì— ì „ì†¡í•©ë‹ˆë‹¤.
ë˜í•œ í”„ë¡œì‹œì € ìš”ì•½ ë²„í‚·ì„ ê°±ì‹ í•´ ëª¨ë“  ë…¸ë“œê°€ ì²˜ë¦¬ë˜ë©´ í›„ì† ìš”ì•½ ì‘ì—…ì„ íŠ¸ë¦¬ê±°í•©ë‹ˆë‹¤.
```535:688:understand/analysis.py
class ApplyManager:
    async def _apply_batch(self, result: BatchResult):
        if not result.general_result:
            general_items: List[Dict[str, Any]] = []
        else:
            general_items = result.general_result.get('analysis', [])

        cypher_queries: List[str] = []
        summary_nodes = list(zip(result.batch.nodes, general_items))
        processed_nodes: set[int] = set()

        for node, analysis in summary_nodes:
            if not analysis:
                node.completion_event.set()
                continue
            cypher_queries.extend(self._build_node_queries(node, analysis))
            self._update_summary_store(node, analysis)
            processed_nodes.add(node.node_id)

        for node in result.batch.nodes:
            if node.node_id not in processed_nodes and node.completion_event.is_set() is False:
                node.completion_event.set()

        if result.table_result:
            cypher_queries.extend(self._build_table_queries(result.batch, result.table_result))

        if cypher_queries:
            await self._send_queries(cypher_queries, result.batch.progress_line)
```

### Analyzer.run: íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
íŒŒì¼ ë‹¨ìœ„ Understanding ì „ì²´ë¥¼ ì œì–´í•˜ëŠ” ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸ë¡œ, ìˆ˜ì§‘â†’ê·¸ë˜í”„ ì´ˆê¸°í™”â†’ë°°ì¹˜ ì‹¤í–‰â†’ê²°ê³¼ ì ìš©ì„ ë¹„ë™ê¸°ë¡œ ì—°ê²°í•©ë‹ˆë‹¤.
ìì‹ ìš”ì•½ ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦° ë’¤ ë°°ì¹˜ë³„ë¡œ LLMì„ í˜¸ì¶œí•˜ê³  ê²°ê³¼ë¥¼ ApplyManagerì— ì „ë‹¬í•´ ìˆœì°¨ ì²˜ë¦¬í•©ë‹ˆë‹¤.
```1252:1316:understand/analysis.py
class Analyzer:
    async def run(self):
        logging.info("[ì§„í–‰] %s ë¶„ì„ ì‹œì‘ (ì´ %sì¤„)", self.folder_file, self.last_line)
        try:
            collector = StatementCollector(self.antlr_data, self.file_content, self.folder_name, self.file_name)
            nodes, procedures = collector.collect()
            await self._initialize_static_graph(nodes)
            planner = BatchPlanner()
            batches = planner.plan(nodes, self.folder_file)

            if not batches:
                await self.send_queue.put({"type": "end_analysis"})
                return

            invoker = LLMInvoker(self.api_key, self.locale)
            apply_manager = ApplyManager(
                node_base_props=self.node_base_props,
                folder_props=self.folder_props,
                table_base_props=self.table_base_props,
                user_id=self.user_id,
                project_name=self.project_name,
                folder_name=self.folder_name,
                file_name=self.file_name,
                dbms=self.dbms,
                api_key=self.api_key,
                locale=self.locale,
                procedures=procedures,
                send_queue=self.send_queue,
                receive_queue=self.receive_queue,
                file_last_line=self.last_line,
            )

            semaphore = asyncio.Semaphore(min(self.max_workers, len(batches)))

            async def worker(batch: AnalysisBatch):
                await self._wait_for_dependencies(batch)
                async with semaphore:
                    general, table = await invoker.invoke(batch)
                await apply_manager.submit(batch, general, table)

            await asyncio.gather(*(worker(batch) for batch in batches))
            await apply_manager.finalize()

            await self.send_queue.put({"type": "end_analysis"})
```


## Converting ê³µìœ  ë¡œì§

### DbmsConversionGenerator.generate & _process_node: ê·¸ë˜í”„ ìˆœíšŒ + í† í° ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬
Neo4jì—ì„œ ì¡°íšŒí•œ ë…¸ë“œë¥¼ ìˆœíšŒí•˜ë©° í† í° ë²„í¼Â·ë¶€ëª¨/ìì‹Â·TRY ìƒíƒœë¥¼ ì¶”ì í•˜ëŠ” DBMS ë³€í™˜ ë©”ì¸ ë£¨í”„ì…ë‹ˆë‹¤.
ëŒ€ìš©ëŸ‰ ë…¸ë“œëŠ” LLM ìŠ¤ì¼ˆë ˆí†¤ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³ , ì„ê³„ í† í°ì„ ë„˜ìœ¼ë©´ ëˆ„ì  ì½”ë“œë¥¼ ë¶„ì„Â·ë³‘í•©í•©ë‹ˆë‹¤.
```62:138:convert/create_dbms_conversion.py
class DbmsConversionGenerator:
    async def generate(self) -> str:
        logging.info(f"ğŸ“‹ DBMS ë³€í™˜ ë…¸ë“œ ìˆœíšŒ ì‹œì‘: postgres â†’ {self.target_dbms}")
        seen_nodes = set()
        node_count = 0
        for record in self.traverse_nodes:
            node = record['n']
            node_key = (node.get('startLine'), node.get('endLine'))
            if node_key in seen_nodes:
                continue
            seen_nodes.add(node_key)
            node_count += 1
            await self._process_node(record)

        await self._finalize_remaining()
        return self.merged_code.strip()

    async def _process_node(self, record: dict) -> None:
        node = record['n']
        node_labels = record.get('nodeLabels', [])
        node_type = node_labels[0] if node_labels else node.get('name', 'UNKNOWN')
        has_children = bool(node.get('has_children', False))
        token = int(node.get('token', 0) or 0)
        start_line = int(node.get('startLine', 0) or 0)
        end_line = int(node.get('endLine', 0) or 0)
        relationship = record['r'][1] if record.get('r') else 'NEXT'

        if node_type == 'TRY':
            self.pending_try_mode = True

        if node_type == 'EXCEPTION':
            await self._handle_exception_node(node, start_line, end_line)
            return

        parent = self.current_parent
        if parent and relationship == 'NEXT' and start_line > parent['end']:
            if self.sp_code_parts:
                await self._analyze_and_merge()
            await self._finalize_parent()

        if token >= TOKEN_THRESHOLD and has_children and node_type not in DML_TYPES:
            if self.sp_code_parts:
                await self._analyze_and_merge()
            await self._handle_large_node(node, start_line, end_line, token)
        else:
            self._handle_small_node(node, start_line, end_line, token)

        if int(self.total_tokens) >= TOKEN_THRESHOLD:
            await self._analyze_and_merge()
```

### DbmsConversionGenerator._handle_large_node â†’ _analyze_and_merge: LLM ê¸°ë°˜ ì½”ë“œ í•©ì„±
ìš”ì•½ëœ ë¶€ëª¨ ë²”ìœ„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤ì¼ˆë ˆí†¤ì„ ì‚½ì…í•œ ë’¤, ëˆ„ì ëœ ë¦¬í”„ ì½”ë“œë¥¼ LLMì— ì „ë‹¬í•´ ì‹¤ì œ ë³€í™˜ ì½”ë“œë¥¼ ì–»ìŠµë‹ˆë‹¤.
TRY ë¸”ë¡ ì—¬ë¶€ì— ë”°ë¼ ì½”ë“œ ë²„í¼ë¥¼ ë¶„ê¸° ì²˜ë¦¬í•´ ì˜ˆì™¸ ì²˜ë¦¬ êµ¬ê°„ì„ ë³´ì¡´í•©ë‹ˆë‹¤.
```141:210:convert/create_dbms_conversion.py
    async def _handle_large_node(self, node: dict, start_line: int, end_line: int, token: int) -> None:
        summarized = (node.get('summarized_code') or '').strip()
        if not summarized:
            return

        result = self.rule_loader.execute(
            role_name='dbms_summarized',
            inputs={
                'summarized_code': summarized,
                'locale': self.locale
            },
            api_key=self.api_key
        )
        skeleton = result['code']

        if not self.current_parent:
            self.current_parent = {'start': start_line, 'end': end_line, 'code': skeleton}
        else:
            self.current_parent['code'] = self.current_parent['code'].replace(
                CODE_PLACEHOLDER, f"\n{textwrap.indent(skeleton, '    ')}", 1
            )

    async def _analyze_and_merge(self) -> None:
        if not self.sp_code_parts or self.sp_start is None:
            return

        sp_code = '\n'.join(self.sp_code_parts)
        result = self.rule_loader.execute(
            role_name='dbms_conversion',
            inputs={
                'code': sp_code,
                'locale': self.locale,
                'parent_code': self.current_parent['code'] if self.current_parent else ""
            },
            api_key=self.api_key
        )

        generated_code = (result.get('code') or '').strip()
        if generated_code:
            if self.current_parent:
                self.code_buffer += f"\n{generated_code}"
            else:
                if self.pending_try_mode:
                    self.code_buffer += f"\n{generated_code}"
                else:
                    self.merged_code += f"\n{generated_code}"

        self.total_tokens = int(0)
        self.sp_code_parts.clear()
        self.sp_start = None
        self.sp_end = None
```

### ServicePreprocessingGenerator._process_node: ì„œë¹„ìŠ¤ ì½”ë“œ ë³€í™˜ íŒŒì´í”„ë¼ì¸
DBMS ë³€í™˜ê³¼ ë™ì¼í•œ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ íŒ¨í„´ìœ¼ë¡œ ìë°” ì„œë¹„ìŠ¤ ìƒì„±ì— ë§ê²Œ ë…¸ë“œë¥¼ ë¶„ê¸° ì²˜ë¦¬í•©ë‹ˆë‹¤.
TRY/EXCEPTION ìƒíƒœ, í† í° ì„ê³„, ë¶€ëª¨ ê²½ê³„ë¥¼ ê°ì§€í•˜ë©° LLM ë¶„ì„ íƒ€ì´ë°ì„ ì œì–´í•©ë‹ˆë‹¤.
```97:188:convert/create_service_preprocessing.py
class ServicePreprocessingGenerator:
    async def _process_node(self, record: dict) -> None:
        node = record['n']
        node_labels = record.get('nodeLabels', [])
        node_type = node_labels[0] if node_labels else node.get('name', 'UNKNOWN')
        has_children = bool(node.get('has_children', False))
        token = int(node.get('token', 0) or 0)
        start_line = int(node.get('startLine', 0) or 0)
        end_line = int(node.get('endLine', 0) or 0)
        relationship = record['r'][1] if record.get('r') else 'NEXT'

        if node_type == 'TRY':
            self.pending_try_mode = True

        if node_type == 'EXCEPTION':
            await self._handle_exception_node(node, start_line, end_line)
            return

        parent = self.current_parent
        if parent and relationship == 'NEXT' and start_line > parent['end']:
            if self.sp_code_parts:
                await self._analyze_and_merge()
            await self._finalize_parent()

        if token >= TOKEN_THRESHOLD and has_children and node_type not in DML_TYPES:
            if self.sp_code_parts:
                await self._analyze_and_merge()
            logging.info(f"  â”Œâ”€ í° ë…¸ë“œ ì§„ì… [{start_line}~{end_line}] (í† í°: {token})")
            await self._handle_large_node(node, start_line, end_line, token)
        else:
            self._handle_small_node(node, start_line, end_line, token)

        if int(self.total_tokens) >= TOKEN_THRESHOLD:
            await self._analyze_and_merge()
```

### ServicePreprocessingGenerator._handle_large_node: ì„œë¹„ìŠ¤ ìŠ¤ì¼ˆë ˆí†¤ í•©ì„±
Neo4jì—ì„œ ìˆ˜ì§‘í•œ ë³€ìˆ˜Â·ì¿¼ë¦¬ ë§¥ë½ê³¼ ìš”ì•½ ì½”ë“œë¥¼ LLMì— ì „ë‹¬í•´ ì„œë¹„ìŠ¤ ë©”ì„œë“œ ìŠ¤ì¼ˆë ˆí†¤ì„ ê°±ì‹ í•©ë‹ˆë‹¤.
ë¶€ëª¨ ì½”ë“œì˜ placeholderë¥¼ ì±„ìš°ë©° Command/Sequence ì •ë³´ë„ í•¨ê»˜ ë°˜ì˜í•©ë‹ˆë‹¤.
```149:184:convert/create_service_preprocessing.py
    async def _handle_large_node(self, node: dict, start_line: int, end_line: int, token: int) -> None:
        summarized = (node.get('summarized_code') or '').strip()
        if not summarized:
            return

        used_vars, used_queries = await self._collect_current_context()

        result = self.rule_loader.execute(
            role_name='service_summarized',
            inputs={
                'summarized_code': summarized,
                'service_skeleton': json.dumps(self.service_skeleton, ensure_ascii=False),
                'variable': json.dumps(used_vars, ensure_ascii=False, indent=2),
                'command_variables': json.dumps(self.command_class_variable, ensure_ascii=False, indent=2),
                'query_method_list': json.dumps(used_queries, ensure_ascii=False, indent=2),
                'sequence_methods': json.dumps(self.sequence_methods, ensure_ascii=False, indent=2),
                'locale': self.locale
            },
            api_key=self.api_key
        )
        skeleton = result['code']

        if not self.current_parent:
            self.current_parent = {'start': start_line, 'end': end_line, 'code': skeleton}
        else:
            self.current_parent['code'] = self.current_parent['code'].replace(
                CODE_PLACEHOLDER, f"\n{textwrap.indent(skeleton, '    ')}", 1
            )
```

### ServicePreprocessingGenerator._analyze_and_merge: ì„œë¹„ìŠ¤ ë©”ì„œë“œ ë³¸ë¬¸ ìƒì„±
ëˆ„ì ëœ ë¦¬í”„ ì½”ë“œì™€ ìˆ˜ì§‘í•œ ë³€ìˆ˜/ì¿¼ë¦¬ ë©”íƒ€ë¥¼ ê²°í•©í•´ `service` ë£°ì„ í˜¸ì¶œí•˜ê³ , ì‹¤ì œ ìë°” ë©”ì„œë“œ ë³¸ë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
ìƒì„±ëœ ì½”ë“œì™€ ë³€ìˆ˜ ì—­í•  ì •ë³´ë¥¼ ê°ê° ë²„í¼ì™€ `tracking_variables`ì— ë°˜ì˜í•©ë‹ˆë‹¤.
```321:383:convert/create_service_preprocessing.py
    async def _analyze_and_merge(self) -> None:
        if not self.sp_code_parts or self.sp_start is None:
            return

        sp_code = '\n'.join(self.sp_code_parts)
        used_variables = []
        try:
            collected = await collect_variables_in_range(self.variable_nodes, self.sp_start, self.sp_end)
            used_variables = [{**v, 'role': self.tracking_variables.get(v['name'], '')} for v in collected]
        except Exception as e:
            logging.debug(f"ë³€ìˆ˜ ìˆ˜ì§‘ ìŠ¤í‚µ: {e}")

        used_query_methods = {}
        try:
            used_query_methods = await extract_used_query_methods(
                self.sp_start, self.sp_end, self.query_method_list, {}
            )
        except Exception as e:
            logging.debug(f"JPA ìˆ˜ì§‘ ìŠ¤í‚µ: {e}")

        result = self.rule_loader.execute(
            role_name='service',
            inputs={
                'code': sp_code,
                'service_skeleton': json.dumps(self.service_skeleton, ensure_ascii=False),
                'variable': json.dumps(used_variables, ensure_ascii=False, indent=2),
                'query_method_list': json.dumps(used_query_methods, ensure_ascii=False, indent=2),
                'sequence_methods': json.dumps(self.sequence_methods, ensure_ascii=False, indent=2),
                'locale': self.locale,
                'parent_code': self.current_parent['code'] if self.current_parent else ""
            },
            api_key=self.api_key
        )

        self.tracking_variables.update(result['analysis'].get('variables', {}))

        java_code = (result.get('analysis', {}).get('code') or '').strip()
        if java_code:
            if self.current_parent:
                self.java_buffer += f"\n{java_code}"
            else:
                if self.pending_try_mode:
                    self.java_buffer += f"\n{java_code}"
                else:
                    self.merged_java_code += f"\n{java_code}"

        self.total_tokens = int(0)
        self.sp_code_parts.clear()
        self.sp_start = None
        self.sp_end = None
```


> ìœ„ ì •ë¦¬ëŠ” Understanding/Converting ê³µí†µ ë¡œì§ì„ êµ¬ì„±í•˜ëŠ” ì‹¤ì œ ì†ŒìŠ¤ ì½”ë“œ ì¡°ê°ì„ ê·¸ëŒ€ë¡œ ë°œì·Œí•´ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ íë¦„ì„ ë¬¸ì„œí™”í•œ ê²ƒì…ë‹ˆë‹¤.

